{
  "2602.21202v1": {
    "title": "Multi-Vector Index Compression in Any Modality",
    "url": "https://www.alphaxiv.org/abs/2602.21202v1",
    "arxiv_id": "2602.21202v1",
    "authors": "Hanxiang Qin, Alexander Martin, Rohan Jha, Chunsheng Zuo, Reno Kriz, Benjamin Van Durme",
    "categories": "cs.IR, cs.CL, cs.CV",
    "pub_date": "2026-02-24 18:57:33",
    "ori_summary": "We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.",
    "summary": "该论文研究多模态检索中多向量表示的存储和计算成本随文档长度线性增长的问题。其核心方法是提出四种查询无关的索引压缩方法，特别是基于注意力引导聚类的机制，通过识别文档语义显著区域作为聚类中心来压缩表示。",
    "translation": "任意模态下的多向量索引压缩",
    "relevance_score": 8,
    "reasoning": "该论文涉及多向量索引压缩技术，这直接关系到推荐系统、搜索和广告中的高效向量检索，是核心领域的重要进展。虽然标题未明确提及LLM或Transformer，但向量压缩技术对于大规模嵌入存储和检索至关重要，可应用于用户/物品表示的高效管理。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文研究多向量索引压缩技术，直接针对检索系统中的计算和存储效率问题，属于核心领域进展，与推荐、搜索系统的实际部署高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.21143v1": {
    "title": "A Benchmark for Deep Information Synthesis",
    "url": "https://www.alphaxiv.org/abs/2602.21143v1",
    "arxiv_id": "2602.21143v1",
    "authors": "Debjit Paul, Daniel Murphy, Milan Gritta, Ronald Cardenas, Victor Prokhorov, Lena Sophia Bolliger, Aysim Toker, Roy Miles, Andreea-Maria Oncescu, Jasivan Alex Sivakumar, Philipp Borchert, Ismail Elezi, Meiru Zhang, Ka Yiu Lee, Guchun Zhang, Jun Wang, Gerasimos Lampouras",
    "categories": "cs.AI, cs.CL, cs.IR, cs.LG",
    "pub_date": "2026-02-24 17:43:32",
    "ori_summary": "Large language model (LLM)-based agents are increasingly used to solve complex tasks involving tool use, such as web browsing, code execution, and data analysis. However, current evaluation benchmarks do not adequately assess their ability to solve real-world tasks that require synthesizing information from multiple sources and inferring insights beyond simple fact retrieval. To address this, we introduce DEEPSYNTH, a novel benchmark designed to evaluate agents on realistic, time-consuming problems that combine information gathering, synthesis, and structured reasoning to produce insights. DEEPSYNTH contains 120 tasks collected across 7 domains and data sources covering 67 countries. DEEPSYNTH is constructed using a multi-stage data collection pipeline that requires annotators to collect official data sources, create hypotheses, perform manual analysis, and design tasks with verifiable answers. When evaluated on DEEPSYNTH, 11 state-of-the-art LLMs and deep research agents achieve a maximum F1 score of 8.97 and 17.5 on the LLM-judge metric, underscoring the difficulty of the benchmark. Our analysis reveals that current agents struggle with hallucinations and reasoning over large information spaces, highlighting DEEPSYNTH as a crucial benchmark for guiding future research.",
    "summary": "",
    "translation": "深度信息合成的基准",
    "relevance_score": 1,
    "reasoning": "该标题涉及基准测试，属于评估范畴，与您关注的推荐系统、搜索或广告领域的技术进展、LLM应用或Transformer架构创新无直接关联。基准测试通常用于评估模型性能，而非推动核心技术进步，因此不符合您的任何重点方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21103v1": {
    "title": "Prompt-Level Distillation: A Non-Parametric Alternative to Model Fine-Tuning for Efficient Reasoning",
    "url": "https://www.alphaxiv.org/abs/2602.21103v1",
    "arxiv_id": "2602.21103v1",
    "authors": "Sanket Badhe, Deep Shah",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2026-02-24 17:03:21",
    "ori_summary": "Advanced reasoning typically requires Chain-of-Thought prompting, which is accurate but incurs prohibitive latency and substantial test-time inference costs. The standard alternative, fine-tuning smaller models, often sacrifices interpretability while introducing significant resource and operational overhead. To address these limitations, we introduce Prompt-Level Distillation (PLD). We extract explicit reasoning patterns from a Teacher model and organize them into a structured list of expressive instructions for the Student model's System Prompt. Evaluated on the StereoSet and Contract-NLI datasets using Gemma-3 4B, PLD improved Macro F1 scores from 57\\% to 90.0\\% and 67\\% to 83\\% respectively, enabling this compact model to match frontier performance with negligible latency overhead. These expressive instructions render the decision-making process transparent, allowing for full human verification of logic, making this approach ideal for regulated industries such as law, finance, and content moderation, as well as high-volume use cases and edge devices.",
    "summary": "论文研究如何高效部署复杂推理能力，核心思想是通过从大模型中提取显式推理模式并结构化组织成系统提示指令，使小模型无需微调即可获得可解释的推理能力。",
    "translation": "提示级蒸馏：一种用于高效推理的非参数化替代模型微调方法",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM推理效率的核心技术进步，属于'Enabling LLM Tech'范畴。提示级蒸馏技术可以显著降低LLM推理的计算成本，这对于需要实时响应的推荐系统、搜索和广告应用至关重要，能够使大规模部署更加高效可行。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出的提示级蒸馏方法直接优化推理效率，属于核心LLM技术进展，对高并发搜索/推荐系统的实时推理有明确应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.21099v1": {
    "title": "Turning Semantics into Topology: LLM-Driven Attribute Augmentation for Collaborative Filtering",
    "url": "https://www.alphaxiv.org/abs/2602.21099v1",
    "arxiv_id": "2602.21099v1",
    "authors": "Junjie Meng, Ranxu zhang, Wei Wu, Rui Zhang, Chuan Qin, Qi Zhang, Qi Liu, Hui Xiong, Chao Wang",
    "categories": "cs.IR",
    "pub_date": "2026-02-24 17:01:47",
    "ori_summary": "Large Language Models (LLMs) have shown great potential for enhancing recommender systems through their extensive world knowledge and reasoning capabilities. However, effectively translating these semantic signals into traditional collaborative embeddings remains an open challenge. Existing approaches typically fall into two extremes: direct inference methods are computationally prohibitive for large-scale retrieval, while embedding-based methods primarily focus on unilateral feature augmentation rather than holistic collaborative signal enhancement. To bridge this gap, we propose Topology-Augmented Graph Collaborative Filtering (TAGCF), a novel framework that transforms semantic knowledge into topological connectivity. Unlike existing approaches that depend on textual features or direct interaction synthesis, TAGCF employs LLMs to infer interaction intents and underlying causal relationships from user-item pairs, representing these insights as intermediate attribute nodes within an enriched User-Attribute-Item (U-A-I) graph. Furthermore, to effectively model the heterogeneous relations in this augmented structure, we propose Adaptive Relation-weighted Graph Convolution (ARGC), which employs relation-specific prediction networks to dynamically estimate the importance of each relation type. Extensive experiments across multiple benchmark datasets and CF backbones demonstrate consistent improvements, with comprehensive evaluations including cold-start scenarios validating the effectiveness and robustness of our framework. All code will be made publicly available. For anonymous review, our code is available at the following anonymous link: https://anonymous.4open.science/r/AGCF-2441353190/.",
    "summary": "该论文研究如何将大型语言模型的语义知识有效融入传统协同过滤推荐系统。核心方法是利用LLM从用户-物品对中推断交互意图和因果关系，将其表示为属性节点，构建增强的用户-属性-物品图，并通过自适应关系加权图卷积建模异构关系。",
    "translation": "将语义转化为拓扑结构：基于大语言模型的属性增强协同过滤",
    "relevance_score": 9,
    "reasoning": "该论文直接应用LLM技术增强协同过滤中的属性表示，属于'Direct LLM Applications'范畴。通过LLM驱动的属性增强，可以改善推荐系统中的用户-物品交互建模，提升推荐质量，对RecSys有明确的应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM在推荐系统中的应用，提出将语义知识转化为图拓扑结构的方法，完美契合核心领域进展和直接LLM应用两个焦点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.21052v1": {
    "title": "Position-Aware Sequential Attention for Accurate Next Item Recommendations",
    "url": "https://www.alphaxiv.org/abs/2602.21052v1",
    "arxiv_id": "2602.21052v1",
    "authors": "Timur Nabiev, Evgeny Frolov",
    "categories": "cs.IR, cs.AI, cs.LG",
    "pub_date": "2026-02-24 16:09:47",
    "ori_summary": "Sequential self-attention models usually rely on additive positional embeddings, which inject positional information into item representations at the input. In the absence of positional signals, the attention block is permutation-equivariant over sequence positions and thus has no intrinsic notion of temporal order beyond causal masking. We argue that additive positional embeddings make the attention mechanism only superficially sensitive to sequence order: positional information is entangled with item embedding semantics, propagates weakly in deep architectures, and limits the ability to capture rich sequential patterns. To address these limitations, we introduce a kernelized self-attention mechanism, where a learnable positional kernel operates purely in the position space, disentangled from semantic similarity, and directly modulates attention weights. When applied per attention block, this kernel enables adaptive multi-scale sequential modeling. Experiments on standard next-item prediction benchmarks show that our positional kernel attention consistently improves over strong competing baselines.",
    "summary": "该论文研究序列推荐中自注意力模型对时序顺序的建模能力不足问题，核心思想是提出一种核化的自注意力机制，通过可学习的位置核在纯位置空间操作，直接调制注意力权重，实现位置信息与语义表示的分离。",
    "translation": "基于位置感知的顺序注意力机制用于精准的下一个物品推荐",
    "relevance_score": 9,
    "reasoning": "该论文直接针对推荐系统（RecSys）中的序列推荐问题，提出改进的注意力机制，属于核心领域进展。位置感知的注意力机制可提升序列建模的准确性，对用户行为序列中的时序模式捕捉有直接帮助，能应用于电商、内容平台等场景的下一个物品推荐任务。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对序列推荐中的核心Transformer架构进行改进，提出解耦位置信息与语义嵌入的新注意力机制，属于Transformer架构效率与注意力机制创新的前沿研究。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.21009v1": {
    "title": "HiSAC: Hierarchical Sparse Activation Compression for Ultra-long Sequence Modeling in Recommenders",
    "url": "https://www.alphaxiv.org/abs/2602.21009v1",
    "arxiv_id": "2602.21009v1",
    "authors": "Kun Yuan, Junyu Bi, Daixuan Cheng, Changfa Wu, Shuwen Xiao, Binbin Cao, Jian Wu, Yuning Jiang",
    "categories": "cs.IR, cs.CL",
    "pub_date": "2026-02-24 15:28:58",
    "ori_summary": "Modern recommender systems leverage ultra-long user behavior sequences to capture dynamic preferences, but end-to-end modeling is infeasible in production due to latency and memory constraints. While summarizing history via interest centers offers a practical alternative, existing methods struggle to (1) identify user-specific centers at appropriate granularity and (2) accurately assign behaviors, leading to quantization errors and loss of long-tail preferences. To alleviate these issues, we propose Hierarchical Sparse Activation Compression (HiSAC), an efficient framework for personalized sequence modeling. HiSAC encodes interactions into multi-level semantic IDs and constructs a global hierarchical codebook. A hierarchical voting mechanism sparsely activates personalized interest-agents as fine-grained preference centers. Guided by these agents, Soft-Routing Attention aggregates historical signals in semantic space, weighting by similarity to minimize quantization error and retain long-tail behaviors. Deployed on Taobao's \"Guess What You Like\" homepage, HiSAC achieves significant compression and cost reduction, with online A/B tests showing a consistent 1.65% CTR uplift -- demonstrating its scalability and real-world effectiveness.",
    "summary": "该论文研究推荐系统中超长用户行为序列的高效建模问题，其核心思想是通过构建全局分层码本和个性化兴趣代理，在语义空间中进行稀疏激活和软路由注意力聚合，以最小化量化误差并保留长尾偏好。",
    "translation": "HiSAC：用于推荐系统中超长序列建模的分层稀疏激活压缩",
    "relevance_score": 9,
    "reasoning": "该论文直接针对推荐系统中的超长序列建模问题，属于核心推荐系统领域的技术进展。分层稀疏激活压缩技术可显著提升Transformer架构在长序列处理时的效率，这对于处理用户历史行为序列等推荐系统核心任务具有直接应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统中的超长序列建模核心挑战，提出创新的分层稀疏激活压缩框架，完美契合核心领域进展和直接LLM应用方向。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.20995v1": {
    "title": "Generative Pseudo-Labeling for Pre-Ranking with LLMs",
    "url": "https://www.alphaxiv.org/abs/2602.20995v1",
    "arxiv_id": "2602.20995v1",
    "authors": "Junyu Bi, Xinting Niu, Daixuan Cheng, Kun Yuan, Tao Wang, Binbin Cao, Jian Wu, Yuning Jiang",
    "categories": "cs.IR, cs.CL",
    "pub_date": "2026-02-24 15:14:49",
    "ori_summary": "Pre-ranking is a critical stage in industrial recommendation systems, tasked with efficiently scoring thousands of recalled items for downstream ranking. A key challenge is the train-serving discrepancy: pre-ranking models are trained only on exposed interactions, yet must score all recalled candidates -- including unexposed items -- during online serving. This mismatch not only induces severe sample selection bias but also degrades generalization, especially for long-tail content. Existing debiasing approaches typically rely on heuristics (e.g., negative sampling) or distillation from biased rankers, which either mislabel plausible unexposed items as negatives or propagate exposure bias into pseudo-labels. In this work, we propose Generative Pseudo-Labeling (GPL), a framework that leverages large language models (LLMs) to generate unbiased, content-aware pseudo-labels for unexposed items, explicitly aligning the training distribution with the online serving space. By offline generating user-specific interest anchors and matching them with candidates in a frozen semantic space, GPL provides high-quality supervision without adding online latency. Deployed in a large-scale production system, GPL improves click-through rate by 3.07%, while significantly enhancing recommendation diversity and long-tail item discovery.",
    "summary": "该论文研究工业推荐系统中预排序阶段因训练数据仅含曝光项而需评估所有召回项所导致的严重样本选择偏差问题。其核心思想是利用大型语言模型生成内容感知的无偏伪标签，通过离线构建用户兴趣锚点并在冻结语义空间匹配候选项目，使训练分布与在线服务空间对齐。",
    "translation": "基于大语言模型的预排序生成式伪标签方法",
    "relevance_score": 9,
    "reasoning": "该论文直接应用LLM技术于推荐/搜索系统的预排序阶段，属于'Direct LLM Applications'范畴。生成式伪标签方法可以提升预排序模型的训练效率和质量，在召回后、精排前的阶段优化候选集质量，对大规模推荐系统具有明确实用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统预排序阶段的训练-服务偏差问题，提出利用LLM生成无偏伪标签的核心方法，完美契合用户对直接LLM应用和核心领域进展的关注点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.20986v1": {
    "title": "Naver Labs Europe @ WSDM CUP | Multilingual Retrieval",
    "url": "https://www.alphaxiv.org/abs/2602.20986v1",
    "arxiv_id": "2602.20986v1",
    "authors": "Thibault Formal, Maxime Louis, Hervé Déjean, Stéphane Clinchant",
    "categories": "cs.IR",
    "pub_date": "2026-02-24 15:09:01",
    "ori_summary": "This report presents our participation to the WSDM Cup 2026 shared task on multilingual document retrieval from English queries. The task provides a challenging benchmark for cross-lingual generalization. It also provides a natural testbed for evaluating SPLARE, our recently proposed learned sparse retrieval model, which produces generalizable sparse latent representations and is particularly well suited to multilingual retrieval settings. We evaluate five progressively enhanced runs, starting from a SPLARE-7B model and incorporating lightweight improvements, including reranking with Qwen3-Reranker-4B and simple score fusion strategies. Our results demonstrate the strength of SPLARE compared to state-of-the-art dense baselines such as Qwen3-8B-Embed. More broadly, our submission highlights the continued relevance and competitiveness of learned sparse retrieval models beyond English-centric scenarios.",
    "summary": "该论文研究多语言文档检索中的跨语言泛化问题。其核心方法是提出SPLARE学习稀疏检索模型，通过生成通用稀疏潜在表示来专门优化多语言检索场景。",
    "translation": "Naver Labs Europe @ WSDM CUP | 多语言检索",
    "relevance_score": 8,
    "reasoning": "该论文标题涉及多语言检索，直接关联搜索领域的核心进展，属于当前关注的“核心领域进展”范畴。多语言检索技术可应用于跨语言搜索系统，提升全球用户的内容发现体验，对搜索和推荐系统具有明确价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接研究多语言检索这一搜索核心问题，并提出了SPLARE稀疏检索模型，其通用稀疏潜在表示方法对跨语言场景具有针对性，与搜索领域进展高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.20877v1": {
    "title": "E-MMKGR: A Unified Multimodal Knowledge Graph Framework for E-commerce Applications",
    "url": "https://www.alphaxiv.org/abs/2602.20877v1",
    "arxiv_id": "2602.20877v1",
    "authors": "Jiwoo Kang, Yeon-Chang Lee",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2026-02-24 13:19:42",
    "ori_summary": "Multimodal recommender systems (MMRSs) enhance collaborative filtering by leveraging item-side modalities, but their reliance on a fixed set of modalities and task-specific objectives limits both modality extensibility and task generalization. We propose E-MMKGR, a framework that constructs an e-commerce-specific Multimodal Knowledge Graph E-MMKG and learns unified item representations through GNN-based propagation and KG-oriented optimization. These representations provide a shared semantic foundation applicable to diverse tasks. Experiments on real-world Amazon datasets show improvements of up to 10.18% in Recall@10 for recommendation and up to 21.72% over vector-based retrieval for product search, demonstrating the effectiveness and extensibility of our approach.",
    "summary": "该论文研究电子商务中多模态推荐系统因固定模态集和任务特定目标导致的模态扩展性和任务泛化受限问题。核心方法是构建电子商务特定多模态知识图谱，通过图神经网络传播和知识图谱导向优化学习统一物品表示，为多样化任务提供共享语义基础。",
    "translation": "E-MMKGR：面向电子商务应用的一体化多模态知识图谱框架",
    "relevance_score": 8,
    "reasoning": "该论文提出统一的多模态知识图谱框架，直接对应“VLM Analogy for Heterogeneous Data”关注点，将商品图像、文本描述、用户行为等异构数据作为不同模态进行统一建模。这种框架在电子商务的推荐和搜索场景中具有直接应用潜力，可用于增强商品理解、个性化推荐和跨模态检索。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出统一多模态知识图谱框架，直接解决推荐系统多模态扩展性和任务泛化问题，与核心领域进展和异构数据统一建模高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.20800v1": {
    "title": "Mitigating Preference Leakage via Strict Estimator Separation for Normative Generative Ranking",
    "url": "https://www.alphaxiv.org/abs/2602.20800v1",
    "arxiv_id": "2602.20800v1",
    "authors": "Dalia Nahhas, Xiaohao Cai, Imran Razzak, Shoaib Jameel",
    "categories": "cs.IR",
    "pub_date": "2026-02-24 11:38:36",
    "ori_summary": "In Generative Information Retrieval (GenIR), the bottleneck has shifted from generation to the selection of candidates, particularly for normative criteria such as cultural relevance. Current LLM-as-a-Judge evaluations often suffer from circularity and preference leakage, where overlapping supervision and evaluation models inflate performance. We address this by formalising cultural relevance as a within-query ranking task and introducing a leakage-free two-judge framework that strictly separates supervision (Judge B) from evaluation (Judge A). On a new benchmark of 33,052 (NGR-33k) culturally grounded stories, we find that while classical baselines yield only modest gains, a dense bi-encoder distilled from a Judge-B-supervised Cross-Encoder is highly effective. Although the Cross-Encoder provides a strong supervision signal for distillation, the distilled BGE-M3 model substantially outperforms it under leakage-free Judge~A evaluation. We validate our framework on the human-curated Moral Stories dataset, showing strong alignment with human norms. Our results demonstrate that rigorous evaluator separation is a prerequisite for credible GenIR evaluation, proving that subtle cultural preferences can be distilled into efficient rankers without leakage.",
    "summary": "该论文研究生成式信息检索中因监督与评估模型重叠导致的偏好泄露问题，核心思想是提出一个泄漏无关的双评估者框架，严格分离监督模型（Judge B）与评估模型（Judge A），并将文化偏好等规范标准通过蒸馏注入高效的排序器。",
    "translation": "通过严格估计器分离缓解偏好泄露以实现规范性生成排序",
    "relevance_score": 8,
    "reasoning": "该论文标题涉及生成排序（Generative Ranking），这是搜索和推荐系统中的核心排序技术。标题中的'缓解偏好泄露'（Mitigating Preference Leakage）直接关系到推荐系统的重要技术挑战——如何在训练和部署中保护用户偏好隐私，同时'规范性生成排序'（Normative Generative Ranking）表明该方法具有生成能力，可能结合LLM技术。该论文属于'核心领域进展'和'直接LLM应用'范畴，因为它直接处理推荐/搜索中的排序问题，并可能应用生成模型技术。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对生成式信息检索中的偏好泄露问题，提出严格的评估者分离框架，并探索了从监督模型到高效排序器的蒸馏方法，与推荐/搜索领域的核心评估挑战和LLM应用高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.20735v1": {
    "title": "RMIT-ADM+S at the MMU-RAG NeurIPS 2025 Competition",
    "url": "https://www.alphaxiv.org/abs/2602.20735v1",
    "arxiv_id": "2602.20735v1",
    "authors": "Kun Ran, Marwah Alaofi, Danula Hettiachchi, Chenglong Ma, Khoi Nguyen Dinh Anh, Khoi Vo Nguyen, Sachin Pathiyan Cherumanal, Lida Rashidi, Falk Scholer, Damiano Spina, Shuoqi Sun, Oleg Zendel",
    "categories": "cs.IR, cs.AI, cs.CL",
    "pub_date": "2026-02-24 09:58:25",
    "ori_summary": "This paper presents the award-winning RMIT-ADM+S system for the Text-to-Text track of the NeurIPS~2025 MMU-RAG Competition. We introduce Routing-to-RAG (R2RAG), a research-focused retrieval-augmented generation (RAG) architecture composed of lightweight components that dynamically adapt the retrieval strategy based on inferred query complexity and evidence sufficiency. The system uses smaller LLMs, enabling operation on a single consumer-grade GPU while supporting complex research tasks. It builds on the G-RAG system, winner of the ACM~SIGIR~2025 LiveRAG Challenge, and extends it with modules informed by qualitative review of outputs. R2RAG won the Best Dynamic Evaluation award in the Open Source category, demonstrating high effectiveness with careful design and efficient use of resources.",
    "summary": "",
    "translation": "RMIT-ADM+S团队在MMU-RAG NeurIPS 2025竞赛中的工作",
    "relevance_score": 1,
    "reasoning": "该标题仅提及学术竞赛参与，未涉及任何技术内容或研究方向。根据标题无法判断其与推荐系统、搜索、广告、LLM技术或Transformer架构的任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20704v1": {
    "title": "IntRR: A Framework for Integrating SID Redistribution and Length Reduction",
    "url": "https://www.alphaxiv.org/abs/2602.20704v1",
    "arxiv_id": "2602.20704v1",
    "authors": "Zesheng Wang, Longfei Xu, Weidong Deng, Huimin Yan, Kaikui Liu, Xiangxiang Chu",
    "categories": "cs.IR",
    "pub_date": "2026-02-24 09:09:40",
    "ori_summary": "Generative Recommendation (GR) has emerged as a transformative paradigm that reformulates the traditional cascade ranking system into a sequence-to-item generation task, facilitated by the use of discrete Semantic IDs (SIDs). However, current SIDs are suboptimal as the indexing objectives (Stage 1) are misaligned with the actual recommendation goals (Stage 2). Since these identifiers remain static (Stage 2), the backbone model lacks the flexibility to adapt them to the evolving complexities of user interactions. Furthermore, the prevailing strategy of flattening hierarchical SIDs into token sequences leads to sequence length inflation, resulting in prohibitive computational overhead and inference latency. To address these challenges, we propose IntRR, a novel framework that integrates objective-aligned SID Redistribution and structural Length Reduction. By leveraging item-specific Unique IDs (UIDs) as collaborative anchors, this approach dynamically redistributes semantic weights across hierarchical codebook layers. Concurrently, IntRR handles the SID hierarchy recursively, eliminating the need to flatten sequences. This ensures a fixed cost of one token per item. Extensive experiments on benchmark datasets demonstrate that IntRR yields substantial improvements over representative generative baselines, achieving superior performance in both recommendation accuracy and efficiency.",
    "summary": "",
    "translation": "IntRR：一种集成SID重分配与长度缩减的框架",
    "relevance_score": 1,
    "reasoning": "该标题涉及SID（可能指会话ID或系统ID）重分配和长度缩减，这听起来像是网络协议、系统优化或数据压缩技术，与推荐系统、搜索、广告、LLM或Transformer架构的核心进展没有明显关联。没有迹象表明该框架在推荐/搜索/广告领域有直接应用或作为使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20676v1": {
    "title": "PRECTR-V2:Unified Relevance-CTR Framework with Cross-User Preference Mining, Exposure Bias Correction, and LLM-Distilled Encoder Optimization",
    "url": "https://www.alphaxiv.org/abs/2602.20676v1",
    "arxiv_id": "2602.20676v1",
    "authors": "Shuzhi Cao, Rong Chen, Ailong He, Shuguang Han, Jufeng Chen",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2026-02-24 08:26:17",
    "ori_summary": "In search systems, effectively coordinating the two core objectives of search relevance matching and click-through rate (CTR) prediction is crucial for discovering users' interests and enhancing platform revenue. In our prior work PRECTR, we proposed a unified framework to integrate these two subtasks,thereby eliminating their inconsistency and leading to mutual benefit.However, our previous work still faces three main challenges. First, low-active users and new users have limited search behavioral data, making it difficult to achieve effective personalized relevance preference modeling. Second, training data for ranking models predominantly come from high-relevance exposures, creating a distribution mismatch with the broader candidate space in coarse-ranking, leading to generalization bias. Third, due to the latency constraint, the original model employs an Emb+MLP architecture with a frozen BERT encoder, which prevents joint optimization and creates misalignment between representation learning and CTR fine-tuning. To solve these issues, we further reinforce our method and propose PRECTR-V2. Specifically, we mitigate the low-activity users' sparse behavior problem by mining global relevance preferences under the specific query, which facilitates effective personalized relevance modeling for cold-start scenarios. Subsequently, we construct hard negative samples through embedding noise injection and relevance label reconstruction, and optimize their relative ranking against positive samples via pairwise loss, thereby correcting exposure bias. Finally, we pretrain a lightweight transformer-based encoder via knowledge distillation from LLM and SFT on the text relevance classification task. This encoder replaces the frozen BERT module, enabling better adaptation to CTR fine-tuning and advancing beyond the traditional Emb+MLP paradigm.",
    "summary": "该论文研究搜索系统中相关性匹配与点击率预测的统一建模问题，核心方法是：通过跨用户查询下的全局偏好挖掘解决冷启动用户建模难题，通过嵌入噪声注入和相关性标签重构构建硬负样本来校正曝光偏差，并通过LLM知识蒸馏预训练轻量级Transformer编码器来替代冻结的BERT模块，实现表示学习与CTR微调的联合优化。",
    "translation": "PRECTR-V2：融合跨用户偏好挖掘、曝光偏差校正与LLM蒸馏编码器优化的统一相关性-CTR框架",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及推荐系统（RecSys）和搜索领域的核心进展，通过统一相关性-CTR框架解决实际排名问题。它结合了LLM蒸馏技术（属于Enabling LLM Tech）来优化编码器，并包含曝光偏差校正等推荐系统关键技术，完全符合核心领域进展和直接LLM应用两个焦点。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对搜索系统中的核心问题（相关性匹配与CTR预测的统一建模），提出了三个关键创新点（跨用户偏好挖掘、曝光偏差校正、LLM蒸馏编码器优化），完全符合核心领域进展和直接LLM应用的重点关注方向。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.20558v1": {
    "title": "From Logs to Language: Learning Optimal Verbalization for LLM-Based Recommendation in Production",
    "url": "https://www.alphaxiv.org/abs/2602.20558v1",
    "arxiv_id": "2602.20558v1",
    "authors": "Yucheng Shi, Ying Li, Yu Wang, Yesu Feng, Arjun Rao, Rein Houthooft, Shradha Sehgal, Jin Wang, Hao Zhen, Ninghao Liu, Linas Baltrunas",
    "categories": "cs.AI, cs.IR",
    "pub_date": "2026-02-24 05:15:24",
    "ori_summary": "Large language models (LLMs) are promising backbones for generative recommender systems, yet a key challenge remains underexplored: verbalization, i.e., converting structured user interaction logs into effective natural language inputs. Existing methods rely on rigid templates that simply concatenate fields, yielding suboptimal representations for recommendation. We propose a data-centric framework that learns verbalization for LLM-based recommendation. Using reinforcement learning, a verbalization agent transforms raw interaction histories into optimized textual contexts, with recommendation accuracy as the training signal. This agent learns to filter noise, incorporate relevant metadata, and reorganize information to improve downstream predictions. Experiments on a large-scale industrial streaming dataset show that learned verbalization delivers up to 93% relative improvement in discovery item recommendation accuracy over template-based baselines. Further analysis reveals emergent strategies such as user interest summarization, noise removal, and syntax normalization, offering insights into effective context construction for LLM-based recommender systems.",
    "summary": "该论文研究LLM推荐系统中结构化用户交互日志如何有效转化为自然语言输入的核心问题，提出通过强化学习训练一个verbalization代理，自动优化日志到文本的转换过程，以提升下游推荐准确性。",
    "translation": "从日志到语言：学习生产环境中基于LLM推荐的最优语言化表达",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及LLM在推荐系统中的应用，属于'Direct LLM Applications'范畴。它探讨如何将系统日志转化为自然语言表达，这对于提升推荐系统的可解释性和用户体验具有直接价值，并且'生产环境'的强调表明其具有实际应用潜力。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接解决LLM在推荐系统中的核心应用挑战——如何将结构化日志转化为有效自然语言输入，并提出了基于强化学习的优化框架，完全符合直接LLM应用和推荐系统核心进展的关注点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.20507v1": {
    "title": "Indaleko: The Unified Personal Index",
    "url": "https://www.alphaxiv.org/abs/2602.20507v1",
    "arxiv_id": "2602.20507v1",
    "authors": "William Anthony Mason",
    "categories": "cs.IR, cs.HC",
    "pub_date": "2026-02-24 03:17:36",
    "ori_summary": "Personal information retrieval fails when systems ignore how human memory works. While existing platforms force keyword searches across isolated silos, humans naturally recall through episodic cues like when, where, and in what context information was encountered. This dissertation presents the Unified Personal Index (UPI), a memory-aligned architecture that bridges this fundamental gap. The Indaleko prototype demonstrates the UPI's feasibility on a 31-million file dataset spanning 160TB across eight storage platforms. By integrating temporal, spatial, and activity metadata into a unified graph database, Indaleko enables natural language queries like \"photos near the conference venue last spring\" that existing systems cannot process. The implementation achieves sub-second query responses through memory anchor indexing, eliminates cross-platform search fragmentation, and maintains perfect precision for well-specified memory patterns. Evaluation against commercial systems (Google Drive, OneDrive, Dropbox, Windows Search) reveals that all fail on memory-based queries, returning overwhelming result sets without contextual filtering. In contrast, Indaleko successfully processes multi-dimensional queries combining time, location, and activity patterns. The extensible architecture supports rapid integration of new data sources (10 minutes to 10 hours per provider) while preserving privacy through UUID-based semantic decoupling. The UPI's architectural synthesis bridges cognitive theory with distributed systems design, as demonstrated through the Indaleko prototype and rigorous evaluation. This work transforms personal information retrieval from keyword matching to memory-aligned finding, providing immediate benefits for existing data while establishing foundations for future context-aware systems.",
    "summary": "",
    "translation": "Indaleko：统一个人索引",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及个人数据索引的统一化，可能属于数据管理或信息检索范畴，但未明确提及推荐系统、搜索或广告的核心技术、LLM应用、Transformer架构进展或异构数据建模。标题过于宽泛，缺乏具体的技术方向说明，难以判断其与当前关注领域的直接相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21201v1": {
    "title": "Aletheia tackles FirstProof autonomously",
    "url": "https://www.alphaxiv.org/abs/2602.21201v1",
    "arxiv_id": "2602.21201v1",
    "authors": "Tony Feng, Junehyuk Jung, Sang-hyun Kim, Carlo Pagano, Sergei Gukov, Chiang-Chiang Tsai, David Woodruff, Adel Javanmard, Aryan Mokhtari, Dawsen Hwang, Yuri Chervonyi, Jonathan N. Lee, Garrett Bingham, Trieu H. Trinh, Vahab Mirrokni, Quoc V. Le, Thang Luong",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2026-02-24 18:56:10",
    "ori_summary": "We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.",
    "summary": "",
    "translation": "阿莱西娅自主攻克FirstProof",
    "relevance_score": 1,
    "reasoning": "该标题涉及自主证明系统或形式验证领域，与推荐系统、搜索、广告、LLM技术或Transformer架构的核心进展无关。标题中的“Aletheia”和“FirstProof”暗示逻辑证明或验证系统，这些主题属于形式方法或自动推理领域，不在您关注的任何技术范畴内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21198v1": {
    "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs",
    "url": "https://www.alphaxiv.org/abs/2602.21198v1",
    "arxiv_id": "2602.21198v1",
    "authors": "Yining Hong, Huang Huang, Manling Li, Li Fei-Fei, Jiajun Wu, Yejin Choi",
    "categories": "cs.LG, cs.AI, cs.CL, cs.CV, cs.RO",
    "pub_date": "2026-02-24 18:55:18",
    "ori_summary": "Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \\textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \\textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.",
    "summary": "",
    "translation": "从试错中学习：具身化大语言模型的反思式测试时规划",
    "relevance_score": 2,
    "reasoning": "该论文主要关注具身化AI和测试时规划，属于机器人或物理交互领域，与推荐系统、搜索或广告的核心关注点无关。虽然涉及LLM技术，但其应用场景（具身化AI）与您关注的RecSys/Search/Ads领域没有直接关联，也没有明确展示在这些领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21193v1": {
    "title": "On Data Engineering for Scaling LLM Terminal Capabilities",
    "url": "https://www.alphaxiv.org/abs/2602.21193v1",
    "arxiv_id": "2602.21193v1",
    "authors": "Renjie Pi, Grace Lam, Mohammad Shoeybi, Pooya Jannaty, Bryan Catanzaro, Wei Ping",
    "categories": "cs.CL",
    "pub_date": "2026-02-24 18:51:04",
    "ori_summary": "Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.",
    "summary": "论文研究如何通过系统化的数据工程策略提升大型语言模型的终端任务能力。核心方法是提出一个轻量级合成任务生成管道（Terminal-Task-Gen），支持基于种子和技能的任务构建，并系统分析数据过滤、课程学习等训练策略，以生成大规模开源终端任务数据集（Terminal-Corpus）。",
    "translation": "论扩展大型语言模型终端能力的数据工程",
    "relevance_score": 8,
    "reasoning": "该论文标题聚焦于数据工程以扩展LLM能力，属于'Enabling LLM Tech'范畴。通过优化数据工程来提升LLM终端能力，可直接应用于推荐系统、搜索和广告领域，例如通过改进数据预处理、特征工程或数据增强技术来增强LLM在这些领域的性能表现。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文聚焦于LLM终端能力的数据工程策略，属于LLM核心技术的使能进展，其数据工程方法（如合成任务生成、过滤、课程学习）对搜索、推荐等领域的LLM应用具有潜在借鉴价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.21165v1": {
    "title": "PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data",
    "url": "https://www.alphaxiv.org/abs/2602.21165v1",
    "arxiv_id": "2602.21165v1",
    "authors": "Samah Fodeh, Linhai Ma, Yan Wang, Srivani Talakokkul, Ganesh Puthiaraju, Afshan Khan, Ashley Hagaman, Sarah Lowe, Aimee Roundtree",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-02-24 18:10:00",
    "ori_summary": "Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health (SDoH). Traditional qualitative coding frameworks are labor intensive and do not scale to large volumes of patient-authored messages across health systems. Existing machine learning (ML) and natural language processing (NLP) approaches provide partial solutions but often treat patient-centered communication (PCC) and SDoH as separate tasks or rely on models not well suited to patient-facing language. We introduce PVminer, a domain-adapted NLP framework for structuring patient voice in secure patient-provider communication. PVminer formulates PV detection as a multi-label, multi-class prediction task integrating patient-specific BERT encoders (PV-BERT-base and PV-BERT-large), unsupervised topic modeling for thematic augmentation (PV-Topic-BERT), and fine-tuned classifiers for Code, Subcode, and Combo-level labels. Topic representations are incorporated during fine-tuning and inference to enrich semantic inputs. PVminer achieves strong performance across hierarchical tasks and outperforms biomedical and clinical pre-trained baselines, achieving F1 scores of 82.25% (Code), 80.14% (Subcode), and up to 77.87% (Combo). An ablation study further shows that author identity and topic-based augmentation each contribute meaningful gains. Pre-trained models, source code, and documentation will be publicly released, with annotated datasets available upon request for research use.",
    "summary": "",
    "translation": "PVminer：一种用于检测患者生成数据中患者声音的领域特定工具",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及医疗领域（患者数据），这属于明确的无关主题。虽然工具开发可能涉及文本分析技术，但标题中明确指定的医疗应用领域使其与搜索、推荐或广告系统完全不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21158v1": {
    "title": "SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards",
    "url": "https://www.alphaxiv.org/abs/2602.21158v1",
    "arxiv_id": "2602.21158v1",
    "authors": "Dengjia Zhang, Xiaoou Liu, Lu Cheng, Yaqing Wang, Kenton Murray, Hua Wei",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2026-02-24 18:04:54",
    "ori_summary": "Large language models (LLMs) are increasingly deployed as multi-step decision-making agents, where effective reward design is essential for guiding learning. Although recent work explores various forms of reward shaping and step-level credit assignment, a key signal remains largely overlooked: the intrinsic uncertainty of LLMs. Uncertainty reflects model confidence, reveals where exploration is needed, and offers valuable learning cues even in failed trajectories. We introduce SELAUR: Self Evolving LLM Agent via Uncertainty-aware Rewards, a reinforcement learning framework that incorporates uncertainty directly into the reward design. SELAUR integrates entropy-, least-confidence-, and margin-based metrics into a combined token-level uncertainty estimate, providing dense confidence-aligned supervision, and employs a failure-aware reward reshaping mechanism that injects these uncertainty signals into step- and trajectory-level rewards to improve exploration efficiency and learning stability. Experiments on two benchmarks, ALFWorld and WebShop, show that our method consistently improves success rates over strong baselines. Ablation studies further demonstrate how uncertainty signals enhance exploration and robustness.",
    "summary": "",
    "translation": "SELAUR：基于不确定性感知奖励的自进化大语言模型智能体",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及LLM智能体与自进化机制，属于LLM应用范畴，但未明确指向推荐、搜索或广告领域。虽然不确定性感知奖励可能间接应用于推荐系统的探索-利用平衡，但标题缺乏与您关注领域的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21082v1": {
    "title": "Beyond the Star Rating: A Scalable Framework for Aspect-Based Sentiment Analysis Using LLMs and Text Classification",
    "url": "https://www.alphaxiv.org/abs/2602.21082v1",
    "arxiv_id": "2602.21082v1",
    "authors": "Vishal Patil, Shree Vaishnavi Bacha, Revanth Yamani, Yidan Sun, Mayank Kejriwal",
    "categories": "cs.CL",
    "pub_date": "2026-02-24 16:45:17",
    "ori_summary": "Customer-provided reviews have become an important source of information for business owners and other customers alike. However, effectively analyzing millions of unstructured reviews remains challenging. While large language models (LLMs) show promise for natural language understanding, their application to large-scale review analysis has been limited by computational costs and scalability concerns. This study proposes a hybrid approach that uses LLMs for aspect identification while employing classic machine-learning methods for sentiment classification at scale. Using ChatGPT to analyze sampled restaurant reviews, we identified key aspects of dining experiences and developed sentiment classifiers using human-labeled reviews, which we subsequently applied to 4.7 million reviews collected over 17 years from a major online platform. Regression analysis reveals that our machine-labeled aspects significantly explain variance in overall restaurant ratings across different aspects of dining experiences, cuisines, and geographical regions. Our findings demonstrate that combining LLMs with traditional machine learning approaches can effectively automate aspect-based sentiment analysis of large-scale customer feedback, suggesting a practical framework for both researchers and practitioners in the hospitality industry and potentially, other service sectors.",
    "summary": "论文研究如何高效分析大规模非结构化客户评论。核心方法是采用混合框架：利用LLM进行方面识别，结合传统机器学习方法实现可扩展的情感分类。",
    "translation": "超越星级评分：基于LLM与文本分类的可扩展方面级情感分析框架",
    "relevance_score": 8,
    "reasoning": "该论文直接应用LLM技术进行方面级情感分析，这在搜索和推荐系统中具有明确应用价值，可用于商品评论分析、用户反馈挖掘和内容理解。虽然标题未明确提及RecSys/Search/Ads，但情感分析是这些领域理解用户偏好和内容质量的关键技术，属于直接LLM应用范畴。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出结合LLM与传统机器学习的大规模情感分析框架，直接应用于客户反馈分析，与搜索/推荐系统中的用户理解高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.21059v1": {
    "title": "An Expert Schema for Evaluating Large Language Model Errors in Scholarly Question-Answering Systems",
    "url": "https://www.alphaxiv.org/abs/2602.21059v1",
    "arxiv_id": "2602.21059v1",
    "authors": "Anna Martin-Boyle, William Humphreys, Martha Brown, Cara Leckey, Harmanpreet Kaur",
    "categories": "cs.HC, cs.CL",
    "pub_date": "2026-02-24 16:16:44",
    "ori_summary": "Large Language Models (LLMs) are transforming scholarly tasks like search and summarization, but their reliability remains uncertain. Current evaluation metrics for testing LLM reliability are primarily automated approaches that prioritize efficiency and scalability, but lack contextual nuance and fail to reflect how scientific domain experts assess LLM outputs in practice. We developed and validated a schema for evaluating LLM errors in scholarly question-answering systems that reflects the assessment strategies of practicing scientists. In collaboration with domain experts, we identified 20 error patterns across seven categories through thematic analysis of 68 question-answer pairs. We validated this schema through contextual inquiries with 10 additional scientists, which showed not only which errors experts naturally identify but also how structured evaluation schemas can help them detect previously overlooked issues. Domain experts use systematic assessment strategies, including technical precision testing, value-based evaluation, and meta-evaluation of their own practices. We discuss implications for supporting expert evaluation of LLM outputs, including opportunities for personalized, schema-driven tools that adapt to individual evaluation patterns and expertise levels.",
    "summary": "",
    "translation": "学术问答系统中评估大型语言模型错误的专家模式",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于LLM在学术问答系统中的错误评估，属于纯粹的LLM评估基准和幻觉检测范畴，与您关注的推荐系统、搜索、广告等应用领域无关。标题中提到的'专家模式'可能指评估框架，但核心内容仍属于NLP-centric的评估主题，不在您的关注范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21054v1": {
    "title": "VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation",
    "url": "https://www.alphaxiv.org/abs/2602.21054v1",
    "arxiv_id": "2602.21054v1",
    "authors": "Seongheon Park, Changdae Oh, Hyeong Kyu Choi, Xuefeng Du, Sharon Li",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2026-02-24 16:11:14",
    "ori_summary": "Large Vision-Language Models (LVLMs) frequently hallucinate, limiting their safe deployment in real-world applications. Existing LLM self-evaluation methods rely on a model's ability to estimate the correctness of its own outputs, which can improve deployment reliability; however, they depend heavily on language priors and are therefore ill-suited for evaluating vision-conditioned predictions. We propose VAUQ, a vision-aware uncertainty quantification framework for LVLM self-evaluation that explicitly measures how strongly a model's output depends on visual evidence. VAUQ introduces the Image-Information Score (IS), which captures the reduction in predictive uncertainty attributable to visual input, and an unsupervised core-region masking strategy that amplifies the influence of salient regions. Combining predictive entropy with this core-masked IS yields a training-free scoring function that reliably reflects answer correctness. Comprehensive experiments show that VAUQ consistently outperforms existing self-evaluation methods across multiple datasets.",
    "summary": "",
    "translation": "VAUQ：面向大型视觉语言模型自评估的视觉感知不确定性量化",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型的自评估和不确定性量化，属于纯视觉/语言交叉领域的技术研究。虽然标题提到“大型视觉语言模型”，但核心是模型自我评估方法，没有明确展示在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21045v1": {
    "title": "PaperTrail: A Claim-Evidence Interface for Grounding Provenance in LLM-based Scholarly Q&A",
    "url": "https://www.alphaxiv.org/abs/2602.21045v1",
    "arxiv_id": "2602.21045v1",
    "authors": "Anna Martin-Boyle, Cara A. C. Leckey, Martha C. Brown, Harmanpreet Kaur",
    "categories": "cs.HC, cs.CL",
    "pub_date": "2026-02-24 16:04:50",
    "ori_summary": "Large language models (LLMs) are increasingly used in scholarly question-answering (QA) systems to help researchers synthesize vast amounts of literature. However, these systems often produce subtle errors (e.g., unsupported claims, errors of omission), and current provenance mechanisms like source citations are not granular enough for the rigorous verification that scholarly domain requires. To address this, we introduce PaperTrail, a novel interface that decomposes both LLM answers and source documents into discrete claims and evidence, mapping them to reveal supported assertions, unsupported claims, and information omitted from the source texts. We evaluated PaperTrail in a within-subjects study with 26 researchers who performed two scholarly editing tasks using PaperTrail and a baseline interface. Our results show that PaperTrail significantly lowered participants' trust compared to the baseline. However, this increased caution did not translate to behavioral changes, as people continued to rely on LLM-generated scholarly edits to avoid a cognitively burdensome task. We discuss the value of claim-evidence matching for understanding LLM trustworthiness in scholarly settings, and present design implications for cognition-friendly communication of provenance information.",
    "summary": "",
    "translation": "PaperTrail：基于LLM的学术问答中用于溯源证据的声明-证据接口",
    "relevance_score": 1,
    "reasoning": "该论文标题关注LLM在学术问答中的溯源和证据接口问题，这属于纯粹的LLM应用研究，与推荐系统、搜索或广告的核心技术进展无关。论文没有涉及Transformer架构改进、多模态建模或任何可能应用于推荐/搜索/广告领域的技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20994v1": {
    "title": "Multimodal MRI Report Findings Supervised Brain Lesion Segmentation with Substructures",
    "url": "https://www.alphaxiv.org/abs/2602.20994v1",
    "arxiv_id": "2602.20994v1",
    "authors": "Yubin Ge, Yongsong Huang, Xiaofeng Liu",
    "categories": "eess.IV, cs.AI, cs.CL, cs.CV, cs.LG",
    "pub_date": "2026-02-24 15:14:04",
    "ori_summary": "Report-supervised (RSuper) learning seeks to alleviate the need for dense tumor voxel labels with constraints derived from radiology reports (e.g., volumes, counts, sizes, locations). In MRI studies of brain tumors, however, we often involve multi-parametric scans and substructures. Here, fine-grained modality/parameter-wise reports are usually provided along with global findings and are correlated with different substructures. Moreover, the reports often describe only the largest lesion and provide qualitative or uncertain cues (``mild,'' ``possible''). Classical RSuper losses (e.g., sum volume consistency) can over-constrain or hallucinate unreported findings under such incompleteness, and are unable to utilize these hierarchical findings or exploit the priors of varied lesion types in a merged dataset. We explicitly parse the global quantitative and modality-wise qualitative findings and introduce a unified, one-sided, uncertainty-aware formulation (MS-RSuper) that: (i) aligns modality-specific qualitative cues (e.g., T1c enhancement, FLAIR edema) with their corresponding substructures using existence and absence losses; (ii) enforces one-sided lower-bounds for partial quantitative cues (e.g., largest lesion size, minimal multiplicity); and (iii) adds extra- vs. intra-axial anatomical priors to respect cohort differences. Certainty tokens scale penalties; missing cues are down-weighted. On 1238 report-labeled BraTS-MET/MEN scans, our MS-RSuper largely outperforms both a sparsely-supervised baseline and a naive RSuper method.",
    "summary": "",
    "translation": "基于多模态MRI报告发现监督的脑部病灶子结构分割",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及医学影像（MRI）和脑部病灶分割，属于医学/生物学领域特定应用，这直接列在无关主题中。虽然提到了“多模态”和“监督学习”，但核心内容与推荐系统、搜索或广告无关，也没有任何潜在的应用联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20976v1": {
    "title": "Evaluating Proactive Risk Awareness of Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.20976v1",
    "arxiv_id": "2602.20976v1",
    "authors": "Xuan Luo, Yubin Chen, Zhiyu Hou, Linpu Yu, Geng Tu, Jing Li, Ruifeng Xu",
    "categories": "cs.CL, cs.CY",
    "pub_date": "2026-02-24 15:00:00",
    "ori_summary": "As large language models (LLMs) are increasingly embedded in everyday decision-making, their safety responsibilities extend beyond reacting to explicit harmful intent toward anticipating unintended but consequential risks. In this work, we introduce a proactive risk awareness evaluation framework that measures whether LLMs can anticipate potential harms and provide warnings before damage occurs. We construct the Butterfly dataset to instantiate this framework in the environmental and ecological domain. It contains 1,094 queries that simulate ordinary solution-seeking activities whose responses may induce latent ecological impact. Through experiments across five widely used LLMs, we analyze the effects of response length, languages, and modality. Experimental results reveal consistent, significant declines in proactive awareness under length-restricted responses, cross-lingual similarities, and persistent blind spots in (multimodal) species protection. These findings highlight a critical gap between current safety alignment and the requirements of real-world ecological responsibility, underscoring the need for proactive safeguards in LLM deployment.",
    "summary": "",
    "translation": "评估大型语言模型的主动风险意识",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于LLM的风险评估，属于评估基准范畴，这在'无关主题'中被明确排除。虽然涉及LLM，但内容是关于模型安全性和评估方法，没有直接或间接展示在推荐系统、搜索或广告领域的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20973v1": {
    "title": "Linear Reasoning vs. Proof by Cases: Obstacles for Large Language Models in FOL Problem Solving",
    "url": "https://www.alphaxiv.org/abs/2602.20973v1",
    "arxiv_id": "2602.20973v1",
    "authors": "Yuliang Ji, Fuchen Shen, Jian Wu, Qiujie Xie, Yue Zhang",
    "categories": "cs.CL",
    "pub_date": "2026-02-24 14:53:34",
    "ori_summary": "To comprehensively evaluate the mathematical reasoning capabilities of Large Language Models (LLMs), researchers have introduced abundant mathematical reasoning datasets. However, most existing datasets primarily focus on linear reasoning, neglecting other parts such as proof by contradiction and proof by cases, which are crucial for investigating LLMs' reasoning abilities. To address this limitation, we first introduce a novel first-order logic (FOL) dataset named PC-FOL, annotated by professional mathematicians, focusing on case-based reasoning problems. All instances in this dataset are equipped with a manually written natural language proof, clearly distinguishing it from conventional linear reasoning datasets. Our experimental results over leading LLMs demonstrate a substantial performance gap between linear reasoning and case-based reasoning problems. To further investigate this phenomenon, we provide a theoretical analysis grounded in graphical model, which provides an explanation for the observed disparity between the two types of reasoning problems. We hope this work can reveal the core challenges in the field of automated natural language mathematical proof generation, paving the way for future research.",
    "summary": "",
    "translation": "线性推理与分情况证明：大型语言模型在一阶逻辑问题求解中的障碍",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于大型语言模型在形式逻辑推理任务中的具体能力限制（线性推理与分情况证明），这属于纯粹的NLP能力评估范畴。虽然涉及LLM技术，但论文明确关注一阶逻辑问题求解这一特定任务，与推荐系统、搜索或广告领域没有直接关联，也没有展示出在这些应用领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20966v1": {
    "title": "Blackbird Language Matrices: A Framework to Investigate the Linguistic Competence of Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.20966v1",
    "arxiv_id": "2602.20966v1",
    "authors": "Paola Merlo, Chunyang Jiang, Giuseppe Samo, Vivi Nastase",
    "categories": "cs.CL",
    "pub_date": "2026-02-24 14:45:08",
    "ori_summary": "This article describes a novel language task, the Blackbird Language Matrices (BLM) task, inspired by intelligence tests, and illustrates the BLM datasets, their construction and benchmarking, and targeted experiments on chunking and systematicity. BLMs are multiple-choice problems, structured at multiple levels: within each sentence, across the input sequence, within each candidate answer. Because of their rich structure, these curated, but naturalistic datasets are key to answer some core questions about current large language models abilities: do LLMs detect linguistic objects and their properties? Do they detect and use systematic patterns across sentences? Are they more prone to linguistic or reasoning errors, and how do these interact? We show that BLMs, while challenging, can be solved at good levels of performance, in more than one language, with simple baseline models or, at better performance levels, with more tailored models. We show that their representations contain the grammatical objects and attributes relevant to solve a linguistic task. We also show that these solutions are reached by detecting systematic patterns across sentences. The paper supports the point of view that curated, structured datasets support multi-faceted investigations of properties of language and large language models. Because they present a curated, articulated structure, because they comprise both learning contexts and expected answers, and because they are partly built by hand, BLMs fall in the category of datasets that can support explainability investigations, and be useful to ask why large language models behave the way they do.",
    "summary": "",
    "translation": "黑鸟语言矩阵：一种用于探究语言模型语言能力的框架",
    "relevance_score": 2,
    "reasoning": "该论文标题表明其专注于评估语言模型的语言能力，这属于语言模型评估基准的范畴。虽然涉及语言模型，但论文似乎更关注语言能力的理论探究而非在推荐系统、搜索或广告中的实际应用，因此与当前关注点的相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20945v1": {
    "title": "The Art of Efficient Reasoning: Data, Reward, and Optimization",
    "url": "https://www.alphaxiv.org/abs/2602.20945v1",
    "arxiv_id": "2602.20945v1",
    "authors": "Taiqiang Wu, Zenan Zu, Bo Zhou, Ngai Wong",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-02-24 14:28:16",
    "ori_summary": "Large Language Models (LLMs) consistently benefit from scaled Chain-of-Thought (CoT) reasoning, but also suffer from heavy computational overhead. To address this issue, efficient reasoning aims to incentivize short yet accurate thinking trajectories, typically through reward shaping with Reinforcement Learning (RL). In this paper, we systematically investigate the mechanics of efficient reasoning for LLMs. For comprehensive evaluation, we advocate for more fine-grained metrics, including length distribution conditioned on correctness and performance across a wide spectrum of token budgets ranging from 2k to 32k. First, we reveal that the training process follows a two-stage paradigm: length adaptation and reasoning refinement. After that, we conduct extensive experiments (about 0.2 million GPU hours) in a unified protocol, deconstructing training prompts and rollouts, reward shaping, and optimization strategies. In particular, a key finding is to train on relatively easier prompts, ensuring the density of positive reward signals and thus avoiding the length collapse. Meanwhile, the learned length bias can be generalized across domains. We distill all findings into valuable insights and practical guidelines, and further validate them across the Qwen3 series, ranging from 0.6B to 30B, demonstrating the robustness and generalization.",
    "summary": "",
    "translation": "高效推理的艺术：数据、奖励与优化",
    "relevance_score": 3,
    "reasoning": "该标题涉及推理、数据和优化等通用概念，可能涵盖强化学习或高效计算技术。虽然优化和数据管理是推荐系统、搜索和广告的基础，但标题过于宽泛，未明确指向LLM、Transformer架构或具体的推荐/搜索/广告应用。对于'赋能技术'，潜在的广告应用可能包括通过高效推理优化广告投放策略，但缺乏具体说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20918v1": {
    "title": "Predicting Sentence Acceptability Judgments in Multimodal Contexts",
    "url": "https://www.alphaxiv.org/abs/2602.20918v1",
    "arxiv_id": "2602.20918v1",
    "authors": "Hyewon Jang, Nikolai Ilinykh, Sharid Loáiciga, Jey Han Lau, Shalom Lappin",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2026-02-24 13:54:38",
    "ori_summary": "Previous work has examined the capacity of deep neural networks (DNNs), particularly transformers, to predict human sentence acceptability judgments, both independently of context, and in document contexts. We consider the effect of prior exposure to visual images (i.e., visual context) on these judgments for humans and large language models (LLMs). Our results suggest that, in contrast to textual context, visual images appear to have little if any impact on human acceptability ratings. However, LLMs display the compression effect seen in previous work on human judgments in document contexts. Different sorts of LLMs are able to predict human acceptability judgments to a high degree of accuracy, but in general, their performance is slightly better when visual contexts are removed. Moreover, the distribution of LLM judgments varies among models, with Qwen resembling human patterns, and others diverging from them. LLM-generated predictions on sentence acceptability are highly correlated with their normalised log probabilities in general. However, the correlations decrease when visual contexts are present, suggesting that a higher gap exists between the internal representations of LLMs and their generated predictions in the presence of visual contexts. Our experimental work suggests interesting points of similarity and of difference between human and LLM processing of sentences in multimodal contexts.",
    "summary": "",
    "translation": "多模态语境下的句子可接受性判断预测",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及多模态语境下的语言可接受性判断，这主要属于自然语言处理中的语言理解评估范畴，与推荐系统、搜索或广告的核心技术关联较弱。虽然提到了多模态，但未明确涉及用户行为序列、上下文特征等推荐/搜索领域的关键异构数据建模，因此潜在应用价值有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20892v1": {
    "title": "Exa-PSD: a new Persian sentiment analysis dataset on Twitter",
    "url": "https://www.alphaxiv.org/abs/2602.20892v1",
    "arxiv_id": "2602.20892v1",
    "authors": "Seyed Himan Ghaderi, Saeed Sarbazi Azad, Mohammad Mehdi Jaziriyan, Ahmad Akbari",
    "categories": "cs.CL",
    "pub_date": "2026-02-24 13:28:23",
    "ori_summary": "Today, Social networks such as Twitter are the most widely used platforms for communication of people. Analyzing this data has useful information to recognize the opinion of people in tweets. Sentiment analysis plays a vital role in NLP, which identifies the opinion of the individuals about a specific topic. Natural language processing in Persian has many challenges despite the adventure of strong language models. The datasets available in Persian are generally in special topics such as products, foods, hotels, etc while users may use ironies, colloquial phrases in social media To overcome these challenges, there is a necessity for having a dataset of Persian sentiment analysis on Twitter. In this paper, we introduce the Exa sentiment analysis Persian dataset, which is collected from Persian tweets. This dataset contains 12,000 tweets, annotated by 5 native Persian taggers. The aforementioned data is labeled in 3 classes: positive, neutral and negative. We present the characteristics and statistics of this dataset and use the pre-trained Pars Bert and Roberta as the base model to evaluate this dataset. Our evaluation reached a 79.87 Macro F-score, which shows the model and data can be adequately valuable for a sentiment analysis system.",
    "summary": "",
    "translation": "Exa-PSD：一个基于Twitter的新波斯语情感分析数据集",
    "relevance_score": 1,
    "reasoning": "该论文标题表明这是一个特定语言（波斯语）的情感分析数据集，属于纯粹的NLP评估基准范畴。虽然情感分析在理论上可能与推荐或搜索中的用户理解相关，但该论文专注于数据集创建而非核心算法、架构创新或直接应用，且未提及任何与推荐系统、搜索、广告相关的具体技术或应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20859v1": {
    "title": "FinAnchor: Aligned Multi-Model Representations for Financial Prediction",
    "url": "https://www.alphaxiv.org/abs/2602.20859v1",
    "arxiv_id": "2602.20859v1",
    "authors": "Zirui He, Huopu Zhang, Yanguang Liu, Sirui Wu, Mengnan Du",
    "categories": "cs.CL",
    "pub_date": "2026-02-24 13:02:09",
    "ori_summary": "Financial prediction from long documents involves significant challenges, as actionable signals are often sparse and obscured by noise, and the optimal LLM for generating embeddings varies across tasks and time periods. In this paper, we propose FinAnchor(Financial Anchored Representations), a lightweight framework that integrates embeddings from multiple LLMs without fine-tuning the underlying models. FinAnchor addresses the incompatibility of feature spaces by selecting an anchor embedding space and learning linear mappings to align representations from other models into this anchor. These aligned features are then aggregated to form a unified representation for downstream prediction. Across multiple financial NLP tasks, FinAnchor consistently outperforms strong single-model baselines and standard ensemble methods, demonstrating the effectiveness of anchoring heterogeneous representations for robust financial prediction.",
    "summary": "",
    "translation": "FinAnchor：用于金融预测的对齐多模态表示",
    "relevance_score": 3,
    "reasoning": "该论文涉及多模态表示对齐技术，这与VLM类比异质数据的概念有一定相关性，但具体应用于金融预测领域。金融预测属于特定领域应用（如股票、投资等），不在当前关注的RecSys/Search/Ads核心领域范围内，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20816v1": {
    "title": "Don't Ignore the Tail: Decoupling top-K Probabilities for Efficient Language Model Distillation",
    "url": "https://www.alphaxiv.org/abs/2602.20816v1",
    "arxiv_id": "2602.20816v1",
    "authors": "Sayantan Dasgupta, Trevor Cohn, Timothy Baldwin",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2026-02-24 11:54:06",
    "ori_summary": "The core learning signal used in language model distillation is the standard Kullback-Leibler (KL) divergence between the student and teacher distributions. Traditional KL divergence tends to be dominated by the next tokens with the highest probabilities, i.e., the teacher's modes, thereby diminishing the influence of less probable yet potentially informative components of the output distribution. We propose a new tail-aware divergence that decouples the contribution of the teacher model's top-K predicted probabilities from that of lower-probability predictions, while maintaining the same computational profile as the KL Divergence. Our decoupled approach reduces the impact of the teacher modes and, consequently, increases the contribution of the tail of the distribution. Experimental results demonstrate that our modified distillation method yields competitive performance in both pre-training and supervised distillation of decoder models across various datasets. Furthermore, the distillation process is efficient and can be performed with a modest academic budget for large datasets, eliminating the need for industry-scale computing.",
    "summary": "该论文研究语言模型蒸馏中KL散度过度关注高概率token而忽略低概率但信息丰富的尾部分布的问题。核心方法是提出一种尾部感知的散度度量，将教师模型的前K个高概率预测与低概率预测解耦，增强尾部分布对蒸馏过程的影响。",
    "translation": "勿忽视长尾分布：解耦Top-K概率以实现高效语言模型蒸馏",
    "relevance_score": 8,
    "reasoning": "该论文属于'核心LLM技术进展'范畴，专注于语言模型蒸馏的效率优化。蒸馏技术可直接应用于推荐/搜索/广告领域，用于压缩大型LLM以部署在资源受限的生产环境中，同时保持排序质量。解耦Top-K概率的方法可能提升蒸馏模型对长尾项目的处理能力，这对推荐系统中的多样性优化具有潜在价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出一种新的语言模型蒸馏方法，直接改进LLM训练效率，属于核心LLM技术进步，对推荐和搜索系统的模型部署有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.20759v1": {
    "title": "Overton Pluralistic Reinforcement Learning for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.20759v1",
    "arxiv_id": "2602.20759v1",
    "authors": "Yu Fu, Seongho Son, Ilija Bogunovic",
    "categories": "cs.CL",
    "pub_date": "2026-02-24 10:39:27",
    "ori_summary": "Existing alignment paradigms remain limited in capturing the pluralistic nature of human values. Overton Pluralism addresses this gap by generating responses with diverse perspectives from a single query. This paper introduces OP-GRPO (Overton Pluralistic Group Relative Policy Optimization), a reinforcement learning framework for implicit Overton Pluralism that enables a single large language model to produce pluralistic responses without explicit prompting or modular orchestration. Our workflow consists of two main steps. First, similarity estimator training fine-tunes a Sentence Transformer for Overton Pluralism tasks to provide more accurate coverage evaluation of generated responses. Second, OP-GRPO training incorporates this similarity estimator into a dual-reward system designed to ensure both broad coverage of genuine human perspectives and the uniqueness of each perspective, thereby promoting diversity. Empirical results demonstrate a \"small models, big perspective coverage\" effect. The trained Qwen2.5-3B-Instruct model surpasses a 20B GPT-OSS baseline with a 37.4 percent relative accuracy gain on a Natural Language Inference benchmark, and also outperforms a modular architecture baseline with a 19.1 percent relative improvement. Additional evaluations using GPT-4.1 as a large language model judge further confirm the robustness of the approach.",
    "summary": "",
    "translation": "面向大语言模型的奥弗顿多元强化学习",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及强化学习（RL）与大语言模型（LLMs）的结合，但未明确说明与推荐系统、搜索或广告的相关性。根据您的关注点，除非论文能清晰展示其在RecSys/Search/Ads中的具体应用（如通过RL优化推荐策略或搜索排序），否则其相关性较低。标题中的“Overton Pluralistic”可能指一种RL方法，但缺乏直接的应用背景说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20751v1": {
    "title": "SibylSense: Adaptive Rubric Learning via Memory Tuning and Adversarial Probing",
    "url": "https://www.alphaxiv.org/abs/2602.20751v1",
    "arxiv_id": "2602.20751v1",
    "authors": "Yifei Xu, Guilherme Potje, Shivam Shandilya, Tiancheng Yuan, Leonardo de Oliveira Nunes, Rakshanda Agarwal, Saeid Asgari, Adam Atkinson, Emre Kıcıman, Songwu Lu, Ranveer Chandra, Tusher Chakraborty",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2026-02-24 10:28:44",
    "ori_summary": "Designing aligned and robust rewards for open-ended generation remains a key barrier to RL post-training. Rubrics provide structured, interpretable supervision, but scaling rubric construction is difficult: expert rubrics are costly, prompted rubrics are often superficial or inconsistent, and fixed-pool discriminative rubrics can saturate and drift, enabling reward hacking. We present SibylSense, an inference-time learning approach that adapts a frozen rubric generator through a tunable memory bank of validated rubric items. Memory is updated via verifier-based item rewards measured by reference-candidate answer discriminative gaps from a handful of examples. SibylSense alternates memory tuning with a rubric-adversarial policy update that produces rubric-satisfying candidate answers, shrinking discriminative gaps and driving the rubric generator to capture new quality dimensions. Experiments on two open-ended tasks show that SibylSense yields more discriminative rubrics and improves downstream RL performance over static and non-adaptive baselines.",
    "summary": "",
    "translation": "SibylSense：通过记忆调优与对抗性探测实现自适应评分标准学习",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及自适应学习和对抗性探测技术，可能属于机器学习方法范畴，但未明确指向推荐系统、搜索或广告领域的具体应用。标题中的“评分标准学习”和“记忆调优”暗示其可能关注模型评估或优化技术，而非直接应用于排名、检索或广告投放等核心任务。尽管自适应学习技术具有通用性，但缺乏明确的跨模态建模、Transformer架构改进或LLM应用等焦点领域的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20749v1": {
    "title": "Explicit Grammar Semantic Feature Fusion for Robust Text Classification",
    "url": "https://www.alphaxiv.org/abs/2602.20749v1",
    "arxiv_id": "2602.20749v1",
    "authors": "Azrin Sultana, Firoz Ahmed",
    "categories": "cs.CL",
    "pub_date": "2026-02-24 10:25:29",
    "ori_summary": "Natural Language Processing enables computers to understand human language by analysing and classifying text efficiently with deep-level grammatical and semantic features. Existing models capture features by learning from large corpora with transformer models, which are computationally intensive and unsuitable for resource-constrained environments. Therefore, our proposed study incorporates comprehensive grammatical rules alongside semantic information to build a robust, lightweight classification model without resorting to full parameterised transformer models or heavy deep learning architectures. The novelty of our approach lies in its explicit encoding of sentence-level grammatical structure, including syntactic composition, phrase patterns, and complexity indicators, into a compact grammar vector, which is then fused with frozen contextual embeddings. These heterogeneous elements unified a single representation that captures both the structural and semantic characteristics of the text. Deep learning models such as Deep Belief Networks (DBNs), Long Short-Term Memory (LSTMs), BiLSTMs, and transformer-based BERT and XLNET were used to train and evaluate the model, with the number of epochs varied. Based on experimental results, the unified feature representation model captures both the semantic and structural properties of text, outperforming baseline models by 2%-15%, enabling more effective learning across heterogeneous domains. Unlike prior syntax-aware transformer models that inject grammatical structure through additional attention layers, tree encoders, or full fine-tuning, the proposed framework treats grammar as an explicit inductive bias rather than a learnable module, resulting in a very lightweight model that delivers better performance on edge devices",
    "summary": "",
    "translation": "用于鲁棒文本分类的显式语法语义特征融合",
    "relevance_score": 3,
    "reasoning": "该论文主要关注文本分类中的特征融合技术，属于传统NLP方法，与推荐系统、搜索或广告的核心技术关联较弱。虽然特征融合技术可能间接应用于文本理解，但论文未明确展示在推荐/搜索/广告领域的直接应用潜力，且不属于LLM、Transformer架构或异构数据建模等当前关注重点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20743v1": {
    "title": "Adaptive Text Anonymization: Learning Privacy-Utility Trade-offs via Prompt Optimization",
    "url": "https://www.alphaxiv.org/abs/2602.20743v1",
    "arxiv_id": "2602.20743v1",
    "authors": "Gabriel Loiseau, Damien Sileo, Damien Riquet, Maxime Meyer, Marc Tommasi",
    "categories": "cs.CL",
    "pub_date": "2026-02-24 10:12:40",
    "ori_summary": "Anonymizing textual documents is a highly context-sensitive problem: the appropriate balance between privacy protection and utility preservation varies with the data domain, privacy objectives, and downstream application. However, existing anonymization methods rely on static, manually designed strategies that lack the flexibility to adjust to diverse requirements and often fail to generalize across domains. We introduce adaptive text anonymization, a new task formulation in which anonymization strategies are automatically adapted to specific privacy-utility requirements. We propose a framework for task-specific prompt optimization that automatically constructs anonymization instructions for language models, enabling adaptation to different privacy goals, domains, and downstream usage patterns. To evaluate our approach, we present a benchmark spanning five datasets with diverse domains, privacy constraints, and utility objectives. Across all evaluated settings, our framework consistently achieves a better privacy-utility trade-off than existing baselines, while remaining computationally efficient and effective on open-source language models, with performance comparable to larger closed-source models. Additionally, we show that our method can discover novel anonymization strategies that explore different points along the privacy-utility trade-off frontier.",
    "summary": "",
    "translation": "自适应文本匿名化：通过提示优化学习隐私-效用权衡",
    "relevance_score": 1,
    "reasoning": "该论文涉及隐私保护技术，这属于明确列出的无关主题。虽然提到了提示优化，但核心焦点是隐私权衡，与推荐系统、搜索或广告的核心技术进展、LLM应用或Transformer架构改进无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20727v1": {
    "title": "ID-LoRA: Efficient Low-Rank Adaptation Inspired by Matrix Interpolative Decomposition",
    "url": "https://www.alphaxiv.org/abs/2602.20727v1",
    "arxiv_id": "2602.20727v1",
    "authors": "Xindian Ma, Rundong Kong, Peng Zhang, Ruoxiang Huang, Yongyu Jiang",
    "categories": "cs.CL",
    "pub_date": "2026-02-24 09:45:10",
    "ori_summary": "LoRA has become a universal Parameter-Efficient Fine-Tuning (PEFT) technique that equips Large Language Models (LLMs) to adapt quickly to new tasks. However, when these models are scaled up, even the latest LoRA variants still introduce considerable overhead in trainable parameters. Conversely, aggressively lowering the rank to curb this overhead markedly degrades performance in complex multi-task settings. We propose ID-LoRA, a novel PEFT framework that breaks the trade-off. Its core innovation lies in extracting and reusing clustered parameter groups from the pretrained weight matrix. These groups are then used to form multiple low-rank components, all of which share only a single initialized trainable low-rank matrix. This approach cuts the number of trainable parameters while keeping the model's capacity intact. We evaluate ID-LoRA on five diverse benchmarks: Mathematical Reasoning, Code Generation, MMLU, CommonsenseQA, and Safety Alignment. ID-LoRA outperforms both full fine-tuning and existing PEFT baselines (e.g., LoRA, DoRA, HydraLoRA) while using up to 46% fewer trainable parameters than the standard LoRA. In multi-task scenarios, it surpasses LoRA and its recent variants (e.g., DoRA and HydraLoRA) on both Code and MMLU tasks, yet requires only 54% of the trainable parameters demanded by the conventional LoRA.",
    "summary": "该论文研究大规模语言模型参数高效微调中低秩矩阵分解的参数量与性能权衡问题，核心思想是通过从预训练权重矩阵中提取聚类参数组来构建多个共享单一可训练矩阵的低秩分量，从而减少可训练参数数量同时保持模型容量。",
    "translation": "ID-LoRA：受矩阵插值分解启发的高效低秩自适应方法",
    "relevance_score": 8,
    "reasoning": "该论文属于'赋能LLM技术'类别，专注于通过低秩自适应（LoRA）提高LLM效率，这是Transformer架构优化的核心方向。高效的参数微调方法可直接应用于推荐系统、搜索和广告中的大规模LLM部署，降低计算成本并加速模型迭代。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的ID-LoRA方法直接改进Transformer架构的微调效率，属于核心的使能技术，对大规模推荐/搜索系统的部署成本优化有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.20710v1": {
    "title": "Counterfactual Simulation Training for Chain-of-Thought Faithfulness",
    "url": "https://www.alphaxiv.org/abs/2602.20710v1",
    "arxiv_id": "2602.20710v1",
    "authors": "Peter Hase, Christopher Potts",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2026-02-24 09:15:30",
    "ori_summary": "Inspecting Chain-of-Thought reasoning is among the most common means of understanding why an LLM produced its output. But well-known problems with CoT faithfulness severely limit what insights can be gained from this practice. In this paper, we introduce a training method called Counterfactual Simulation Training (CST), which aims to improve CoT faithfulness by rewarding CoTs that enable a simulator to accurately predict a model's outputs over counterfactual inputs. We apply CST in two settings: (1) CoT monitoring with cue-based counterfactuals, to detect when models rely on spurious features, reward hack, or are sycophantic, and (2) counterfactual simulation over generic model-based counterfactuals, to encourage models to produce more faithful, generalizable reasoning in the CoT. Experiments with models up to 235B parameters show that CST can substantially improve monitor accuracy on cue-based counterfactuals (by 35 accuracy points) as well as simulatability over generic counterfactuals (by 2 points). We further show that: (1) CST outperforms prompting baselines, (2) rewriting unfaithful CoTs with an LLM is 5x more efficient than RL alone, (3) faithfulness improvements do not generalize to dissuading cues (as opposed to persuading cues), and (4) larger models do not show more faithful CoT out of the box, but they do benefit more from CST. These results suggest that CST can improve CoT faithfulness in general, with promising applications for CoT monitoring. Code for experiments in this paper is available at https://github.com/peterbhase/counterfactual-simulation-training",
    "summary": "",
    "translation": "用于思维链忠实性的反事实模拟训练",
    "relevance_score": 2,
    "reasoning": "该论文关注思维链（Chain-of-Thought）的忠实性评估与训练，属于LLM推理可靠性的研究范畴。虽然思维链技术可能间接影响推荐/搜索系统中LLM的推理质量，但论文本身并未明确指向RecSys/Search/Ads的具体应用，且更偏向纯NLP领域的评估方法研究，与当前关注的直接应用或使能技术关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20670v1": {
    "title": "CAMEL: Confidence-Gated Reflection for Reward Modeling",
    "url": "https://www.alphaxiv.org/abs/2602.20670v1",
    "arxiv_id": "2602.20670v1",
    "authors": "Zirui Zhu, Hailun Xu, Yang Luo, Yong Liu, Kanchan Sarkar, Kun Xu, Yang You",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-02-24 08:20:08",
    "ori_summary": "Reward models play a fundamental role in aligning large language models with human preferences. Existing methods predominantly follow two paradigms: scalar discriminative preference models, which are efficient but lack interpretability, and generative judging models, which offer richer reasoning at the cost of higher computational overhead. We observe that the log-probability margin between verdict tokens strongly correlates with prediction correctness, providing a reliable proxy for instance difficulty without additional inference cost. Building on this insight, we propose CAMEL, a confidence-gated reflection framework that performs a lightweight single-token preference decision first and selectively invokes reflection only for low-confidence instances. To induce effective self-correction, we train the model via reinforcement learning with counterfactual prefix augmentation, which exposes the model to diverse initial verdicts and encourages genuine revision. Empirically, CAMEL achieves state-of-the-art performance on three widely used reward-model benchmarks with 82.9% average accuracy, surpassing the best prior model by 3.2% and outperforming 70B-parameter models using only 14B parameters, while establishing a strictly better accuracy-efficiency Pareto frontier.",
    "summary": "",
    "translation": "CAMEL：基于置信度门控的奖励模型反思机制",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及奖励建模中的置信度门控和反思机制，这主要属于强化学习（RL）领域的技术。虽然奖励建模在理论上可能与推荐系统或搜索相关，但标题没有明确表明其与RecSys/Search/Ads的具体应用关联。根据您的关注点，RL论文若无明确相关性则属于无关主题，因此评分较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20648v1": {
    "title": "CARE: An Explainable Computational Framework for Assessing Client-Perceived Therapeutic Alliance Using Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.20648v1",
    "arxiv_id": "2602.20648v1",
    "authors": "Anqi Li, Chenxiao Wang, Yu Lu, Renjun Xu, Lizhi Ma, Zhenzhong Lan",
    "categories": "cs.CL",
    "pub_date": "2026-02-24 07:52:56",
    "ori_summary": "Client perceptions of the therapeutic alliance are critical for counseling effectiveness. Accurately capturing these perceptions remains challenging, as traditional post-session questionnaires are burdensome and often delayed, while existing computational approaches produce coarse scores, lack interpretable rationales, and fail to model holistic session context. We present CARE, an LLM-based framework to automatically predict multi-dimensional alliance scores and generate interpretable rationales from counseling transcripts. Built on the CounselingWAI dataset and enriched with 9,516 expert-curated rationales, CARE is fine-tuned using rationale-augmented supervision with the LLaMA-3.1-8B-Instruct backbone. Experiments show that CARE outperforms leading LLMs and substantially reduces the gap between counselor evaluations and client-perceived alliance, achieving over 70% higher Pearson correlation with client ratings. Rationale-augmented supervision further improves predictive accuracy. CARE also produces high-quality, contextually grounded rationales, validated by both automatic and human evaluations. Applied to real-world Chinese online counseling sessions, CARE uncovers common alliance-building challenges, illustrates how interaction patterns shape alliance development, and provides actionable insights, demonstrating its potential as an AI-assisted tool for supporting mental health care.",
    "summary": "",
    "translation": "CARE：一种基于大语言模型评估客户感知治疗联盟的可解释计算框架",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医疗/心理学领域的应用（治疗联盟评估），属于明确的医学领域特定应用，这被列为不相关主题。虽然使用了LLMs，但应用场景与推荐系统、搜索或广告完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20647v1": {
    "title": "Semantic Novelty at Scale: Narrative Shape Taxonomy and Readership Prediction in 28,606 Books",
    "url": "https://www.alphaxiv.org/abs/2602.20647v1",
    "arxiv_id": "2602.20647v1",
    "authors": "W. Frederick Zimmerman",
    "categories": "cs.CL",
    "pub_date": "2026-02-24 07:52:35",
    "ori_summary": "I introduce semantic novelty--cosine distance between each paragraph's sentence embedding and the running centroid of all preceding paragraphs--as an information-theoretic measure of narrative structure at corpus scale. Applying it to 28,606 books in PG19 (pre-1920 English literature), I compute paragraph-level novelty curves using 768-dimensional SBERT embeddings, then reduce each to a 16-segment Piecewise Aggregate Approximation (PAA). Ward-linkage clustering on PAA vectors reveals eight canonical narrative shape archetypes, from Steep Descent (rapid convergence) to Steep Ascent (escalating unpredictability). Volume--variance of the novelty trajectory--is the strongest length-independent predictor of readership (partial rho = 0.32), followed by speed (rho = 0.19) and Terminal/Initial ratio (rho = 0.19). Circuitousness shows strong raw correlation (rho = 0.41) but is 93 percent correlated with length; after control, partial rho drops to 0.11--demonstrating that naive correlations in corpus studies can be dominated by length confounds. Genre strongly constrains narrative shape (chi squared = 2121.6, p < 10 to the power negative 242), with fiction maintaining plateau profiles while nonfiction front-loads information. Historical analysis shows books became progressively more predictable between 1840 and 1910 (T/I ratio trend r = negative 0.74, p = 0.037). SAX analysis reveals 85 percent signature uniqueness, suggesting each book traces a nearly unique path through semantic space. These findings demonstrate that information-density dynamics, distinct from sentiment or topic, constitute a fundamental dimension of narrative structure with measurable consequences for reader engagement. Dataset: https://huggingface.co/datasets/wfzimmerman/pg19-semantic-novelty",
    "summary": "",
    "translation": "语义新颖性的大规模研究：28,606本书籍中的叙事结构分类与读者阅读量预测",
    "relevance_score": 2,
    "reasoning": "该论文主要研究书籍的语义新颖性和读者预测，这属于内容分析和读者行为分析的范畴，与推荐系统、搜索或广告的核心技术（如排序、召回、用户建模）没有直接关联。虽然读者预测可能间接涉及用户行为建模，但论文聚焦于书籍内容和叙事结构分析，缺乏明确的推荐/搜索/广告应用场景或技术迁移路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20634v1": {
    "title": "Enhancing Hate Speech Detection on Social Media: A Comparative Analysis of Machine Learning Models and Text Transformation Approaches",
    "url": "https://www.alphaxiv.org/abs/2602.20634v1",
    "arxiv_id": "2602.20634v1",
    "authors": "Saurabh Mishra, Shivani Thakur, Radhika Mamidi",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-02-24 07:26:17",
    "ori_summary": "The proliferation of hate speech on social media platforms has necessitated the development of effective detection and moderation tools. This study evaluates the efficacy of various machine learning models in identifying hate speech and offensive language and investigates the potential of text transformation techniques to neutralize such content. We compare traditional models like CNNs and LSTMs with advanced neural network models such as BERT and its derivatives, alongside exploring hybrid models that combine different architectural features. Our results indicate that while advanced models like BERT show superior accuracy due to their deep contextual understanding, hybrid models exhibit improved capabilities in certain scenarios. Furthermore, we introduce innovative text transformation approaches that convert negative expressions into neutral ones, thereby potentially mitigating the impact of harmful content. The implications of these findings are discussed, highlighting the strengths and limitations of current technologies and proposing future directions for more robust hate speech detection systems.",
    "summary": "",
    "translation": "增强社交媒体仇恨言论检测：机器学习模型与文本转换方法的比较分析",
    "relevance_score": 1,
    "reasoning": "该论文专注于仇恨言论检测这一特定NLP任务，属于内容安全/内容审核领域，与搜索、推荐或广告系统的核心技术无关。论文标题中提到的机器学习模型比较和文本转换方法属于通用NLP技术，没有明确指向推荐系统、搜索或广告领域的应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20610v1": {
    "title": "SpecMind: Cognitively Inspired, Interactive Multi-Turn Framework for Postcondition Inference",
    "url": "https://www.alphaxiv.org/abs/2602.20610v1",
    "arxiv_id": "2602.20610v1",
    "authors": "Cuong Chi Le, Minh V. T Pham, Tung Vu Duy, Cuong Duc Van, Huy N. Phan, Hoang N. Phan, Tien N. Nguyen",
    "categories": "cs.SE, cs.CL",
    "pub_date": "2026-02-24 07:01:17",
    "ori_summary": "Specifications are vital for ensuring program correctness, yet writing them manually remains challenging and time-intensive. Recent large language model (LLM)-based methods have shown successes in generating specifications such as postconditions, but existing single-pass prompting often yields inaccurate results. In this paper, we present SpecMind, a novel framework for postcondition generation that treats LLMs as interactive and exploratory reasoners rather than one-shot generators. SpecMind employs feedback-driven multi-turn prompting approaches, enabling the model to iteratively refine candidate postconditions by incorporating implicit and explicit correctness feedback, while autonomously deciding when to stop. This process fosters deeper code comprehension and improves alignment with true program behavior via exploratory attempts. Our empirical evaluation shows that SpecMind significantly outperforms state-of-the-art approaches in both accuracy and completeness of generated postconditions.",
    "summary": "",
    "translation": "SpecMind：受认知启发的交互式多轮后置条件推断框架",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及程序验证中的后置条件推断，属于形式化方法领域，与推荐系统、搜索或广告的核心技术无直接关联。标题中的“认知启发”和“交互式多轮”框架暗示其专注于程序分析或软件工程任务，而非推荐、搜索或广告领域的技术进步。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20580v1": {
    "title": "Personal Information Parroting in Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.20580v1",
    "arxiv_id": "2602.20580v1",
    "authors": "Nishant Subramani, Kshitish Ghate, Mona Diab",
    "categories": "cs.CL, cs.AI, cs.CR, cs.LG",
    "pub_date": "2026-02-24 06:02:03",
    "ori_summary": "Modern language models (LM) are trained on large scrapes of the Web, containing millions of personal information (PI) instances, many of which LMs memorize, increasing privacy risks. In this work, we develop the regexes and rules (R&R) detector suite to detect email addresses, phone numbers, and IP addresses, which outperforms the best regex-based PI detectors. On a manually curated set of 483 instances of PI, we measure memorization: finding that 13.6% are parroted verbatim by the Pythia-6.9b model, i.e., when the model is prompted with the tokens that precede the PI in the original document, greedy decoding generates the entire PI span exactly. We expand this analysis to study models of varying sizes (160M-6.9B) and pretraining time steps (70k-143k iterations) in the Pythia model suite and find that both model size and amount of pretraining are positively correlated with memorization. Even the smallest model, Pythia-160m, parrots 2.7% of the instances exactly. Consequently, we strongly recommend that pretraining datasets be aggressively filtered and anonymized to minimize PI parroting.",
    "summary": "",
    "translation": "语言模型中的个人信息复述",
    "relevance_score": 2,
    "reasoning": "该标题涉及语言模型中的个人信息处理，这属于隐私相关话题，与您明确排除的隐私、安全等非技术性主题直接相关。虽然涉及语言模型，但未提及在推荐系统、搜索或广告中的潜在应用，也未涉及Transformer架构改进、多模态建模等您关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20574v1": {
    "title": "GATES: Self-Distillation under Privileged Context with Consensus Gating",
    "url": "https://www.alphaxiv.org/abs/2602.20574v1",
    "arxiv_id": "2602.20574v1",
    "authors": "Alex Stein, Furong Huang, Tom Goldstein",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2026-02-24 05:56:20",
    "ori_summary": "We study self-distillation in settings where supervision is unreliable: there are no ground truth labels, verifiable rewards, or external graders to evaluate answers. We focus on document-grounded question answering with asymmetric context, where a single model serves as both tutor (with access to a relevant source document during training) and student (answering from the question alone at test time). Rather than assuming tutor correctness, we derive supervision online from tutor consensus by sampling multiple document-grounded reasoning traces and using agreement to gate learning. Conditioned on this reliability signal, we distill knowledge through full tutor reasoning trajectories (not just final answers), providing a dense and stable learning signal. Empirically, this consensus-gated trajectory distillation substantially improves transfer to the document-free student. Held-out in-domain accuracy under asymmetric evaluation improves from 46.0\\% to 62.0\\%, and average (maj@8) accuracy on public document-free math benchmarks improves from 20.2\\% to 35.4\\%.",
    "summary": "该论文研究在监督不可靠（无真实标签）场景下的自蒸馏问题，核心思想是通过采样多个文档推理轨迹，利用它们之间的一致性作为门控信号，选择可靠的轨迹进行知识蒸馏，从而在文档缺失的测试条件下提升模型性能。",
    "translation": "GATES：基于特权上下文与共识门控的自蒸馏方法",
    "relevance_score": 8,
    "reasoning": "该论文涉及自蒸馏技术（属于模型压缩/知识蒸馏领域），这是LLM效率提升的重要方向，可直接应用于推荐/搜索系统中的模型部署优化。标题中的'特权上下文'和'共识门控'暗示了多模态或异构数据处理机制，与'VLM类比'焦点中处理异构数据作为不同模态的思路高度相关。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出了一种基于共识门控的自蒸馏方法，通过多文档推理轨迹的一致性来生成监督信号，这直接属于LLM应用中的知识蒸馏技术，对推荐/搜索系统中的模型训练有潜在价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.20532v1": {
    "title": "Actor-Curator: Co-adaptive Curriculum Learning via Policy-Improvement Bandits for RL Post-Training",
    "url": "https://www.alphaxiv.org/abs/2602.20532v1",
    "arxiv_id": "2602.20532v1",
    "authors": "Zhengyao Gu, Jonathan Light, Raul Astudillo, Ziyu Ye, Langzhou He, Henry Peng Zou, Wei Cheng, Santiago Paternain, Philip S. Yu, Yisong Yue",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2026-02-24 04:19:48",
    "ori_summary": "Post-training large foundation models with reinforcement learning typically relies on massive and heterogeneous datasets, making effective curriculum learning both critical and challenging. In this work, we propose ACTOR-CURATOR, a scalable and fully automated curriculum learning framework for reinforcement learning post-training of large language models (LLMs). ACTOR-CURATOR learns a neural curator that dynamically selects training problems from large problem banks by directly optimizing for expected policy performance improvement. We formulate problem selection as a non-stationary stochastic bandit problem, derive a principled loss function based on online stochastic mirror descent, and establish regret guarantees under partial feedback. Empirically, ACTOR-CURATOR consistently outperforms uniform sampling and strong curriculum baselines across a wide range of challenging reasoning benchmarks, demonstrating improved training stability and efficiency. Notably, it achieves relative gains of 28.6% on AIME2024 and 30.5% on ARC-1D over the strongest baseline and up to 80% speedup. These results suggest that ACTOR-CURATOR is a powerful and practical approach for scalable LLM post-training.",
    "summary": "",
    "translation": "Actor-Curator：基于策略改进赌博机的协同自适应课程学习用于强化学习后训练",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于强化学习（RL）的后训练方法，属于纯粹的强化学习技术范畴。虽然强化学习在推荐系统中有应用，但标题未提及任何与推荐、搜索或广告相关的具体应用场景或连接点，不符合当前关注点的任何类别。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20528v1": {
    "title": "Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning",
    "url": "https://www.alphaxiv.org/abs/2602.20528v1",
    "arxiv_id": "2602.20528v1",
    "authors": "Justin Lovelace, Christian Belardi, Sofian Zalouk, Adhitya Polavaram, Srivatsa Kundurthy, Kilian Q. Weinberger",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2026-02-24 04:09:31",
    "ori_summary": "The Stop-Think-AutoRegress Language Diffusion Model (STAR-LDM) integrates latent diffusion planning with autoregressive generation. Unlike conventional autoregressive language models limited to token-by-token decisions, STAR-LDM incorporates a \"thinking\" phase that pauses generation to refine a semantic plan through diffusion before continuing. This enables global planning in continuous space prior to committing to discrete tokens. Evaluations show STAR-LDM significantly outperforms similar-sized models on language understanding benchmarks and achieves $>70\\%$ win rates in LLM-as-judge comparisons for narrative coherence and commonsense reasoning. The architecture also allows straightforward control through lightweight classifiers, enabling fine-grained steering of attributes without model retraining while maintaining better fluency-control trade-offs than specialized approaches.",
    "summary": "该论文研究传统自回归语言模型缺乏全局规划能力的问题，核心思想是通过引入一个基于潜在扩散的“思考”阶段，在连续语义空间中进行全局规划后再生成离散标记，从而提升语言生成的连贯性和可控性。",
    "translation": "停止-思考-自回归：基于潜在扩散规划的语言建模",
    "relevance_score": 8,
    "reasoning": "该论文提出了一种结合潜在扩散规划的语言建模方法，属于'核心LLM技术进展'范畴。潜在扩散规划技术可以应用于推荐/搜索系统中的序列生成、内容规划或决策过程优化，例如生成更连贯的推荐理由或优化搜索结果的排序逻辑。",
    "rerank_relevance_score": 6,
    "rerank_reasoning": "该论文提出了一种结合扩散规划与自回归生成的新语言模型架构，属于核心LLM技术进步，但其在推荐、搜索或广告领域的直接应用潜力未在摘要中明确体现。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.20517v1": {
    "title": "Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination",
    "url": "https://www.alphaxiv.org/abs/2602.20517v1",
    "arxiv_id": "2602.20517v1",
    "authors": "Rakshit Trivedi, Kartik Sharma, David C Parkes",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2026-02-24 03:37:42",
    "ori_summary": "Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations. We open source our code and provide pre-trained MIMIC agents and qualitative demos at: https://mimic-research.github.io.",
    "summary": "",
    "translation": "内在言语作为行为引导：用于人机协调的可操控多样化行为模仿",
    "relevance_score": 2,
    "reasoning": "该论文主要关注人机协调中的行为模仿和内在言语引导，属于人机交互和强化学习领域。虽然涉及AI行为控制，但缺乏与推荐系统、搜索或广告排名的直接联系，也没有明确展示在Transformer架构、LLM技术或异构数据建模方面的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20513v1": {
    "title": "From Performance to Purpose: A Sociotechnical Taxonomy for Evaluating Large Language Model Utility",
    "url": "https://www.alphaxiv.org/abs/2602.20513v1",
    "arxiv_id": "2602.20513v1",
    "authors": "Gavin Levinson, Keith Feldman",
    "categories": "cs.CL",
    "pub_date": "2026-02-24 03:31:07",
    "ori_summary": "As large language models (LLMs) continue to improve at completing discrete tasks, they are being integrated into increasingly complex and diverse real-world systems. However, task-level success alone does not establish a model's fit for use in practice. In applied, high-stakes settings, LLM effectiveness is driven by a wider array of sociotechnical determinants that extend beyond conventional performance measures. Although a growing set of metrics capture many of these considerations, they are rarely organized in a way that supports consistent evaluation, leaving no unified taxonomy for assessing and comparing LLM utility across use cases. To address this gap, we introduce the Language Model Utility Taxonomy (LUX), a comprehensive framework that structures utility evaluation across four domains: performance, interaction, operations, and governance. Within each domain, LUX is organized hierarchically into thematically aligned dimensions and components, each grounded in metrics that enable quantitative comparison and alignment of model selection with intended use. In addition, an external dynamic web tool is provided to support exploration of the framework by connecting each component to a repository of relevant metrics (factors) for applied evaluation.",
    "summary": "",
    "translation": "从性能到目的：评估大型语言模型实用性的社会技术分类法",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于LLM的评估分类法和社会技术维度，属于纯粹的LLM评估和伦理范畴。根据用户明确的排除标准，这属于'Hallucination, Evaluation benchmarks, or other purely NLP-centric topics'的无关主题，与RecSys/Search/Ads的技术应用或架构进步无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20459v1": {
    "title": "PreScience: A Benchmark for Forecasting Scientific Contributions",
    "url": "https://www.alphaxiv.org/abs/2602.20459v1",
    "arxiv_id": "2602.20459v1",
    "authors": "Anirudh Ajith, Amanpreet Singh, Jay DeYoung, Nadav Kunievsky, Austin C. Kozlowski, Oyvind Tafjord, James Evans, Daniel S. Weld, Tom Hope, Doug Downey",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2026-02-24 01:37:53",
    "ori_summary": "Can AI systems trained on the scientific record up to a fixed point in time forecast the scientific advances that follow? Such a capability could help researchers identify collaborators and impactful research directions, and anticipate which problems and methods will become central next. We introduce PreScience -- a scientific forecasting benchmark that decomposes the research process into four interdependent generative tasks: collaborator prediction, prior work selection, contribution generation, and impact prediction. PreScience is a carefully curated dataset of 98K recent AI-related research papers, featuring disambiguated author identities, temporally aligned scholarly metadata, and a structured graph of companion author publication histories and citations spanning 502K total papers. We develop baselines and evaluations for each task, including LACERScore, a novel LLM-based measure of contribution similarity that outperforms previous metrics and approximates inter-annotator agreement. We find substantial headroom remains in each task -- e.g. in contribution generation, frontier LLMs achieve only moderate similarity to the ground-truth (GPT-5, averages 5.6 on a 1-10 scale). When composed into a 12-month end-to-end simulation of scientific production, the resulting synthetic corpus is systematically less diverse and less novel than human-authored research from the same period.",
    "summary": "",
    "translation": "PreScience：一个用于预测科学贡献的基准",
    "relevance_score": 1,
    "reasoning": "该论文标题表明其专注于科学贡献预测的基准测试，这属于纯粹的学术评估领域，与推荐系统、搜索或广告的核心技术进展、LLM应用或Transformer架构改进没有直接关联。它不涉及任何异构数据建模、效率提升或实际应用场景，因此与您的关注点高度不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20449v1": {
    "title": "Protein Language Models Diverge from Natural Language: Comparative Analysis and Improved Inference",
    "url": "https://www.alphaxiv.org/abs/2602.20449v1",
    "arxiv_id": "2602.20449v1",
    "authors": "Anna Hart, Chi Han, Jeonghwan Kim, Huimin Zhao, Heng Ji",
    "categories": "cs.LG, cs.AI, cs.CL, q-bio.BM",
    "pub_date": "2026-02-24 01:18:30",
    "ori_summary": "Modern Protein Language Models (PLMs) apply transformer-based model architectures from natural language processing to biological sequences, predicting a variety of protein functions and properties. However, protein language has key differences from natural language, such as a rich functional space despite a vocabulary of only 20 amino acids. These differences motivate research into how transformer-based architectures operate differently in the protein domain and how we can better leverage PLMs to solve protein-related tasks. In this work, we begin by directly comparing how the distribution of information stored across layers of attention heads differs between the protein and natural language domain. Furthermore, we adapt a simple early-exit technique-originally used in the natural language domain to improve efficiency at the cost of performance-to achieve both increased accuracy and substantial efficiency gains in protein non-structural property prediction by allowing the model to automatically select protein representations from the intermediate layers of the PLMs for the specific task and protein at hand. We achieve performance gains ranging from 0.4 to 7.01 percentage points while simultaneously improving efficiency by over 10 percent across models and non-structural prediction tasks. Our work opens up an area of research directly comparing how language models change behavior when moved into the protein domain and advances language modeling in biological domains.",
    "summary": "",
    "translation": "蛋白质语言模型与自然语言模型的差异：比较分析与改进推理",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于蛋白质语言模型，这属于生物学/化学领域的特定应用，属于明确的无关主题范畴。虽然论文涉及语言模型分析，但其核心应用领域与推荐系统、搜索或广告无关，也没有提及任何可能应用于这些领域的潜在技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20433v1": {
    "title": "Disentangling Geometry, Performance, and Training in Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.20433v1",
    "arxiv_id": "2602.20433v1",
    "authors": "Atharva Kulkarni, Jacob Mitchell Springer, Arjun Subramonian, Swabha Swayamdipta",
    "categories": "cs.CL",
    "pub_date": "2026-02-24 00:31:04",
    "ori_summary": "Geometric properties of Transformer weights, particularly the unembedding matrix, have been widely useful in language model interpretability research. Yet, their utility for estimating downstream performance remains unclear. In this work, we systematically investigate the relationship between model performance and the unembedding matrix geometry, particularly its effective rank. Our experiments, involving a suite of 108 OLMo-style language models trained under controlled variation, reveal several key findings. While the best-performing models often exhibit a high effective rank, this trend is not universal across tasks and training setups. Contrary to prior work, we find that low effective rank does not cause late-stage performance degradation in small models, but instead co-occurs with it; we find adversarial cases where low-rank models do not exhibit saturation. Moreover, we show that effective rank is strongly influenced by pre-training hyperparameters, such as batch size and weight decay, which in-turn affect the model's performance. Lastly, extending our analysis to other geometric metrics and final-layer representation, we find that these metrics are largely aligned, but none can reliably predict downstream performance. Overall, our findings suggest that the model's geometry, as captured by existing metrics, primarily reflects training choices rather than performance.",
    "summary": "该论文研究Transformer语言模型中解嵌入矩阵几何特性（特别是有效秩）与下游任务性能之间的关系。核心发现是现有几何指标主要反映训练超参数选择（如批大小、权重衰减），而非可靠预测性能，揭示了训练过程对模型内部结构的影响机制。",
    "translation": "解耦语言模型的几何结构、性能与训练过程",
    "relevance_score": 8,
    "reasoning": "该论文探讨语言模型内部工作机制的解耦分析，属于核心LLM技术进展范畴。理解语言模型的几何结构、性能表现和训练动态之间的分离关系，对于优化推荐/搜索系统中的模型效率、解释性和适应性具有直接应用潜力，能够帮助设计更有效的检索和排序机制。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文系统研究Transformer权重几何特性（如解嵌入矩阵有效秩）与下游性能的关系，直接关联Transformer架构分析与模型训练优化，对理解模型内在机制有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.21204v1": {
    "title": "Test-Time Training with KV Binding Is Secretly Linear Attention",
    "url": "https://www.alphaxiv.org/abs/2602.21204v1",
    "arxiv_id": "2602.21204v1",
    "authors": "Junchen Liu, Sven Elflein, Or Litany, Zan Gojcic, Ruilong Li",
    "categories": "cs.LG, cs.AI, cs.CV",
    "pub_date": "2026-02-24 18:59:30",
    "ori_summary": "Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.",
    "summary": "该论文研究测试时训练（TTT）中KV绑定机制的本质解释问题，其核心思想是通过理论分析证明，一类广泛的TTT架构可被重新表述为一种学习的线性注意力算子，从而提供了更高效且可并行化的计算视角。",
    "translation": "基于键值绑定的测试时训练本质上是线性注意力机制",
    "relevance_score": 8,
    "reasoning": "该论文探讨测试时训练与键值绑定技术，揭示了其本质是线性注意力机制，这直接属于'使能Transformer技术'范畴，涉及注意力机制效率优化。线性注意力机制在推荐系统和搜索中具有重要应用潜力，能够显著提升大规模序列建模的效率，特别是在处理长用户行为序列时。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文揭示了测试时训练（TTT）与KV绑定的本质是线性注意力，这直接关联到Transformer架构的效率优化和新注意力机制研究，对推荐系统和搜索中的序列建模有潜在应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.21203v1": {
    "title": "Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics",
    "url": "https://www.alphaxiv.org/abs/2602.21203v1",
    "arxiv_id": "2602.21203v1",
    "authors": "Abdulaziz Almuzairee, Henrik I. Christensen",
    "categories": "cs.RO, cs.CV, cs.LG",
    "pub_date": "2026-02-24 18:58:11",
    "ori_summary": "Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. Extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. To address these challenges, we introduce Squint, a visual Soft Actor Critic method that achieves faster wall-clock training than prior visual off-policy and on-policy methods. Squint achieves this via parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. We evaluate on the SO-101 Task Set, a new suite of eight manipulation tasks in ManiSkill3 with heavy domain randomization, and demonstrate sim-to-real transfer to a real SO-101 robot. We train policies for 15 minutes on a single RTX 3090 GPU, with most tasks converging in under 6 minutes.",
    "summary": "",
    "translation": "Squint：面向仿真到现实机器人的快速视觉强化学习",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及视觉强化学习（Visual Reinforcement Learning）和机器人学（Robotics），这属于纯粹视觉和机器人领域的应用，与推荐系统、搜索或广告的核心技术焦点无关。标题中未提及任何与推荐、搜索、广告相关的技术或潜在应用，因此完全不符合当前关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21195v1": {
    "title": "Region of Interest Segmentation and Morphological Analysis for Membranes in Cryo-Electron Tomography",
    "url": "https://www.alphaxiv.org/abs/2602.21195v1",
    "arxiv_id": "2602.21195v1",
    "authors": "Xingyi Cheng, Julien Maufront, Aurélie Di Cicco, Daniël M. Pelt, Manuela Dezi, Daniel Lévy",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 18:53:33",
    "ori_summary": "Cryo-electron tomography (cryo-ET) enables high resolution, three-dimensional reconstruction of biological structures, including membranes and membrane proteins. Identification of regions of interest (ROIs) is central to scientific imaging, as it enables isolation and quantitative analysis of specific structural features within complex datasets. In practice, however, ROIs are typically derived indirectly through full structure segmentation followed by post hoc analysis. This limitation is especially apparent for continuous and geometrically complex structures such as membranes, which are segmented as single entities. Here, we developed TomoROIS-SurfORA, a two step framework for direct, shape-agnostic ROI segmentation and morphological surface analysis. TomoROIS performs deep learning-based ROI segmentation and can be trained from scratch using small annotated datasets, enabling practical application across diverse imaging data. SurfORA processes segmented structures as point clouds and surface meshes to extract quantitative morphological features, including inter-membrane distances, curvature, and surface roughness. It supports both closed and open surfaces, with specific considerations for open surfaces, which are common in cryo-ET due to the missing wedge effect. We demonstrate both tools using in vitro reconstituted membrane systems containing deformable vesicles with complex geometries, enabling automatic quantitative analysis of membrane contact sites and remodeling events such as invagination. While demonstrated here on cryo-ET membrane data, the combined approach is applicable to ROI detection and surface analysis in broader scientific imaging contexts.",
    "summary": "",
    "translation": "冷冻电子断层扫描中膜结构的感兴趣区域分割与形态学分析",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及生物学领域的冷冻电子断层扫描技术，专注于膜结构的分割与分析，属于明确的生物医学应用范畴。根据用户指定的无关主题列表，这属于“Medical, Biology, Chemistry, Physics or other domain-specific applications”，与用户关注的推荐系统、搜索、广告、LLM或Transformer架构等核心领域完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21188v1": {
    "title": "Human Video Generation from a Single Image with 3D Pose and View Control",
    "url": "https://www.alphaxiv.org/abs/2602.21188v1",
    "arxiv_id": "2602.21188v1",
    "authors": "Tiantian Wang, Chun-Han Yao, Tao Hu, Mallikarjun Byrasandra Ramalinga Reddy, Ming-Hsuan Yang, Varun Jampani",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 18:42:20",
    "ori_summary": "Recent diffusion methods have made significant progress in generating videos from single images due to their powerful visual generation capabilities. However, challenges persist in image-to-video synthesis, particularly in human video generation, where inferring view-consistent, motion-dependent clothing wrinkles from a single image remains a formidable problem. In this paper, we present Human Video Generation in 4D (HVG), a latent video diffusion model capable of generating high-quality, multi-view, spatiotemporally coherent human videos from a single image with 3D pose and view control. HVG achieves this through three key designs: (i) Articulated Pose Modulation, which captures the anatomical relationships of 3D joints via a novel dual-dimensional bone map and resolves self-occlusions across views by introducing 3D information; (ii) View and Temporal Alignment, which ensures multi-view consistency and alignment between a reference image and pose sequences for frame-to-frame stability; and (iii) Progressive Spatio-Temporal Sampling with temporal alignment to maintain smooth transitions in long multi-view animations. Extensive experiments on image-to-video tasks demonstrate that HVG outperforms existing methods in generating high-quality 4D human videos from diverse human images and pose inputs.",
    "summary": "",
    "translation": "基于单张图像与三维姿态及视角控制的人类视频生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于从单张图像生成人类视频，属于计算机视觉和生成式AI领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然涉及生成技术，但主要应用于视频内容生成，而非推荐/搜索/广告的排序、匹配或理解任务，因此不符合当前关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21186v1": {
    "title": "Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning",
    "url": "https://www.alphaxiv.org/abs/2602.21186v1",
    "arxiv_id": "2602.21186v1",
    "authors": "Haoyi Jiang, Liu Liu, Xinjie Wang, Yonghao He, Wei Sui, Zhizhong Su, Wenyu Liu, Xinggang Wang",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 18:37:34",
    "ori_summary": "While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D space--a cornerstone of spatial intelligence--remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstructing holistic 3D geometry from sparse cues. In this paper, we argue that spatial intelligence can emerge inherently from 2D vision alone, rather than being imposed via explicit spatial instruction tuning. To this end, we introduce Spa3R, a self-supervised framework that learns a unified, view-invariant spatial representation directly from unposed multi-view images. Spa3R is built upon the proposed Predictive Spatial Field Modeling (PSFM) paradigm, where Spa3R learns to synthesize feature fields for arbitrary unseen views conditioned on a compact latent representation, thereby internalizing a holistic and coherent understanding of the underlying 3D scene. We further integrate the pre-trained Spa3R Encoder into existing VLMs via a lightweight adapter to form Spa3-VLM, effectively grounding language reasoning in a global spatial context. Experiments on the challenging VSI-Bench demonstrate that Spa3-VLM achieves state-of-the-art accuracy of 58.6% on 3D VQA, significantly outperforming prior methods. These results highlight PSFM as a scalable path toward advancing spatial intelligence. Code is available at https://github.com/hustvl/Spa3R.",
    "summary": "",
    "translation": "Spa3R：面向三维视觉推理的预测性空间场建模",
    "relevance_score": 2,
    "reasoning": "该论文标题明确聚焦于三维视觉推理和空间场建模，属于纯粹的计算机视觉领域。虽然空间建模技术在某些特定场景下可能间接辅助推荐或搜索系统（例如3D产品推荐），但论文本身没有表明与推荐系统、搜索或广告的直接关联，也不涉及LLM、Transformer架构或异构数据统一建模等当前关注的核心技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21179v1": {
    "title": "Mask-HybridGNet: Graph-based segmentation with emergent anatomical correspondence from pixel-level supervision",
    "url": "https://www.alphaxiv.org/abs/2602.21179v1",
    "arxiv_id": "2602.21179v1",
    "authors": "Nicolás Gaggion, Maria J. Ledesma-Carbayo, Stergios Christodoulidis, Maria Vakalopoulou, Enzo Ferrante",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 18:29:13",
    "ori_summary": "Graph-based medical image segmentation represents anatomical structures using boundary graphs, providing fixed-topology landmarks and inherent population-level correspondences. However, their clinical adoption has been hindered by a major requirement: training datasets with manually annotated landmarks that maintain point-to-point correspondences across patients rarely exist in practice. We introduce Mask-HybridGNet, a framework that trains graph-based models directly using standard pixel-wise masks, eliminating the need for manual landmark annotations. Our approach aligns variable-length ground truth boundaries with fixed-length landmark predictions by combining Chamfer distance supervision and edge-based regularization to ensure local smoothness and regular landmark distribution, further refined via differentiable rasterization. A significant emergent property of this framework is that predicted landmark positions become consistently associated with specific anatomical locations across patients without explicit correspondence supervision. This implicit atlas learning enables temporal tracking, cross-slice reconstruction, and morphological population analyses. Beyond direct segmentation, Mask-HybridGNet can extract correspondences from existing segmentation masks, allowing it to generate stable anatomical atlases from any high-quality pixel-based model. Experiments across chest radiography, cardiac ultrasound, cardiac MRI, and fetal imaging demonstrate that our model achieves competitive results against state-of-the-art pixel-based methods, while ensuring anatomical plausibility by enforcing boundary connectivity through a fixed graph adjacency matrix. This framework leverages the vast availability of standard segmentation masks to build structured models that maintain topological integrity and provide implicit correspondences.",
    "summary": "",
    "translation": "Mask-HybridGNet：基于图的分割方法，通过像素级监督实现涌现的解剖学对应关系",
    "relevance_score": 1,
    "reasoning": "该论文标题表明研究重点是基于图的医学图像分割方法，属于计算机视觉中的特定领域应用。虽然涉及图神经网络技术，但核心是医学图像分析和解剖学对应关系，与推荐系统、搜索、广告等商业应用领域没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21178v1": {
    "title": "XMorph: Explainable Brain Tumor Analysis Via LLM-Assisted Hybrid Deep Intelligence",
    "url": "https://www.alphaxiv.org/abs/2602.21178v1",
    "arxiv_id": "2602.21178v1",
    "authors": "Sepehr Salem Ghahfarokhi, M. Moein Esfahani, Raj Sunderraman, Vince Calhoun, Mohammed Alser",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-24 18:28:08",
    "ori_summary": "Deep learning has significantly advanced automated brain tumor diagnosis, yet clinical adoption remains limited by interpretability and computational constraints. Conventional models often act as opaque ''black boxes'' and fail to quantify the complex, irregular tumor boundaries that characterize malignant growth. To address these challenges, we present XMorph, an explainable and computationally efficient framework for fine-grained classification of three prominent brain tumor types: glioma, meningioma, and pituitary tumors. We propose an Information-Weighted Boundary Normalization (IWBN) mechanism that emphasizes diagnostically relevant boundary regions alongside nonlinear chaotic and clinically validated features, enabling a richer morphological representation of tumor growth. A dual-channel explainable AI module combines GradCAM++ visual cues with LLM-generated textual rationales, translating model reasoning into clinically interpretable insights. The proposed framework achieves a classification accuracy of 96.0%, demonstrating that explainability and high performance can co-exist in AI-based medical imaging systems. The source code and materials for XMorph are all publicly available at: https://github.com/ALSER-Lab/XMorph.",
    "summary": "",
    "translation": "XMorph：基于大语言模型辅助混合深度智能的可解释脑肿瘤分析",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医学领域（脑肿瘤分析），属于明确的无关主题。虽然提到了LLM技术，但应用场景是医疗诊断而非推荐系统、搜索或广告领域。该研究没有展示任何与RecSys/Search/Ads相关的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21175v1": {
    "title": "Seeing Through Words: Controlling Visual Retrieval Quality with Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.21175v1",
    "arxiv_id": "2602.21175v1",
    "authors": "Jianglin Lu, Simon Jenni, Kushal Kafle, Jing Shi, Handong Zhao, Yun Fu",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 18:20:57",
    "ori_summary": "Text-to-image retrieval is a fundamental task in vision-language learning, yet in real-world scenarios it is often challenged by short and underspecified user queries. Such queries are typically only one or two words long, rendering them semantically ambiguous, prone to collisions across diverse visual interpretations, and lacking explicit control over the quality of retrieved images. To address these issues, we propose a new paradigm of quality-controllable retrieval, which enriches short queries with contextual details while incorporating explicit notions of image quality. Our key idea is to leverage a generative language model as a query completion function, extending underspecified queries into descriptive forms that capture fine-grained visual attributes such as pose, scene, and aesthetics. We introduce a general framework that conditions query completion on discretized quality levels, derived from relevance and aesthetic scoring models, so that query enrichment is not only semantically meaningful but also quality-aware. The resulting system provides three key advantages: 1) flexibility, it is compatible with any pretrained vision-language model (VLMs) without modification; 2) transparency, enriched queries are explicitly interpretable by users; and 3) controllability, enabling retrieval results to be steered toward user-preferred quality levels. Extensive experiments demonstrate that our proposed approach significantly improves retrieval results and provides effective quality control, bridging the gap between the expressive capacity of modern VLMs and the underspecified nature of short user queries. Our code is available at https://github.com/Jianglin954/QCQC.",
    "summary": "该论文研究短文本查询导致的图像检索质量不可控问题。核心思想是利用生成式语言模型将短查询扩展为包含细粒度视觉属性和质量描述的详细查询，从而实现对检索结果质量的显式控制。",
    "translation": "透过文字看本质：利用语言模型控制视觉检索质量",
    "relevance_score": 6,
    "reasoning": "该论文涉及语言模型在视觉检索任务中的应用，属于跨模态技术，与'VLM Analogy for Heterogeneous Data'焦点相关，可用于处理搜索和推荐中的多模态数据。然而，其核心是视觉检索，可能更偏向纯粹的视觉任务，而非直接针对推荐/搜索/广告的异构数据建模。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出利用生成式语言模型增强短查询的视觉检索质量控制，直接应用于搜索领域，并借鉴了多模态建模思想。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.21172v1": {
    "title": "NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning",
    "url": "https://www.alphaxiv.org/abs/2602.21172v1",
    "arxiv_id": "2602.21172v1",
    "authors": "Ishaan Rawal, Shubh Gupta, Yihan Hu, Wei Zhan",
    "categories": "cs.AI, cs.CV",
    "pub_date": "2026-02-24 18:17:21",
    "ori_summary": "Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with \\modelname (\\textbf{No} \\textbf{R}easoning for \\textbf{D}riving). Compared to existing VLAs, \\modelname achieves competitive performance while being fine-tuned on $<$60\\% of the data and no reasoning annotations, resulting in 3$\\times$ fewer tokens. We identify that standard Group Relative Policy Optimization (GRPO) fails to yield significant improvements when applied to policies trained on such small, reasoning-free datasets. We show that this limitation stems from difficulty bias, which disproportionately penalizes reward signals from scenarios that produce high-variance rollouts within GRPO. \\modelname overcomes this by incorporating Dr.~GRPO, a recent algorithm designed to mitigate difficulty bias in LLMs. As a result, \\modelname achieves competitive performance on Waymo and NAVSIM with a fraction of the training data and no reasoning overhead, enabling more efficient autonomous systems.",
    "summary": "",
    "translation": "NoRD：一种无需推理即可驾驶的数据高效视觉-语言-动作模型",
    "relevance_score": 2,
    "reasoning": "虽然该论文涉及视觉-语言模型（VLM）和多模态学习，但其核心应用是自动驾驶领域，这属于明确的无关主题（\"Purely Vision... without clear relevance to RecSys/Search/Ads\"）。论文标题强调\"Drives without Reasoning\"，表明其专注于机器人控制任务，而非推荐系统、搜索或广告中的异构数据统一建模应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21153v1": {
    "title": "SPRITETOMESH: Automatic Mesh Generation for 2D Skeletal Animation Using Learned Segmentation and Contour-Aware Vertex Placement",
    "url": "https://www.alphaxiv.org/abs/2602.21153v1",
    "arxiv_id": "2602.21153v1",
    "authors": "Bastien Gimbert",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 17:58:31",
    "ori_summary": "We present SPRITETOMESH, a fully automatic pipeline for converting 2D game sprite images into triangle meshes compatible with skeletal animation frameworks such as Spine2D. Creating animation-ready meshes is traditionally a tedious manual process requiring artists to carefully place vertices along visual boundaries, a task that typically takes 15-60 minutes per sprite. Our method addresses this through a hybrid learned-algorithmic approach. A segmentation network (EfficientNet-B0 encoder with U-Net decoder) trained on over 100,000 sprite-mask pairs from 172 games achieves an IoU of 0.87, providing accurate binary masks from arbitrary input images. From these masks, we extract exterior contour vertices using Douglas-Peucker simplification with adaptive arc subdivision, and interior vertices along visual boundaries detected via bilateral-filtered multi-channel Canny edge detection with contour-following placement. Delaunay triangulation with mask-based centroid filtering produces the final mesh. Through controlled experiments, we demonstrate that direct vertex position prediction via neural network heatmap regression is fundamentally not viable for this task: the heatmap decoder consistently fails to converge (loss plateau at 0.061) while the segmentation decoder trains normally under identical conditions. We attribute this to the inherently artistic nature of vertex placement - the same sprite can be meshed validly in many different ways. This negative result validates our hybrid design: learned segmentation where ground truth is unambiguous, algorithmic placement where domain heuristics are appropriate. The complete pipeline processes a sprite in under 3 seconds, representing a speedup of 300x-1200x over manual creation. We release our trained model to the game development community.",
    "summary": "",
    "translation": "SPRITETOMESH：基于学习型分割和轮廓感知顶点放置的二维骨骼动画自动网格生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机图形学中的网格生成技术，属于纯粹的视觉/图形领域，与推荐系统、搜索或广告的核心技术栈无直接关联。论文标题中提到的“2D Skeletal Animation”、“Mesh Generation”和“Segmentation”等技术，在您的关注领域中缺乏明确的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21142v1": {
    "title": "LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis",
    "url": "https://www.alphaxiv.org/abs/2602.21142v1",
    "arxiv_id": "2602.21142v1",
    "authors": "Zhifan Jiang, Dong Yang, Vishwesh Nath, Abhijeet Parida, Nishad P. Kulkarni, Ziyue Xu, Daguang Xu, Syed Muhammad Anwar, Holger R. Roth, Marius George Linguraru",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-02-24 17:42:46",
    "ori_summary": "Large vision-language models (VLMs) have evolved from general-purpose applications to specialized use cases such as in the clinical domain, demonstrating potential for decision support in radiology. One promising application is assisting radiologists in decision-making by the analysis of radiology imaging data such as chest X-rays (CXR) via a visual and natural language question-answering (VQA) interface. When longitudinal imaging is available, radiologists analyze temporal changes, which are essential for accurate diagnosis and prognosis. The manual longitudinal analysis is a time-consuming process, motivating the development of a training framework that can provide prognostic capabilities. We introduce a novel training framework LUMEN, that is optimized for longitudinal CXR interpretation, leveraging multi-image and multi-task instruction fine-tuning to enhance prognostic and diagnostic performance. We conduct experiments on the publicly available MIMIC-CXR and its associated Medical-Diff-VQA datasets. We further formulate and construct a novel instruction-following dataset incorporating longitudinal studies, enabling the development of a prognostic VQA task. Our method demonstrates significant improvements over baseline models in diagnostic VQA tasks, and more importantly, shows promising potential for prognostic capabilities. These results underscore the value of well-designed, instruction-tuned VLMs in enabling more accurate and clinically meaningful radiological interpretation of longitudinal radiological imaging data.",
    "summary": "",
    "translation": "LUMEN：用于预后与诊断的纵向多模态放射学模型",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医学放射学领域（radiology），属于您指定的不相关主题中的“Medical, Biology, Chemistry, Physics or other domain-specific applications”。尽管提到了“Multi-Modal”和“Longitudinal”，但其应用场景（预后与诊断）与推荐系统、搜索或广告的核心技术进展、使能技术或直接应用完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21141v1": {
    "title": "SynthRender and IRIS: Open-Source Framework and Dataset for Bidirectional Sim-Real Transfer in Industrial Object Perception",
    "url": "https://www.alphaxiv.org/abs/2602.21141v1",
    "arxiv_id": "2602.21141v1",
    "authors": "Jose Moises Araya-Martinez, Thushar Tom, Adrián Sanchis Reig, Pablo Rey Valiente, Jens Lambrecht, Jörg Krüger",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 17:42:34",
    "ori_summary": "Object perception is fundamental for tasks such as robotic material handling and quality inspection. However, modern supervised deep-learning perception models require large datasets for robust automation under semi-uncontrolled conditions. The cost of acquiring and annotating such data for proprietary parts is a major barrier for widespread deployment. In this context, we release SynthRender, an open source framework for synthetic image generation with Guided Domain Randomization capabilities. Furthermore, we benchmark recent Reality-to-Simulation techniques for 3D asset creation from 2D images of real parts. Combined with Domain Randomization, these synthetic assets provide low-overhead, transferable data even for parts lacking 3D files. We also introduce IRIS, the Industrial Real-Sim Imagery Set, containing 32 categories with diverse textures, intra-class variation, strong inter-class similarities and about 20,000 labels. Ablations on multiple benchmarks outline guidelines for efficient data generation with SynthRender. Our method surpasses existing approaches, achieving 99.1% mAP@50 on a public robotics dataset, 98.3% mAP@50 on an automotive benchmark, and 95.3% mAP@50 on IRIS.",
    "summary": "",
    "translation": "SynthRender与IRIS：面向工业物体感知双向仿真-现实迁移的开源框架与数据集",
    "relevance_score": 2,
    "reasoning": "该论文标题聚焦于工业物体感知的仿真-现实迁移，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然仿真数据生成技术在某些场景下可能辅助数据增强，但标题未表明其在RecSys/Search/Ads中的具体应用潜力，且主要涉及视觉感知而非用户行为建模或排序任务。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21137v1": {
    "title": "UDVideoQA: A Traffic Video Question Answering Dataset for Multi-Object Spatio-Temporal Reasoning in Urban Dynamics",
    "url": "https://www.alphaxiv.org/abs/2602.21137v1",
    "arxiv_id": "2602.21137v1",
    "authors": "Joseph Raj Vishal, Nagasiri Poluri, Katha Naik, Rutuja Patil, Kashyap Hegde Kota, Krishna Vinod, Prithvi Jai Ramesh, Mohammad Farhadi, Yezhou Yang, Bharatesh Chakravarthi",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 17:33:12",
    "ori_summary": "Understanding the complex, multi-agent dynamics of urban traffic remains a fundamental challenge for video language models. This paper introduces Urban Dynamics VideoQA, a benchmark dataset that captures the unscripted real-world behavior of dynamic urban scenes. UDVideoQA is curated from 16 hours of traffic footage recorded at multiple city intersections under diverse traffic, weather, and lighting conditions. It employs an event-driven dynamic blur technique to ensure privacy preservation without compromising scene fidelity. Using a unified annotation pipeline, the dataset contains 28K question-answer pairs generated across 8 hours of densely annotated video, averaging one question per second. Its taxonomy follows a hierarchical reasoning level, spanning basic understanding and attribution to event reasoning, reverse reasoning, and counterfactual inference, enabling systematic evaluation of both visual grounding and causal reasoning. Comprehensive experiments benchmark 10 SOTA VideoLMs on UDVideoQA and 8 models on a complementary video question generation benchmark. Results reveal a persistent perception-reasoning gap, showing models that excel in abstract inference often fail with fundamental visual grounding. While models like Gemini Pro achieve the highest zero-shot accuracy, fine-tuning the smaller Qwen2.5-VL 7B model on UDVideoQA bridges this gap, achieving performance comparable to proprietary systems. In VideoQGen, Gemini 2.5 Pro, and Qwen3 Max generate the most relevant and complex questions, though all models exhibit limited linguistic diversity, underscoring the need for human-centric evaluation. The UDVideoQA suite, including the dataset, annotation tools, and benchmarks for both VideoQA and VideoQGen, provides a foundation for advancing robust, privacy-aware, and real-world multimodal reasoning. UDVideoQA is available at https://ud-videoqa.github.io/UD-VideoQA/UD-VideoQA/.",
    "summary": "",
    "translation": "UDVideoQA：面向城市动态多目标时空推理的交通视频问答数据集",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向交通视频问答数据集，属于纯粹的计算机视觉领域研究，专注于视频理解和时空推理。虽然涉及多模态数据，但缺乏与推荐系统、搜索或广告领域的明确关联，且未提及任何Transformer架构、LLM技术或其在推荐/搜索/广告中的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21105v1": {
    "title": "BrepGaussian: CAD reconstruction from Multi-View Images with Gaussian Splatting",
    "url": "https://www.alphaxiv.org/abs/2602.21105v1",
    "arxiv_id": "2602.21105v1",
    "authors": "Jiaxing Yu, Dongyang Ren, Hangyu Xu, Zhouyuxiao Yang, Yuanqi Li, Jie Guo, Zhengkang Zhou, Yanwen Guo",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 17:03:45",
    "ori_summary": "The boundary representation (B-rep) models a 3D solid as its explicit boundaries: trimmed corners, edges, and faces. Recovering B-rep representation from unstructured data is a challenging and valuable task of computer vision and graphics. Recent advances in deep learning have greatly improved the recovery of 3D shape geometry, but still depend on dense and clean point clouds and struggle to generalize to novel shapes. We propose B-rep Gaussian Splatting (BrepGaussian), a novel framework that learns 3D parametric representations from 2D images. We employ a Gaussian Splatting renderer with learnable features, followed by a specific fitting strategy. To disentangle geometry reconstruction and feature learning, we introduce a two-stage learning framework that first captures geometry and edges and then refines patch features to achieve clean geometry and coherent instance representations. Extensive experiments demonstrate the superior performance of our approach to state-of-the-art methods. We will release our code and datasets upon acceptance.",
    "summary": "",
    "translation": "BrepGaussian：基于高斯泼溅的多视角图像CAD重建",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及计算机辅助设计（CAD）重建和3D视觉技术（高斯泼溅），属于纯粹的3D视觉/图形学领域。虽然标题提到“多视角图像”，但这与推荐系统、搜索或广告的核心技术（如排序、检索、用户建模）没有直接关联。没有证据表明该技术能应用于异构数据处理、Transformer架构改进或LLM在推荐/搜索领域的直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21101v1": {
    "title": "Event-Aided Sharp Radiance Field Reconstruction for Fast-Flying Drones",
    "url": "https://www.alphaxiv.org/abs/2602.21101v1",
    "arxiv_id": "2602.21101v1",
    "authors": "Rong Zou, Marco Cannici, Davide Scaramuzza",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2026-02-24 17:02:56",
    "ori_summary": "Fast-flying aerial robots promise rapid inspection under limited battery constraints, with direct applications in infrastructure inspection, terrain exploration, and search and rescue. However, high speeds lead to severe motion blur in images and induce significant drift and noise in pose estimates, making dense 3D reconstruction with Neural Radiance Fields (NeRFs) particularly challenging due to their high sensitivity to such degradations. In this work, we present a unified framework that leverages asynchronous event streams alongside motion-blurred frames to reconstruct high-fidelity radiance fields from agile drone flights. By embedding event-image fusion into NeRF optimization and jointly refining event-based visual-inertial odometry priors using both event and frame modalities, our method recovers sharp radiance fields and accurate camera trajectories without ground-truth supervision. We validate our approach on both synthetic data and real-world sequences captured by a fast-flying drone. Despite highly dynamic drone flights, where RGB frames are severely degraded by motion blur and pose priors become unreliable, our method reconstructs high-fidelity radiance fields and preserves fine scene details, delivering a performance gain of over 50% on real-world data compared to state-of-the-art methods.",
    "summary": "",
    "translation": "面向快速飞行无人机的基于事件辅助的锐利辐射场重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于无人机视觉和3D重建领域，主要涉及计算机视觉中的辐射场技术和事件相机应用。虽然标题提到“快速飞行”，但这属于机器人视觉和SLAM范畴，与推荐系统、搜索或广告的核心技术领域没有直接关联。论文内容明显属于纯粹的视觉/3D视觉研究，没有展示任何与异构数据处理、Transformer架构或LLM技术相关的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21100v1": {
    "title": "Skullptor: High Fidelity 3D Head Reconstruction in Seconds with Multi-View Normal Prediction",
    "url": "https://www.alphaxiv.org/abs/2602.21100v1",
    "arxiv_id": "2602.21100v1",
    "authors": "Noé Artru, Rukhshanda Hussain, Emeline Got, Alexandre Messier, David B. Lindell, Abdallah Dib",
    "categories": "cs.CV, cs.GR",
    "pub_date": "2026-02-24 17:02:11",
    "ori_summary": "Reconstructing high-fidelity 3D head geometry from images is critical for a wide range of applications, yet existing methods face fundamental limitations. Traditional photogrammetry achieves exceptional detail but requires extensive camera arrays (25-200+ views), substantial computation, and manual cleanup in challenging areas like facial hair. Recent alternatives present a fundamental trade-off: foundation models enable efficient single-image reconstruction but lack fine geometric detail, while optimization-based methods achieve higher fidelity but require dense views and expensive computation. We bridge this gap with a hybrid approach that combines the strengths of both paradigms. Our method introduces a multi-view surface normal prediction model that extends monocular foundation models with cross-view attention to produce geometrically consistent normals in a feed-forward pass. We then leverage these predictions as strong geometric priors within an inverse rendering optimization framework to recover high-frequency surface details. Our approach outperforms state-of-the-art single-image and multi-view methods, achieving high-fidelity reconstruction on par with dense-view photogrammetry while reducing camera requirements and computational cost. The code and model will be released.",
    "summary": "",
    "translation": "Skullptor：通过多视角法线预测在数秒内实现高保真度的三维头部重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于三维头部重建和计算机视觉，属于纯粹的视觉/三维视觉领域，与推荐系统、搜索或广告的核心技术焦点（如排序、召回、用户建模、内容理解或Transformer架构进展）没有直接关联。其技术（多视角法线预测、快速三维重建）主要应用于图形学、虚拟现实或生物识别，在RecSys/Search/Ads中缺乏明确的应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21098v1": {
    "title": "Optimizing Occupancy Sensor Placement in Smart Environments",
    "url": "https://www.alphaxiv.org/abs/2602.21098v1",
    "arxiv_id": "2602.21098v1",
    "authors": "Hao Lu, Richard J. Radke",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 17:01:36",
    "ori_summary": "Understanding the locations of occupants in a commercial built environment is critical for realizing energy savings by delivering lighting, heating, and cooling only where it is needed. The key to achieving this goal is being able to recognize zone occupancy in real time, without impeding occupants' activities or compromising privacy. While low-resolution, privacy-preserving time-of-flight (ToF) sensor networks have demonstrated good performance in zone counting, the performance depends on careful sensor placement. To address this issue, we propose an automatic sensor placement method that determines optimal sensor layouts for a given number of sensors, and can predict the counting accuracy of such a layout. In particular, given the geometric constraints of an office environment, we simulate a large number of occupant trajectories. We then formulate the sensor placement problem as an integer linear programming (ILP) problem and solve it with the branch and bound method. We demonstrate the effectiveness of the proposed method based on simulations of several different office environments.",
    "summary": "",
    "translation": "优化智能环境中的占用传感器布局",
    "relevance_score": 1,
    "reasoning": "该论文标题关注智能环境中的传感器布局优化，属于物联网或智能建筑领域。这与我的核心关注点（推荐系统、搜索、广告、LLM技术、Transformer架构或异构数据建模）没有直接关联，也不涉及任何可能应用于这些领域的使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21078v1": {
    "title": "ProxyFL: A Proxy-Guided Framework for Federated Semi-Supervised Learning",
    "url": "https://www.alphaxiv.org/abs/2602.21078v1",
    "arxiv_id": "2602.21078v1",
    "authors": "Duowen Chen, Yan Wang",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2026-02-24 16:41:16",
    "ori_summary": "Federated Semi-Supervised Learning (FSSL) aims to collaboratively train a global model across clients by leveraging partially-annotated local data in a privacy-preserving manner. In FSSL, data heterogeneity is a challenging issue, which exists both across clients and within clients. External heterogeneity refers to the data distribution discrepancy across different clients, while internal heterogeneity represents the mismatch between labeled and unlabeled data within clients. Most FSSL methods typically design fixed or dynamic parameter aggregation strategies to collect client knowledge on the server (external) and / or filter out low-confidence unlabeled samples to reduce mistakes in local client (internal). But, the former is hard to precisely fit the ideal global distribution via direct weights, and the latter results in fewer data participation into FL training. To this end, we propose a proxy-guided framework called ProxyFL that focuses on simultaneously mitigating external and internal heterogeneity via a unified proxy. I.e., we consider the learnable weights of classifier as proxy to simulate the category distribution both locally and globally. For external, we explicitly optimize global proxy against outliers instead of direct weights; for internal, we re-include the discarded samples into training by a positive-negative proxy pool to mitigate the impact of potentially-incorrect pseudo-labels. Insight experiments & theoretical analysis show our significant performance and convergence in FSSL.",
    "summary": "",
    "translation": "ProxyFL：一种代理引导的联邦半监督学习框架",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及联邦学习（Federated Learning），这属于明确的无关主题（Irrelevant Topics）范畴。虽然提到了半监督学习，但核心框架围绕联邦学习构建，与推荐系统、搜索、广告的核心技术进展、LLM技术、Transformer架构或直接应用均无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21064v1": {
    "title": "Motivation is Something You Need",
    "url": "https://www.alphaxiv.org/abs/2602.21064v1",
    "arxiv_id": "2602.21064v1",
    "authors": "Mehdi Acheli, Walid Gaaloul",
    "categories": "cs.AI, cs.CV, cs.LG",
    "pub_date": "2026-02-24 16:26:52",
    "ori_summary": "This work introduces a novel training paradigm that draws from affective neuroscience. Inspired by the interplay of emotions and cognition in the human brain and more specifically the SEEKING motivational state, we design a dual-model framework where a smaller base model is trained continuously, while a larger motivated model is activated intermittently during predefined \"motivation conditions\". The framework mimics the emotional state of high curiosity and anticipation of reward in which broader brain regions are recruited to enhance cognitive performance. Exploiting scalable architectures where larger models extend smaller ones, our method enables shared weight updates and selective expansion of network capacity during noteworthy training steps. Empirical evaluation on the image classification task demonstrates that, not only does the alternating training scheme efficiently and effectively enhance the base model compared to a traditional scheme, in some cases, the motivational model also surpasses its standalone counterpart despite seeing less data per epoch. This opens the possibility of simultaneously training two models tailored to different deployment constraints with competitive or superior performance while keeping training cost lower than when training the larger model.",
    "summary": "",
    "translation": "动机是你需要的东西",
    "relevance_score": 1,
    "reasoning": "这个标题看起来与心理学或行为科学相关，而不是技术性的研究。它没有提到任何与推荐系统、搜索、广告、LLM、Transformer架构或异构数据建模相关的技术内容。标题过于宽泛和哲学化，无法识别出与指定技术领域的任何潜在联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21053v1": {
    "title": "OCR-Agent: Agentic OCR with Capability and Memory Reflection",
    "url": "https://www.alphaxiv.org/abs/2602.21053v1",
    "arxiv_id": "2602.21053v1",
    "authors": "Shimin Wen, Zeyu Zhang, Xingdou Bian, Hongjie Zhu, Lulu He, Layi Shama, Daji Ergu, Ying Cai",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 16:10:27",
    "ori_summary": "Large Vision-Language Models (VLMs) have demonstrated significant potential on complex visual understanding tasks through iterative optimization methods.However, these models generally lack effective self-correction mechanisms, making it difficult for them to independently rectify cognitive biases. Consequently, during multi-turn revisions, they often fall into repetitive and ineffective attempts, failing to achieve stable improvements in answer quality.To address this issue, we propose a novel iterative self-correction framework that endows models with two key capabilities: Capability Reflection and Memory Reflection. This framework guides the model to first diagnose errors and generate a correction plan via Capability Reflection, then leverage Memory Reflection to review past attempts to avoid repetition and explore new solutions, and finally, optimize the answer through rigorous re-reasoning. Experiments on the challenging OCRBench v2 benchmark show that OCR-Agent outperforms the current open-source SOTA model InternVL3-8B by +2.0 on English and +1.2 on Chinese subsets, while achieving state-of-the-art results in Visual Understanding (79.9) and Reasoning (66.5) - surpassing even larger fine-tuned models. Our method demonstrates that structured, self-aware reflection can significantly enhance VLMs' reasoning robustness without additional training. Code: https://github.com/AIGeeksGroup/OCR-Agent.",
    "summary": "",
    "translation": "OCR-Agent：具备能力与记忆反思的智能OCR代理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注OCR（光学字符识别）领域的智能代理技术，涉及能力反思和记忆机制，属于特定计算机视觉任务。虽然代理架构和反思机制可能对推荐系统或搜索中的智能决策有间接启发，但论文标题明确聚焦于OCR这一具体视觉任务，与您关注的LLM技术、推荐系统核心进展、Transformer架构改进或异构数据统一建模等焦点领域没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21042v1": {
    "title": "OmniOCR: Generalist OCR for Ethnic Minority Languages",
    "url": "https://www.alphaxiv.org/abs/2602.21042v1",
    "arxiv_id": "2602.21042v1",
    "authors": "Bonan Liu, Zeyu Zhang, Bingbing Meng, Han Wang, Hanshuo Zhang, Chengping Wang, Daji Ergu, Ying Cai",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 16:02:49",
    "ori_summary": "Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR.",
    "summary": "",
    "translation": "OmniOCR：面向少数民族语言的通用光学字符识别",
    "relevance_score": 1,
    "reasoning": "该论文专注于特定领域（少数民族语言）的OCR技术，属于计算机视觉中的文档分析子领域。虽然OCR在理论上可以处理文本数据，但该论文没有涉及推荐系统、搜索或广告的核心技术（如排序、召回、用户建模），也没有涉及LLM、Transformer架构或多模态建模等使能技术。其应用场景（少数民族语言处理）与当前关注的RecSys/Search/Ads领域缺乏直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21035v1": {
    "title": "Not Just What's There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning",
    "url": "https://www.alphaxiv.org/abs/2602.21035v1",
    "arxiv_id": "2602.21035v1",
    "authors": "Junhao Xiao, Zhiyu Wu, Hao Lin, Yi Chen, Yahui Liu, Xiaoran Zhao, Zixu Wang, Zejiang He",
    "categories": "cs.CV, cs.MM",
    "pub_date": "2026-02-24 15:55:39",
    "ori_summary": "Vision-Language Models (VLMs) like CLIP struggle to understand negation, often embedding affirmatives and negatives similarly (e.g., matching \"no dog\" with dog images). Existing methods refine negation understanding via fine-tuning CLIP's text encoder, risking overfitting. In this work, we propose CLIPGlasses, a plug-and-play framework that enhances CLIP's ability to comprehend negated visual descriptions. CLIPGlasses adopts a dual-stage design: a Lens module disentangles negated semantics from text embeddings, and a Frame module predicts context-aware repulsion strength, which is integrated into a modified similarity computation to penalize alignment with negated semantics, thereby reducing false positive matches. Experiments show that CLIP equipped with CLIPGlasses achieves competitive in-domain performance and outperforms state-of-the-art methods in cross-domain generalization. Its superiority is especially evident under low-resource conditions, indicating stronger robustness across domains.",
    "summary": "",
    "translation": "不仅仅是存在之物：无需微调即可使CLIP理解否定式视觉描述",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型（CLIP）处理否定描述的能力，属于VLM技术改进范畴。虽然VLM技术可能启发异构数据处理（如将用户序列和上下文特征视为不同模态），但该论文明确聚焦于视觉描述理解，与RecSys/Search/Ads的核心技术需求关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21033v1": {
    "title": "MIP Candy: A Modular PyTorch Framework for Medical Image Processing",
    "url": "https://www.alphaxiv.org/abs/2602.21033v1",
    "arxiv_id": "2602.21033v1",
    "authors": "Tianhao Fu, Yucheng Chen",
    "categories": "cs.CV, cs.AI, cs.LG, cs.SE",
    "pub_date": "2026-02-24 15:55:04",
    "ori_summary": "Medical image processing demands specialized software that handles high-dimensional volumetric data, heterogeneous file formats, and domain-specific training procedures. Existing frameworks either provide low-level components that require substantial integration effort or impose rigid, monolithic pipelines that resist modification. We present MIP Candy (MIPCandy), a freely available, PyTorch-based framework designed specifically for medical image processing. MIPCandy provides a complete, modular pipeline spanning data loading, training, inference, and evaluation, allowing researchers to obtain a fully functional process workflow by implementing a single method, $\\texttt{build_network}$, while retaining fine-grained control over every component. Central to the design is $\\texttt{LayerT}$, a deferred configuration mechanism that enables runtime substitution of convolution, normalization, and activation modules without subclassing. The framework further offers built-in $k$-fold cross-validation, dataset inspection with automatic region-of-interest detection, deep supervision, exponential moving average, multi-frontend experiment tracking (Weights & Biases, Notion, MLflow), training state recovery, and validation score prediction via quotient regression. An extensible bundle ecosystem provides pre-built model implementations that follow a consistent trainer--predictor pattern and integrate with the core framework without modification. MIPCandy is open-source under the Apache-2.0 license and requires Python~3.12 or later. Source code and documentation are available at https://github.com/ProjectNeura/MIPCandy.",
    "summary": "",
    "translation": "MIP Candy：一个用于医学图像处理的模块化PyTorch框架",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医学图像处理，属于明确的医学领域应用，与用户指定的RecSys、搜索、广告等核心领域完全无关。用户明确将医学、生物学、化学等特定领域应用列为不相关主题，因此该论文完全不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21015v1": {
    "title": "From Perception to Action: An Interactive Benchmark for Vision Reasoning",
    "url": "https://www.alphaxiv.org/abs/2602.21015v1",
    "arxiv_id": "2602.21015v1",
    "authors": "Yuhao Wu, Maojia Song, Yihuai Lan, Lei Wang, Zhiqiang Hu, Yao Xiao, Heng Zhou, Weihua Zheng, Dylan Raharja, Soujanya Poria, Roy Ka-Wei Lee",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 15:33:02",
    "ori_summary": "Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents' ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.",
    "summary": "",
    "translation": "从感知到行动：视觉推理的交互式基准测试",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于视觉推理的基准测试，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然标题提及“推理”，但上下文表明这是视觉模态的推理，而非语言或序列建模，因此对所列焦点领域没有明显应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.21010v1": {
    "title": "Le-DETR: Revisiting Real-Time Detection Transformer with Efficient Encoder Design",
    "url": "https://www.alphaxiv.org/abs/2602.21010v1",
    "arxiv_id": "2602.21010v1",
    "authors": "Jiannan Huang, Aditya Kane, Fengzhe Zhou, Yunchao Wei, Humphrey Shi",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 15:29:55",
    "ori_summary": "Real-time object detection is crucial for real-world applications as it requires high accuracy with low latency. While Detection Transformers (DETR) have demonstrated significant performance improvements, current real-time DETR models are challenging to reproduce from scratch due to excessive pre-training overheads on the backbone, constraining research advancements by hindering the exploration of novel backbone architectures. In this paper, we want to show that by using general good design, it is possible to have \\textbf{high performance} with \\textbf{low pre-training cost}. After a thorough study of the backbone architecture, we propose EfficientNAT at various scales, which incorporates modern efficient convolution and local attention mechanisms. Moreover, we re-design the hybrid encoder with local attention, significantly enhancing both performance and inference speed. Based on these advancements, we present Le-DETR (\\textbf{L}ow-cost and \\textbf{E}fficient \\textbf{DE}tection \\textbf{TR}ansformer), which achieves a new \\textbf{SOTA} in real-time detection using only ImageNet1K and COCO2017 training datasets, saving about 80\\% images in pre-training stage compared with previous methods. We demonstrate that with well-designed, real-time DETR models can achieve strong performance without the need for complex and computationally expensive pretraining. Extensive experiments show that Le-DETR-M/L/X achieves \\textbf{52.9/54.3/55.1 mAP} on COCO Val2017 with \\textbf{4.45/5.01/6.68 ms} on an RTX4090. It surpasses YOLOv12-L/X by \\textbf{+0.6/-0.1 mAP} while achieving similar speed and \\textbf{+20\\%} speedup. Compared with DEIM-D-FINE, Le-DETR-M achieves \\textbf{+0.2 mAP} with slightly faster inference, and surpasses DEIM-D-FINE-L by \\textbf{+0.4 mAP} with only \\textbf{0.4 ms} additional latency. Code and weights will be open-sourced.",
    "summary": "",
    "translation": "Le-DETR：通过高效编码器设计重新审视实时检测Transformer",
    "relevance_score": 3,
    "reasoning": "该论文专注于计算机视觉中的目标检测任务，属于纯粹的视觉领域研究。虽然涉及Transformer架构的效率改进，但缺乏与推荐系统、搜索或广告领域的明确联系或潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20999v1": {
    "title": "VII: Visual Instruction Injection for Jailbreaking Image-to-Video Generation Models",
    "url": "https://www.alphaxiv.org/abs/2602.20999v1",
    "arxiv_id": "2602.20999v1",
    "authors": "Bowen Zheng, Yongli Xiang, Ziming Hong, Zerong Lin, Chaojian Yu, Tongliang Liu, Xinge You",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 15:20:01",
    "ori_summary": "Image-to-Video (I2V) generation models, which condition video generation on reference images, have shown emerging visual instruction-following capability, allowing certain visual cues in reference images to act as implicit control signals for video generation. However, this capability also introduces a previously overlooked risk: adversaries may exploit visual instructions to inject malicious intent through the image modality. In this work, we uncover this risk by proposing Visual Instruction Injection (VII), a training-free and transferable jailbreaking framework that intentionally disguises the malicious intent of unsafe text prompts as benign visual instructions in the safe reference image. Specifically, VII coordinates a Malicious Intent Reprogramming module to distill malicious intent from unsafe text prompts while minimizing their static harmfulness, and a Visual Instruction Grounding module to ground the distilled intent onto a safe input image by rendering visual instructions that preserve semantic consistency with the original unsafe text prompt, thereby inducing harmful content during I2V generation. Empirically, our extensive experiments on four state-of-the-art commercial I2V models (Kling-v2.5-turbo, Gemini Veo-3.1, Seedance-1.5-pro, and PixVerse-V5) demonstrate that VII achieves Attack Success Rates of up to 83.5% while reducing Refusal Rates to near zero, significantly outperforming existing baselines.",
    "summary": "",
    "translation": "VII：视觉指令注入用于越狱图像到视频生成模型",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及图像到视频生成模型的安全漏洞（越狱），这属于安全/隐私领域，属于明确列出的无关主题。虽然提到了视觉指令，但核心焦点是模型安全攻击而非推荐/搜索/广告应用。没有迹象表明该技术能应用于推荐系统、搜索或广告领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20989v1": {
    "title": "Cycle-Consistent Tuning for Layered Image Decomposition",
    "url": "https://www.alphaxiv.org/abs/2602.20989v1",
    "arxiv_id": "2602.20989v1",
    "authors": "Zheng Gu, Min Lu, Zhida Sun, Dani Lischinski, Daniel Cohen-O, Hui Huang",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 15:10:31",
    "ori_summary": "Disentangling visual layers in real-world images is a persistent challenge in vision and graphics, as such layers often involve non-linear and globally coupled interactions, including shading, reflection, and perspective distortion. In this work, we present an in-context image decomposition framework that leverages large diffusion foundation models for layered separation. We focus on the challenging case of logo-object decomposition, where the goal is to disentangle a logo from the surface on which it appears while faithfully preserving both layers. Our method fine-tunes a pretrained diffusion model via lightweight LoRA adaptation and introduces a cycle-consistent tuning strategy that jointly trains decomposition and composition models, enforcing reconstruction consistency between decomposed and recomposed images. This bidirectional supervision substantially enhances robustness in cases where the layers exhibit complex interactions. Furthermore, we introduce a progressive self-improving process, which iteratively augments the training set with high-quality model-generated examples to refine performance. Extensive experiments demonstrate that our approach achieves accurate and coherent decompositions and also generalizes effectively across other decomposition types, suggesting its potential as a unified framework for layered image decomposition.",
    "summary": "",
    "translation": "用于分层图像分解的循环一致性调优",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及图像分解技术，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术无关。虽然分层分解可能涉及多模态处理，但论文专注于纯视觉任务，没有表明与异构数据建模或RecSys/Search/Ads应用的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20985v1": {
    "title": "EW-DETR: Evolving World Object Detection via Incremental Low-Rank DEtection TRansformer",
    "url": "https://www.alphaxiv.org/abs/2602.20985v1",
    "arxiv_id": "2602.20985v1",
    "authors": "Munish Monga, Vishal Chudasama, Pankaj Wasnik, C. V. Jawahar",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 15:06:04",
    "ori_summary": "Real-world object detection must operate in evolving environments where new classes emerge, domains shift, and unseen objects must be identified as \"unknown\": all without accessing prior data. We introduce Evolving World Object Detection (EWOD), a paradigm coupling incremental learning, domain adaptation, and unknown detection under exemplar-free constraints. To tackle EWOD, we propose EW-DETR framework that augments DETR-based detectors with three synergistic modules: Incremental LoRA Adapters for exemplar-free incremental learning under evolving domains; a Query-Norm Objectness Adapter that decouples objectness-aware features from DETR decoder queries; and Entropy-Aware Unknown Mixing for calibrated unknown detection. This framework generalises across DETR-based detectors, enabling state-of-the-art RF-DETR to operate effectively in evolving-world settings. We also introduce FOGS (Forgetting, Openness, Generalisation Score) to holistically evaluate performance across these dimensions. Extensive experiments on Pascal Series and Diverse Weather benchmarks show EW-DETR outperforms other methods, improving FOGS by 57.24%.",
    "summary": "",
    "translation": "EW-DETR：通过增量低秩检测Transformer实现演化世界目标检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的目标检测任务，属于纯粹的视觉领域研究。虽然涉及Transformer架构，但其核心是视觉检测而非推荐/搜索/广告应用。低秩方法可能对模型效率有启发，但论文未展示在异构数据或多模态建模方面的明确潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20981v1": {
    "title": "Echoes Over Time: Unlocking Length Generalization in Video-to-Audio Generation Models",
    "url": "https://www.alphaxiv.org/abs/2602.20981v1",
    "arxiv_id": "2602.20981v1",
    "authors": "Christian Simon, MAsato Ishii, Wei-Yao Wang, Koichi Saito, Akio Hayakawa, Dongseok Shim, Zhi Zhong, Shuyang Cui, Shusuke Takahashi, Takashi Shibuya, Yuki Mitsufuji",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-24 15:01:39",
    "ori_summary": "Scaling multimodal alignment between video and audio is challenging, particularly due to limited data and the mismatch between text descriptions and frame-level video information. In this work, we tackle the scaling challenge in multimodal-to-audio generation, examining whether models trained on short instances can generalize to longer ones during testing. To tackle this challenge, we present multimodal hierarchical networks so-called MMHNet, an enhanced extension of state-of-the-art video-to-audio models. Our approach integrates a hierarchical method and non-causal Mamba to support long-form audio generation. Our proposed method significantly improves long audio generation up to more than 5 minutes. We also prove that training short and testing long is possible in the video-to-audio generation tasks without training on the longer durations. We show in our experiments that our proposed method could achieve remarkable results on long-video to audio benchmarks, beating prior works in video-to-audio tasks. Moreover, we showcase our model capability in generating more than 5 minutes, while prior video-to-audio methods fall short in generating with long durations.",
    "summary": "",
    "translation": "时光回响：解锁视频到音频生成模型中的长度泛化能力",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频到音频的生成任务，属于跨模态生成领域，与推荐系统、搜索或广告的核心技术关联较弱。虽然涉及生成模型，但未明确展示在推荐、搜索或广告场景中的潜在应用价值，且更偏向于多媒体处理而非用户行为建模或内容排序。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20980v1": {
    "title": "CrystaL: Spontaneous Emergence of Visual Latents in MLLMs",
    "url": "https://www.alphaxiv.org/abs/2602.20980v1",
    "arxiv_id": "2602.20980v1",
    "authors": "Yang Zhang, Danyang Li, Yuxuan Li, Xin Zhang, Tianyu Xie, Mingming Cheng, Xiang Li",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-24 15:01:30",
    "ori_summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable performance by integrating powerful language backbones with large-scale visual encoders. Among these, latent Chain-of-Thought (CoT) methods enable implicit reasoning in continuous hidden states, facilitating seamless vision-language integration and faster inference. However, existing heuristically predefined supervision signals in latent CoT provide limited guidance for preserving critical visual information in intermediate latent states. To address this limitation, we propose CrystaL (Crystallized Latent Reasoning), a single-stage framework with two paths to process intact and corrupted images, respectively. By explicitly aligning the attention patterns and prediction distributions across the two paths, CrystaL crystallizes latent representations into task-relevant visual semantics, without relying on auxiliary annotations or external modules. Extensive experiments on perception-intensive benchmarks demonstrate that CrystaL consistently outperforms state-of-the-art baselines, achieving substantial gains in fine-grained visual understanding while maintaining robust reasoning capabilities.",
    "summary": "",
    "translation": "CrystaL：多模态大语言模型中视觉潜在表征的自发涌现",
    "relevance_score": 2,
    "reasoning": "该论文研究多模态大语言模型（MLLMs）中视觉潜在表征的自发涌现，属于纯粹的视觉-语言多模态研究，没有明确展示与推荐系统、搜索或广告的直接关联。虽然视觉-语言模型的思路可能启发异构数据处理（如将用户序列和上下文特征视为不同模态），但论文标题未表明这种应用潜力，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20972v1": {
    "title": "Are Multimodal Large Language Models Good Annotators for Image Tagging?",
    "url": "https://www.alphaxiv.org/abs/2602.20972v1",
    "arxiv_id": "2602.20972v1",
    "authors": "Ming-Kun Xie, Jia-Hao Xiao, Zhiqiang Kou, Zhongnian Li, Gang Niu, Masashi Sugiyama",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 14:53:16",
    "ori_summary": "Image tagging, a fundamental vision task, traditionally relies on human-annotated datasets to train multi-label classifiers, which incurs significant labor and costs. While Multimodal Large Language Models (MLLMs) offer promising potential to automate annotation, their capability to replace human annotators remains underexplored. This paper aims to analyze the gap between MLLM-generated and human annotations and to propose an effective solution that enables MLLM-based annotation to replace manual labeling. Our analysis of MLLM annotations reveals that, under a conservative estimate, MLLMs can reduce annotation cost to as low as one-thousandth of the human cost, mainly accounting for GPU usage, which is nearly negligible compared to manual efforts. Their annotation quality reaches about 50\\% to 80\\% of human performance, while achieving over 90\\% performance on downstream training tasks.Motivated by these findings, we propose TagLLM, a novel framework for image tagging, which aims to narrow the gap between MLLM-generated and human annotations. TagLLM comprises two components: Candidates generation, which employs structured group-wise prompting to efficiently produce a compact candidate set that covers as many true labels as possible while reducing subsequent annotation workload; and label disambiguation, which interactively calibrates the semantic concept of categories in the prompts and effectively refines the candidate labels. Extensive experiments show that TagLLM substantially narrows the gap between MLLM-generated and human annotations, especially in downstream training performance, where it closes about 60\\% to 80\\% of the difference.",
    "summary": "",
    "translation": "多模态大语言模型是图像标注的优秀标注工具吗？",
    "relevance_score": 2,
    "reasoning": "该论文主要评估多模态大语言模型在图像标注任务中的性能，属于纯粹的视觉-语言模型评估范畴，与推荐系统、搜索或广告的核心技术进展无关。虽然多模态模型技术本身可能具有潜在应用价值，但该标题明确聚焦于标注能力评估这一特定任务，缺乏对推荐/搜索/广告领域的直接或间接应用潜力说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20951v1": {
    "title": "See and Fix the Flaws: Enabling VLMs and Diffusion Models to Comprehend Visual Artifacts via Agentic Data Synthesis",
    "url": "https://www.alphaxiv.org/abs/2602.20951v1",
    "arxiv_id": "2602.20951v1",
    "authors": "Jaehyun Park, Minyoung Ahn, Minkyu Kim, Jonghyun Lee, Jae-Gil Lee, Dongmin Park",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-24 14:34:13",
    "ori_summary": "Despite recent advances in diffusion models, AI generated images still often contain visual artifacts that compromise realism. Although more thorough pre-training and bigger models might reduce artifacts, there is no assurance that they can be completely eliminated, which makes artifact mitigation a highly crucial area of study. Previous artifact-aware methodologies depend on human-labeled artifact datasets, which are costly and difficult to scale, underscoring the need for an automated approach to reliably acquire artifact-annotated datasets. In this paper, we propose ArtiAgent, which efficiently creates pairs of real and artifact-injected images. It comprises three agents: a perception agent that recognizes and grounds entities and subentities from real images, a synthesis agent that introduces artifacts via artifact injection tools through novel patch-wise embedding manipulation within a diffusion transformer, and a curation agent that filters the synthesized artifacts and generates both local and global explanations for each instance. Using ArtiAgent, we synthesize 100K images with rich artifact annotations and demonstrate both efficacy and versatility across diverse applications. Code is available at link.",
    "summary": "",
    "translation": "洞察并修复缺陷：通过智能数据合成使视觉语言模型和扩散模型能够理解视觉伪影",
    "relevance_score": 1,
    "reasoning": "该论文主要关注视觉伪影理解和生成模型（VLM/扩散模型）的改进，属于纯粹的视觉/生成领域。虽然提到了“智能数据合成”，但其核心应用场景是视觉内容理解和修复，与推荐系统、搜索或广告的排名、匹配、个性化等核心任务没有直接关联。该技术可能适用于广告创意生成等非排名广告任务，但根据用户明确的排除标准，这些属于不相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20947v1": {
    "title": "Estimation of Confidence Bounds in Binary Classification using Wilson Score Kernel Density Estimation",
    "url": "https://www.alphaxiv.org/abs/2602.20947v1",
    "arxiv_id": "2602.20947v1",
    "authors": "Thorbjørn Mosekjær Iversen, Zebin Duan, Frederik Hagelskjær",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2026-02-24 14:31:28",
    "ori_summary": "The performance and ease of use of deep learning-based binary classifiers have improved significantly in recent years. This has opened up the potential for automating critical inspection tasks, which have traditionally only been trusted to be done manually. However, the application of binary classifiers in critical operations depends on the estimation of reliable confidence bounds such that system performance can be ensured up to a given statistical significance. We present Wilson Score Kernel Density Classification, which is a novel kernel-based method for estimating confidence bounds in binary classification. The core of our method is the Wilson Score Kernel Density Estimator, which is a function estimator for estimating confidence bounds in Binomial experiments with conditionally varying success probabilities. Our method is evaluated in the context of selective classification on four different datasets, illustrating its use as a classification head of any feature extractor, including vision foundation models. Our proposed method shows similar performance to Gaussian Process Classification, but at a lower computational complexity.",
    "summary": "",
    "translation": "基于威尔逊分数核密度估计的二分类置信区间估计",
    "relevance_score": 2,
    "reasoning": "该论文专注于二分类中的置信区间估计方法，属于统计机器学习范畴，与推荐系统、搜索或广告的核心领域进展没有直接关联。虽然置信度估计在模型评估中有一定作用，但该论文没有展示在推荐/搜索/广告场景中的具体应用潜力，也不涉及LLM技术、Transformer架构或异构数据统一建模等当前关注方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20943v1": {
    "title": "UFO: Unifying Feed-Forward and Optimization-based Methods for Large Driving Scene Modeling",
    "url": "https://www.alphaxiv.org/abs/2602.20943v1",
    "arxiv_id": "2602.20943v1",
    "authors": "Kaiyuan Tan, Yingying Shen, Mingfei Tu, Haohui Zhu, Bing Wang, Guang Chen, Hangjun Ye, Haiyang Sun",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 14:24:50",
    "ori_summary": "Dynamic driving scene reconstruction is critical for autonomous driving simulation and closed-loop learning. While recent feed-forward methods have shown promise for 3D reconstruction, they struggle with long-range driving sequences due to quadratic complexity in sequence length and challenges in modeling dynamic objects over extended durations. We propose UFO, a novel recurrent paradigm that combines the benefits of optimization-based and feed-forward methods for efficient long-range 4D reconstruction. Our approach maintains a 4D scene representation that is iteratively refined as new observations arrive, using a visibility-based filtering mechanism to select informative scene tokens and enable efficient processing of long sequences. For dynamic objects, we introduce an object pose-guided modeling approach that supports accurate long-range motion capture. Experiments on the Waymo Open Dataset demonstrate that our method significantly outperforms both per-scene optimization and existing feed-forward methods across various sequence lengths. Notably, our approach can reconstruct 16-second driving logs within 0.5 second while maintaining superior visual quality and geometric accuracy.",
    "summary": "",
    "translation": "UFO：统一前馈与基于优化的方法用于大规模驾驶场景建模",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶场景建模，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术焦点无关。虽然提到了统一建模方法，但其应用场景（驾驶场景）与异构数据处理（如用户序列和上下文特征）的VLM类比没有直接关联，且未涉及Transformer架构、LLM技术或推荐/搜索/广告的具体应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20933v1": {
    "title": "Dropping Anchor and Spherical Harmonics for Sparse-view Gaussian Splatting",
    "url": "https://www.alphaxiv.org/abs/2602.20933v1",
    "arxiv_id": "2602.20933v1",
    "authors": "Shuangkang Fang, I-Chao Shen, Xuanyang Zhang, Zesheng Wang, Yufeng Wang, Wenrui Ding, Gang Yu, Takeo Igarashi",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 14:11:56",
    "ori_summary": "Recent 3D Gaussian Splatting (3DGS) Dropout methods address overfitting under sparse-view conditions by randomly nullifying Gaussian opacities. However, we identify a neighbor compensation effect in these approaches: dropped Gaussians are often compensated by their neighbors, weakening the intended regularization. Moreover, these methods overlook the contribution of high-degree spherical harmonic coefficients (SH) to overfitting. To address these issues, we propose DropAnSH-GS, a novel anchor-based Dropout strategy. Rather than dropping Gaussians independently, our method randomly selects certain Gaussians as anchors and simultaneously removes their spatial neighbors. This effectively disrupts local redundancies near anchors and encourages the model to learn more robust, globally informed representations. Furthermore, we extend the Dropout to color attributes by randomly dropping higher-degree SH to concentrate appearance information in lower-degree SH. This strategy further mitigates overfitting and enables flexible post-training model compression via SH truncation. Experimental results demonstrate that DropAnSH-GS substantially outperforms existing Dropout methods with negligible computational overhead, and can be readily integrated into various 3DGS variants to enhance their performances. Project Website: https://sk-fun.fun/DropAnSH-GS",
    "summary": "",
    "translation": "基于锚点丢弃与球谐函数的稀疏视角高斯溅射",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及计算机视觉中的3D重建技术（高斯溅射）和图形学方法（球谐函数），属于纯粹的视觉/图形学领域。虽然提到了稀疏视角这一概念，但论文核心是3D场景重建技术，与推荐系统、搜索或广告的排名、建模等核心任务没有直接关联，也不属于LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20930v1": {
    "title": "Computing a Characteristic Orientation for Rotation-Independent Image Analysis",
    "url": "https://www.alphaxiv.org/abs/2602.20930v1",
    "arxiv_id": "2602.20930v1",
    "authors": "Cristian Valero-Abundio, Emilio Sansano-Sansano, Raúl Montoliu, Marina Martínez García",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 14:08:12",
    "ori_summary": "Handling geometric transformations, particularly rotations, remains a challenge in deep learning for computer vision. Standard neural networks lack inherent rotation invariance and typically rely on data augmentation or architectural modifications to improve robustness. Although effective, these approaches increase computational demands, require specialised implementations, or alter network structures, limiting their applicability. This paper introduces General Intensity Direction (GID), a preprocessing method that improves rotation robustness without modifying the network architecture. The method estimates a global orientation for each image and aligns it to a canonical reference frame, allowing standard models to process inputs more consistently across different rotations. Unlike moment-based approaches that extract invariant descriptors, this method directly transforms the image while preserving spatial structure, making it compatible with convolutional networks. Experimental evaluation on the rotated MNIST dataset shows that the proposed method achieves higher accuracy than state-of-the-art rotation-invariant architectures. Additional experiments on the CIFAR-10 dataset, confirm that the method remains effective under more complex conditions.",
    "summary": "",
    "translation": "计算用于旋转无关图像分析的特征方向",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于计算机视觉中的图像旋转不变性分析，属于纯粹的视觉处理技术。虽然图像分析可能在某些广告或推荐场景中出现（如产品图像识别），但标题本身没有表明与推荐系统、搜索、广告排名或LLM/Transformer技术的任何直接关联，也没有提出处理异构数据或跨模态建模的方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20925v1": {
    "title": "LST-SLAM: A Stereo Thermal SLAM System for Kilometer-Scale Dynamic Environments",
    "url": "https://www.alphaxiv.org/abs/2602.20925v1",
    "arxiv_id": "2602.20925v1",
    "authors": "Zeyu Jiang, Kuan Xu, Changhao Chen",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2026-02-24 14:04:54",
    "ori_summary": "Thermal cameras offer strong potential for robot perception under challenging illumination and weather conditions. However, thermal Simultaneous Localization and Mapping (SLAM) remains difficult due to unreliable feature extraction, unstable motion tracking, and inconsistent global pose and map construction, particularly in dynamic large-scale outdoor environments. To address these challenges, we propose LST-SLAM, a novel large-scale stereo thermal SLAM system that achieves robust performance in complex, dynamic scenes. Our approach combines self-supervised thermal feature learning, stereo dual-level motion tracking, and geometric pose optimization. We also introduce a semantic-geometric hybrid constraint that suppresses potentially dynamic features lacking strong inter-frame geometric consistency. Furthermore, we develop an online incremental bag-of-words model for loop closure detection, coupled with global pose optimization to mitigate accumulated drift. Extensive experiments on kilometer-scale dynamic thermal datasets show that LST-SLAM significantly outperforms recent representative SLAM systems, including AirSLAM and DROID-SLAM, in both robustness and accuracy.",
    "summary": "",
    "translation": "LST-SLAM：一种用于千米级动态环境的立体热成像SLAM系统",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的SLAM（同时定位与地图构建）技术，使用热成像传感器处理动态环境。这属于纯粹的视觉/机器人领域研究，与推荐系统、搜索、广告或LLM技术没有直接关联，也不涉及异构数据处理或多模态建模的类比应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20913v1": {
    "title": "LongVideo-R1: Smart Navigation for Low-cost Long Video Understanding",
    "url": "https://www.alphaxiv.org/abs/2602.20913v1",
    "arxiv_id": "2602.20913v1",
    "authors": "Jihao Qiu, Lingxi Xie, Xinyue Huo, Qi Tian, Qixiang Ye",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 13:49:47",
    "ori_summary": "This paper addresses the critical and underexplored challenge of long video understanding with low computational budgets. We propose LongVideo-R1, an active, reasoning-equipped multimodal large language model (MLLM) agent designed for efficient video context navigation, avoiding the redundancy of exhaustive search. At the core of LongVideo-R1 lies a reasoning module that leverages high-level visual cues to infer the most informative video clip for subsequent processing. During inference, the agent initiates traversal from top-level visual summaries and iteratively refines its focus, immediately halting the exploration process upon acquiring sufficient knowledge to answer the query. To facilitate training, we first extract hierarchical video captions from CGBench, a video corpus with grounding annotations, and guide GPT-5 to generate 33K high-quality chain-of-thought-with-tool trajectories. The LongVideo-R1 agent is fine-tuned upon the Qwen-3-8B model through a two-stage paradigm: supervised fine-tuning (SFT) followed by reinforcement learning (RL), where RL employs a specifically designed reward function to maximize selective and efficient clip navigation. Experiments on multiple long video benchmarks validate the effectiveness of name, which enjoys superior tradeoff between QA accuracy and efficiency. All curated data and source code are provided in the supplementary material and will be made publicly available. Code and data are available at: https://github.com/qiujihao19/LongVideo-R1",
    "summary": "",
    "translation": "LongVideo-R1：面向低成本长视频理解的智能导航",
    "relevance_score": 2,
    "reasoning": "该论文主要关注长视频理解与智能导航，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术（如排序、召回、用户建模）没有直接关联。虽然视频理解技术可能间接应用于内容推荐，但论文标题未明确涉及异构数据处理、Transformer架构改进或LLM在推荐/搜索中的直接应用，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20911v1": {
    "title": "From Isolation to Integration: Building an Adaptive Expert Forest for Pre-Trained Model-based Class-Incremental Learning",
    "url": "https://www.alphaxiv.org/abs/2602.20911v1",
    "arxiv_id": "2602.20911v1",
    "authors": "Ruiqi Liu, Boyu Diao, Hangda Liu, Zhulin An, Fei Wang, Yongjun Xu",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2026-02-24 13:48:13",
    "ori_summary": "Class-Incremental Learning (CIL) requires models to learn new classes without forgetting old ones. A common method is to freeze a pre-trained model and train a new, lightweight adapter for each task. While this prevents forgetting, it treats the learned knowledge as a simple, unstructured collection and fails to use the relationships between tasks. To this end, we propose the Semantic-guided Adaptive Expert Forest (SAEF), a new method that organizes adapters into a structured hierarchy for better knowledge sharing. SAEF first groups tasks into conceptual clusters based on their semantic relationships. Then, within each cluster, it builds a balanced expert tree by creating new adapters from merging the adapters of similar tasks. At inference time, SAEF finds and activates a set of relevant experts from the forest for any given input. The final prediction is made by combining the outputs of these activated experts, weighted by how confident each expert is. Experiments on several benchmark datasets show that SAEF achieves SOTA performance.",
    "summary": "",
    "translation": "从隔离到整合：为基于预训练模型的类增量学习构建自适应专家森林",
    "relevance_score": 3,
    "reasoning": "该论文涉及预训练模型和专家混合（MoE）架构，属于Transformer技术进展范畴，可能对推荐系统或搜索中的增量学习场景有潜在应用。然而，标题明确聚焦于类增量学习这一特定任务，而非直接针对推荐/搜索/广告的核心排序或检索问题，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20903v1": {
    "title": "TextPecker: Rewarding Structural Anomaly Quantification for Enhancing Visual Text Rendering",
    "url": "https://www.alphaxiv.org/abs/2602.20903v1",
    "arxiv_id": "2602.20903v1",
    "authors": "Hanshen Zhu, Yuliang Liu, Xuecheng Wu, An-Lan Wang, Hao Feng, Dingkang Yang, Chao Feng, Can Huang, Jingqun Tang, Xiang Bai",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 13:40:23",
    "ori_summary": "Visual Text Rendering (VTR) remains a critical challenge in text-to-image generation, where even advanced models frequently produce text with structural anomalies such as distortion, blurriness, and misalignment. However, we find that leading MLLMs and specialist OCR models largely fail to perceive these structural anomalies, creating a critical bottleneck for both VTR evaluation and RL-based optimization. As a result, even state-of-the-art generators (e.g., SeedDream4.0, Qwen-Image) still struggle to render structurally faithful text. To address this, we propose TextPecker, a plug-and-play structural anomaly perceptive RL strategy that mitigates noisy reward signals and works with any textto-image generator. To enable this capability, we construct a recognition dataset with character-level structural-anomaly annotations and develop a stroke-editing synthesis engine to expand structural-error coverage. Experiments show that TextPecker consistently improves diverse text-to-image models; even on the well-optimized Qwen-Image, it significantly yields average gains of 4% in structural fidelity and 8.7% in semantic alignment for Chinese text rendering, establishing a new state-of-the-art in high-fidelity VTR. Our work fills a gap in VTR optimization, providing a foundational step towards reliable and structural faithful visual text generation.",
    "summary": "",
    "translation": "TextPecker：通过奖励结构异常量化来增强视觉文本渲染",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉文本渲染的质量评估和增强，属于计算机视觉和文档处理领域。虽然提到了结构异常量化，但没有明确涉及推荐系统、搜索或广告的核心技术，也不属于LLM、Transformer架构或异构数据统一建模等当前关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20901v1": {
    "title": "SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.20901v1",
    "arxiv_id": "2602.20901v1",
    "authors": "Yuechen Xie, Xiaoyan Zhang, Yicheng Shan, Hao Zhu, Rui Tang, Rong Wei, Mingli Song, Yuanyu Wan, Jie Song",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-02-24 13:38:37",
    "ori_summary": "Vision-Language Models (VLMs) have been increasingly applied in real-world scenarios due to their outstanding understanding and reasoning capabilities. Although VLMs have already demonstrated impressive capabilities in common visual question answering and logical reasoning, they still lack the ability to make reasonable decisions in complex real-world environments. We define this ability as spatial logical reasoning, which not only requires understanding the spatial relationships among objects in complex scenes, but also the logical dependencies between steps in multi-step tasks. To bridge this gap, we introduce Spatial Logical Question Answering (SpatiaLQA), a benchmark designed to evaluate the spatial logical reasoning capabilities of VLMs. SpatiaLQA consists of 9,605 question answer pairs derived from 241 real-world indoor scenes. We conduct extensive experiments on 41 mainstream VLMs, and the results show that even the most advanced models still struggle with spatial logical reasoning. To address this issue, we propose a method called recursive scene graph assisted reasoning, which leverages visual foundation models to progressively decompose complex scenes into task-relevant scene graphs, thereby enhancing the spatial logical reasoning ability of VLMs, outperforming all previous methods. Code and dataset are available at https://github.com/xieyc99/SpatiaLQA.",
    "summary": "",
    "translation": "SpatiaLQA：一个用于评估视觉语言模型中空间逻辑推理能力的基准",
    "relevance_score": 1,
    "reasoning": "该论文标题明确表明这是一个专注于视觉语言模型（VLM）评估基准的研究，属于纯粹的VLM/NLP评估范畴。虽然提到了空间逻辑推理，但这是针对视觉-语言模态的特定评估任务，没有展示如何将异构数据（如上下文特征和用户序列）作为不同模态进行统一建模的潜力，也没有直接应用于推荐系统、搜索或广告的明确路径。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20880v1": {
    "title": "When Safety Collides: Resolving Multi-Category Harmful Conflicts in Text-to-Image Diffusion via Adaptive Safety Guidance",
    "url": "https://www.alphaxiv.org/abs/2602.20880v1",
    "arxiv_id": "2602.20880v1",
    "authors": "Yongli Xiang, Ziming Hong, Zhaoqing Wang, Xiangyu Zhao, Bo Han, Tongliang Liu",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 13:20:31",
    "ori_summary": "Text-to-Image (T2I) diffusion models have demonstrated significant advancements in generating high-quality images, while raising potential safety concerns regarding harmful content generation. Safety-guidance-based methods have been proposed to mitigate harmful outputs by steering generation away from harmful zones, where the zones are averaged across multiple harmful categories based on predefined keywords. However, these approaches fail to capture the complex interplay among different harm categories, leading to \"harmful conflicts\" where mitigating one type of harm may inadvertently amplify another, thus increasing overall harmful rate. To address this issue, we propose Conflict-aware Adaptive Safety Guidance (CASG), a training-free framework that dynamically identifies and applies the category-aligned safety direction during generation. CASG is composed of two components: (i) Conflict-aware Category Identification (CaCI), which identifies the harmful category most aligned with the model's evolving generative state, and (ii) Conflict-resolving Guidance Application (CrGA), which applies safety steering solely along the identified category to avoid multi-category interference. CASG can be applied to both latent-space and text-space safeguards. Experiments on T2I safety benchmarks demonstrate CASG's state-of-the-art performance, reducing the harmful rate by up to 15.4% compared to existing methods.",
    "summary": "",
    "translation": "当安全碰撞时：通过自适应安全引导解决文本到图像扩散中的多类别有害冲突",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本到图像扩散模型的安全引导机制，属于AIGC/内容生成领域。虽然涉及多类别冲突解决，但核心是图像生成安全而非推荐/搜索/广告的排名或建模，且未提及异构数据处理或Transformer架构改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20873v1": {
    "title": "MUSE: Harnessing Precise and Diverse Semantics for Few-Shot Whole Slide Image Classification",
    "url": "https://www.alphaxiv.org/abs/2602.20873v1",
    "arxiv_id": "2602.20873v1",
    "authors": "Jiahao Xu, Sheng Huang, Xin Zhang, Zhixiong Nan, Jiajun Dong, Nankun Mu",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 13:17:35",
    "ori_summary": "In computational pathology, few-shot whole slide image classification is primarily driven by the extreme scarcity of expert-labeled slides. Recent vision-language methods incorporate textual semantics generated by large language models, but treat these descriptions as static class-level priors that are shared across all samples and lack sample-wise refinement. This limits both the diversity and precision of visual-semantic alignment, hindering generalization under limited supervision. To overcome this, we propose the stochastic MUlti-view Semantic Enhancement (MUSE), a framework that first refines semantic precision via sample-wise adaptation and then enhances semantic richness through retrieval-augmented multi-view generation. Specifically, MUSE introduces Sample-wise Fine-grained Semantic Enhancement (SFSE), which yields a fine-grained semantic prior for each sample through MoE-based adaptive visual-semantic interaction. Guided by this prior, Stochastic Multi-view Model Optimization (SMMO) constructs an LLM-generated knowledge base of diverse pathological descriptions per class, then retrieves and stochastically integrates multiple matched textual views during training. These dynamically selected texts serve as enriched semantic supervisions to stochastically optimize the vision-language model, promoting robustness and mitigating overfitting. Experiments on three benchmark WSI datasets show that MUSE consistently outperforms existing vision-language baselines in few-shot settings, demonstrating that effective few-shot pathology learning requires not only richer semantic sources but also their active and sample-aware semantic optimization. Our code is available at: https://github.com/JiahaoXu-god/CVPR2026_MUSE.",
    "summary": "",
    "translation": "MUSE：利用精确且多样化的语义进行少样本全切片图像分类",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像（全切片图像）分类，属于明确的医学领域应用，与用户关注的搜索/推荐/广告技术栈完全无关。标题中提到的'少样本学习'和'语义'虽然具有通用性，但整体应用场景被限定在医疗影像分析，属于被明确排除的'Medical, Biology, Chemistry, Physics or other domain-specific applications'范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20860v1": {
    "title": "DA-Cal: Towards Cross-Domain Calibration in Semantic Segmentation",
    "url": "https://www.alphaxiv.org/abs/2602.20860v1",
    "arxiv_id": "2602.20860v1",
    "authors": "Wangkai Li, Rui Sun, Zhaoyang Li, Yujia Chen, Tianzhu Zhang",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 13:03:41",
    "ori_summary": "While existing unsupervised domain adaptation (UDA) methods greatly enhance target domain performance in semantic segmentation, they often neglect network calibration quality, resulting in misalignment between prediction confidence and actual accuracy -- a significant risk in safety-critical applications. Our key insight emerges from observing that performance degrades substantially when soft pseudo-labels replace hard pseudo-labels in cross-domain scenarios due to poor calibration, despite the theoretical equivalence of perfectly calibrated soft pseudo-labels to hard pseudo-labels. Based on this finding, we propose DA-Cal, a dedicated cross-domain calibration framework that transforms target domain calibration into soft pseudo-label optimization. DA-Cal introduces a Meta Temperature Network to generate pixel-level calibration parameters and employs bi-level optimization to establish the relationship between soft pseudo-labels and UDA supervision, while utilizing complementary domain-mixing strategies to prevent overfitting and reduce domain discrepancies. Experiments demonstrate that DA-Cal seamlessly integrates with existing self-training frameworks across multiple UDA segmentation benchmarks, significantly improving target domain calibration while delivering performance gains without inference overhead. The code will be released.",
    "summary": "",
    "translation": "DA-Cal：面向语义分割的跨域校准",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于计算机视觉领域的语义分割任务，属于纯粹的视觉研究范畴。虽然提到了跨域校准的概念，但这是针对视觉模型的特定技术，没有展示与推荐系统、搜索或广告领域的潜在应用联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20853v1": {
    "title": "On the Explainability of Vision-Language Models in Art History",
    "url": "https://www.alphaxiv.org/abs/2602.20853v1",
    "arxiv_id": "2602.20853v1",
    "authors": "Stefanie Schneider",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 12:53:28",
    "ori_summary": "Vision-Language Models (VLMs) transfer visual and textual data into a shared embedding space. In so doing, they enable a wide range of multimodal tasks, while also raising critical questions about the nature of machine 'understanding.' In this paper, we examine how Explainable Artificial Intelligence (XAI) methods can render the visual reasoning of a VLM - namely, CLIP - legible in art-historical contexts. To this end, we evaluate seven methods, combining zero-shot localization experiments with human interpretability studies. Our results indicate that, while these methods capture some aspects of human interpretation, their effectiveness hinges on the conceptual stability and representational availability of the examined categories.",
    "summary": "",
    "translation": "论视觉语言模型在艺术史中的可解释性",
    "relevance_score": 1,
    "reasoning": "该论文专注于艺术史领域的视觉语言模型可解释性，属于特定领域应用（艺术史），与推荐系统、搜索或广告的核心技术进展无关。虽然提到了视觉语言模型，但论文焦点是特定领域的可解释性研究，而非通用的异构数据处理技术或推荐/搜索/广告应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20851v1": {
    "title": "Hybrid Fusion: One-Minute Efficient Training for Zero-Shot Cross-Domain Image Fusion",
    "url": "https://www.alphaxiv.org/abs/2602.20851v1",
    "arxiv_id": "2602.20851v1",
    "authors": "Ran Zhang, Xuanhua He, Liu Liu",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 12:47:53",
    "ori_summary": "Image fusion seeks to integrate complementary information from multiple sources into a single, superior image. While traditional methods are fast, they lack adaptability and performance. Conversely, deep learning approaches achieve state-of-the-art (SOTA) results but suffer from critical inefficiencies: their reliance on slow, resource-intensive, patch-based training introduces a significant gap with full-resolution inference. We propose a novel hybrid framework that resolves this trade-off. Our method utilizes a learnable U-Net to generate a dynamic guidance map that directs a classic, fixed Laplacian pyramid fusion kernel. This decoupling of policy learning from pixel synthesis enables remarkably efficient full-resolution training, eliminating the train-inference gap. Consequently, our model achieves SOTA-comparable performance in about one minute on a RTX 4090 or two minutes on a consumer laptop GPU from scratch without any external model and demonstrates powerful zero-shot generalization across diverse tasks, from infrared-visible to medical imaging. By design, the fused output is linearly constructed solely from source information, ensuring high faithfulness for critical applications. The codes are available at https://github.com/Zirconium233/HybridFusion",
    "summary": "",
    "translation": "混合融合：面向零样本跨域图像融合的一分钟高效训练",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于图像融合领域，涉及跨域图像处理和高效训练方法。虽然提到了“零样本”和“高效训练”等概念，但其核心应用场景（图像融合）与推荐系统、搜索或广告的异构数据处理需求没有直接关联。标题中未提及任何与推荐、搜索、广告相关的关键词（如用户、序列、上下文、排名等），也未表明其方法可迁移至这些领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20845v1": {
    "title": "FLIM Networks with Bag of Feature Points",
    "url": "https://www.alphaxiv.org/abs/2602.20845v1",
    "arxiv_id": "2602.20845v1",
    "authors": "João Deltregia Martinelli, Marcelo Luis Rodrigues Filho, Felipe Crispim da Rocha Salvagnini, Gilson Junior Soares, Jefersson A. dos Santos, Alexandre X. Falcão",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 12:36:22",
    "ori_summary": "Convolutional networks require extensive image annotation, which can be costly and time-consuming. Feature Learning from Image Markers (FLIM) tackles this challenge by estimating encoder filters (i.e., kernel weights) from user-drawn markers on discriminative regions of a few representative images without traditional optimization. Such an encoder combined with an adaptive decoder comprises a FLIM network fully trained without backpropagation. Prior research has demonstrated their effectiveness in Salient Object Detection (SOD), being significantly lighter than existing lightweight models. This study revisits FLIM SOD and introduces FLIM-Bag of Feature Points (FLIM-BoFP), a considerably faster filter estimation method. The previous approach, FLIM-Cluster, derives filters through patch clustering at each encoder's block, leading to computational overhead and reduced control over filter locations. FLIM-BoFP streamlines this process by performing a single clustering at the input block, creating a bag of feature points, and defining filters directly from mapped feature points across all blocks. The paper evaluates the benefits in efficiency, effectiveness, and generalization of FLIM-BoFP compared to FLIM-Cluster and other state-of-the-art baselines for parasite detection in optical microscopy images.",
    "summary": "",
    "translation": "基于特征点袋的FLIM网络",
    "relevance_score": 2,
    "reasoning": "该标题涉及FLIM（可能指荧光寿命成像）网络和特征点袋方法，这似乎是计算机视觉或医学成像领域的技术。虽然特征点袋方法在传统计算机视觉中有应用，但该论文没有明确展示与推荐系统、搜索或广告的直接关联，也没有涉及LLM、Transformer架构或异构数据统一建模等当前关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20839v1": {
    "title": "Training-Free Multi-Concept Image Editing",
    "url": "https://www.alphaxiv.org/abs/2602.20839v1",
    "arxiv_id": "2602.20839v1",
    "authors": "Niki Foteinopoulou, Ignas Budvytis, Stephan Liwicki",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 12:27:51",
    "ori_summary": "Editing images with diffusion models without training remains challenging. While recent optimisation-based methods achieve strong zero-shot edits from text, they struggle to preserve identity or capture details that language alone cannot express. Many visual concepts such as facial structure, material texture, or object geometry are impossible to express purely through text prompts alone. To address this gap, we introduce a training-free framework for concept-based image editing, which unifies Optimised DDS with LoRA-driven concept composition, where the training data of the LoRA represent the concept. Our approach enables combining and controlling multiple visual concepts directly within the diffusion process, integrating semantic guidance from text with low-level cues from pretrained concept adapters. We further refine DDS for stability and controllability through ordered timesteps, regularisation, and negative-prompt guidance. Quantitative and qualitative results demonstrate consistent improvements over existing training-free diffusion editing methods on InstructPix2Pix and ComposLoRA benchmarks. Code will be made publicly available.",
    "summary": "",
    "translation": "无需训练的多概念图像编辑",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向图像编辑领域，属于计算机视觉应用范畴。虽然涉及多概念处理，但缺乏与推荐系统、搜索或广告领域的直接联系，也不涉及LLM、Transformer架构或异构数据统一建模等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20818v1": {
    "title": "GatedCLIP: Gated Multimodal Fusion for Hateful Memes Detection",
    "url": "https://www.alphaxiv.org/abs/2602.20818v1",
    "arxiv_id": "2602.20818v1",
    "authors": "Yingying Guo, Ke Zhang, Zirong Zeng",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 11:54:54",
    "ori_summary": "Detecting hateful content in multimodal memes presents unique challenges, as harmful messages often emerge from the complex interplay between benign images and text. We propose GatedCLIP, a Vision-Language model that enhances CLIP's multimodal capabilities with specialized architectural improvements for hateful memes detection. Our approach introduces learned projection heads that map CLIP embeddings to a task-optimized semantic space, a dynamic gated fusion mechanism that adaptively weights visual and textual features, and a contrastive learning objective that maintains cross-modal semantic alignment. Experiments on the Hateful Memes dataset demonstrate that GatedCLIP achieves an AUROC of 0.66, substantially outperforming the CLIP baseline (AUROC 0.49) while maintaining computational efficiency with only 350K trainable parameters.",
    "summary": "",
    "translation": "GatedCLIP：基于门控多模态融合的仇恨表情包检测",
    "relevance_score": 2,
    "reasoning": "该论文虽然涉及多模态融合技术（图像+文本），但其应用领域是仇恨内容检测，属于内容安全/审核范畴，而非推荐系统、搜索或广告的核心排序/匹配任务。门控融合机制本身可能有技术启发，但论文的明确应用场景与当前关注的RecSys/Search/Ads领域直接相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20807v1": {
    "title": "RU4D-SLAM: Reweighting Uncertainty in Gaussian Splatting SLAM for 4D Scene Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2602.20807v1",
    "arxiv_id": "2602.20807v1",
    "authors": "Yangfan Zhao, Hanwei Zhang, Ke Huang, Qiufeng Wang, Zhenzhou Shao, Dengyu Wu",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2026-02-24 11:47:43",
    "ori_summary": "Combining 3D Gaussian splatting with Simultaneous Localization and Mapping (SLAM) has gained popularity as it enables continuous 3D environment reconstruction during motion. However, existing methods struggle in dynamic environments, particularly moving objects complicate 3D reconstruction and, in turn, hinder reliable tracking. The emergence of 4D reconstruction, especially 4D Gaussian splatting, offers a promising direction for addressing these challenges, yet its potential for 4D-aware SLAM remains largely underexplored. Along this direction, we propose a robust and efficient framework, namely Reweighting Uncertainty in Gaussian Splatting SLAM (RU4D-SLAM) for 4D scene reconstruction, that introduces temporal factors into spatial 3D representation while incorporating uncertainty-aware perception of scene changes, blurred image synthesis, and dynamic scene reconstruction. We enhance dynamic scene representation by integrating motion blur rendering, and improve uncertainty-aware tracking by extending per-pixel uncertainty modeling, which is originally designed for static scenarios, to handle blurred images. Furthermore, we propose a semantic-guided reweighting mechanism for per-pixel uncertainty estimation in dynamic scenes, and introduce a learnable opacity weight to support adaptive 4D mapping. Extensive experiments on standard benchmarks demonstrate that our method substantially outperforms state-of-the-art approaches in both trajectory accuracy and 4D scene reconstruction, particularly in dynamic environments with moving objects and low-quality inputs. Code available: https://ru4d-slam.github.io",
    "summary": "",
    "translation": "RU4D-SLAM：用于4D场景重建的高斯溅射SLAM中的不确定性重加权",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的SLAM（同时定位与地图构建）和4D场景重建，属于纯粹的视觉/3D视觉领域。虽然涉及不确定性建模，但论文内容与推荐系统、搜索或广告的排名任务没有直接关联，也不涉及Transformer架构、LLM技术或异构数据统一建模等当前关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20794v1": {
    "title": "VGGDrive: Empowering Vision-Language Models with Cross-View Geometric Grounding for Autonomous Driving",
    "url": "https://www.alphaxiv.org/abs/2602.20794v1",
    "arxiv_id": "2602.20794v1",
    "authors": "Jie Wang, Guang Li, Zhijian Huang, Chenxu Dang, Hangjun Ye, Yahong Han, Long Chen",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 11:33:44",
    "ori_summary": "The significance of cross-view 3D geometric modeling capabilities for autonomous driving is self-evident, yet existing Vision-Language Models (VLMs) inherently lack this capability, resulting in their mediocre performance. While some promising approaches attempt to mitigate this by constructing Q&A data for auxiliary training, they still fail to fundamentally equip VLMs with the ability to comprehensively handle diverse evaluation protocols. We thus chart a new course, advocating for the infusion of VLMs with the cross-view geometric grounding of mature 3D foundation models, closing this critical capability gap in autonomous driving. In this spirit, we propose a novel architecture, VGGDrive, which empowers Vision-language models with cross-view Geometric Grounding for autonomous Driving. Concretely, to bridge the cross-view 3D geometric features from the frozen visual 3D model with the VLM's 2D visual features, we introduce a plug-and-play Cross-View 3D Geometric Enabler (CVGE). The CVGE decouples the base VLM architecture and effectively empowers the VLM with 3D features through a hierarchical adaptive injection mechanism. Extensive experiments show that VGGDrive enhances base VLM performance across five autonomous driving benchmarks, including tasks like cross-view risk perception, motion prediction, and trajectory planning. It's our belief that mature 3D foundation models can empower autonomous driving tasks through effective integration, and we hope our initial exploration demonstrates the potential of this paradigm to the autonomous driving community.",
    "summary": "",
    "translation": "VGGDrive：通过跨视角几何基础赋能视觉语言模型用于自动驾驶",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域的视觉语言模型应用，属于纯粹的视觉/自动驾驶研究范畴。虽然涉及多模态建模，但其应用场景（自动驾驶）与搜索/推荐/广告领域没有直接关联，且论文未提及任何可能应用于这些领域的通用技术或方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20792v1": {
    "title": "SIMSPINE: A Biomechanics-Aware Simulation Framework for 3D Spine Motion Annotation and Benchmarking",
    "url": "https://www.alphaxiv.org/abs/2602.20792v1",
    "arxiv_id": "2602.20792v1",
    "authors": "Muhammad Saif Ullah Khan, Didier Stricker",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 11:31:20",
    "ori_summary": "Modeling spinal motion is fundamental to understanding human biomechanics, yet remains underexplored in computer vision due to the spine's complex multi-joint kinematics and the lack of large-scale 3D annotations. We present a biomechanics-aware keypoint simulation framework that augments existing human pose datasets with anatomically consistent 3D spinal keypoints derived from musculoskeletal modeling. Using this framework, we create the first open dataset, named SIMSPINE, which provides sparse vertebra-level 3D spinal annotations for natural full-body motions in indoor multi-camera capture without external restraints. With 2.14 million frames, this enables data-driven learning of vertebral kinematics from subtle posture variations and bridges the gap between musculoskeletal simulation and computer vision. In addition, we release pretrained baselines covering fine-tuned 2D detectors, monocular 3D pose lifting models, and multi-view reconstruction pipelines, establishing a unified benchmark for biomechanically valid spine motion estimation. Specifically, our 2D spine baselines improve the state-of-the-art from 0.63 to 0.80 AUC in controlled environments, and from 0.91 to 0.93 AP for in-the-wild spine tracking. Together, the simulation framework and SIMSPINE dataset advance research in vision-based biomechanics, motion analysis, and digital human modeling by enabling reproducible, anatomically grounded 3D spine estimation under natural conditions.",
    "summary": "",
    "translation": "SIMSPINE：一种生物力学感知的仿真框架，用于三维脊柱运动标注与基准测试",
    "relevance_score": 1,
    "reasoning": "该论文专注于生物力学和医学领域的3D脊柱运动仿真，属于明确的医学/生物领域应用。标题中没有任何元素表明与推荐系统、搜索、广告、LLM技术或Transformer架构相关，完全属于指定的不相关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20790v1": {
    "title": "Real-time Motion Segmentation with Event-based Normal Flow",
    "url": "https://www.alphaxiv.org/abs/2602.20790v1",
    "arxiv_id": "2602.20790v1",
    "authors": "Sheng Zhong, Zhongyang Ren, Xiya Zhu, Dehao Yuan, Cornelia Fermuller, Yi Zhou",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2026-02-24 11:29:07",
    "ori_summary": "Event-based cameras are bio-inspired sensors with pixels that independently and asynchronously respond to brightness changes at microsecond resolution, offering the potential to handle visual tasks in challenging scenarios. However, due to the sparse information content in individual events, directly processing the raw event data to solve vision tasks is highly inefficient, which severely limits the applicability of state-of-the-art methods in real-time tasks, such as motion segmentation, a fundamental task for dynamic scene understanding. Incorporating normal flow as an intermediate representation to compress motion information from event clusters within a localized region provides a more effective solution. In this work, we propose a normal flow-based motion segmentation framework for event-based vision. Leveraging the dense normal flow directly learned from event neighborhoods as input, we formulate the motion segmentation task as an energy minimization problem solved via graph cuts, and optimize it iteratively with normal flow clustering and motion model fitting. By using a normal flow-based motion model initialization and fitting method, the proposed system is able to efficiently estimate the motion models of independently moving objects with only a limited number of candidate models, which significantly reduces the computational complexity and ensures real-time performance, achieving nearly a 800x speedup in comparison to the open-source state-of-the-art method. Extensive evaluations on multiple public datasets fully demonstrate the accuracy and efficiency of our framework.",
    "summary": "",
    "translation": "基于事件相机法向流的实时运动分割",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的事件相机和运动分割技术，属于纯粹的视觉处理领域。虽然标题涉及实时处理，但没有展示与推荐系统、搜索或广告领域的任何潜在应用联系，也不涉及LLM、Transformer架构或异构数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20773v1": {
    "title": "Federated Learning for Cross-Modality Medical Image Segmentation via Augmentation-Driven Generalization",
    "url": "https://www.alphaxiv.org/abs/2602.20773v1",
    "arxiv_id": "2602.20773v1",
    "authors": "Sachin Dudda Nagaraju, Ashkan Moradi, Bendik Skarre Abrahamsen, Mattijs Elschot",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 11:13:01",
    "ori_summary": "Artificial intelligence has emerged as a transformative tool in medical image analysis, yet developing robust and generalizable segmentation models remains difficult due to fragmented, privacy-constrained imaging data siloed across institutions. While federated learning (FL) enables collaborative model training without centralizing data, cross-modality domain shifts pose a critical challenge, particularly when models trained on one modality fail to generalize to another. Many existing solutions require paired multimodal data per patient or rely on complex architectures, both of which are impractical in real clinical settings. In this work, we consider a realistic FL scenario where each client holds single-modality data (CT or MRI), and systematically investigate augmentation strategies for cross-modality generalization. Using abdominal organ segmentation and whole-heart segmentation as representative multi-class and binary segmentation benchmarks, we evaluate convolution-based spatial augmentation, frequency-domain manipulation, domain-specific normalization, and global intensity nonlinear (GIN) augmentation. Our results show that GIN consistently outperforms alternatives in both centralized and federated settings by simulating cross-modality appearance variations while preserving anatomical structure. For the pancreas, Dice score improved from 0.073 to 0.437, a 498% gain. Our federated approach achieves 93-98% of centralized training accuracy, demonstrating strong cross-modality generalization without compromising data privacy, pointing toward feasible federated AI deployment across diverse healthcare systems.",
    "summary": "",
    "translation": "基于增强驱动泛化的跨模态医学图像分割联邦学习",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及联邦学习和医学图像分割，这两个主题均属于明确列出的无关主题范畴（联邦学习属于隐私/安全相关技术，医学图像属于特定领域应用）。尽管提到了跨模态和泛化，但这些概念在医学领域的应用与推荐系统、搜索或广告没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20752v1": {
    "title": "OrthoDiffusion: A Generalizable Multi-Task Diffusion Foundation Model for Musculoskeletal MRI Interpretation",
    "url": "https://www.alphaxiv.org/abs/2602.20752v1",
    "arxiv_id": "2602.20752v1",
    "authors": "Tian Lan, Lei Xu, Zimu Yuan, Shanggui Liu, Jiajun Liu, Jiaxin Liu, Weilai Xiang, Hongyu Yang, Dong Jiang, Jianxin Yin, Dingyu Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-24 10:29:10",
    "ori_summary": "Musculoskeletal disorders represent a significant global health burden and are a leading cause of disability worldwide. While MRI is essential for accurate diagnosis, its interpretation remains exceptionally challenging. Radiologists must identify multiple potential abnormalities within complex anatomical structures across different imaging planes, a process that requires significant expertise and is prone to variability. We developed OrthoDiffusion, a unified diffusion-based foundation model designed for multi-task musculoskeletal MRI interpretation. The framework utilizes three orientation-specific 3D diffusion models, pre-trained in a self-supervised manner on 15,948 unlabeled knee MRI scans, to learn robust anatomical features from sagittal, coronal, and axial views. These view-specific representations are integrated to support diverse clinical tasks, including anatomical segmentation and multi-label diagnosis. Our evaluation demonstrates that OrthoDiffusion achieves excellent performance in the segmentation of 11 knee structures and the detection of 8 knee abnormalities. The model exhibited remarkable robustness across different clinical centers and MRI field strengths, consistently outperforming traditional supervised models. Notably, in settings where labeled data was scarce, OrthoDiffusion maintained high diagnostic precision using only 10\\% of training labels. Furthermore, the anatomical representations learned from knee imaging proved highly transferable to other joints, achieving strong diagnostic performance across 11 diseases of the ankle and shoulder. These findings suggest that diffusion-based foundation models can serve as a unified platform for multi-disease diagnosis and anatomical segmentation, potentially improving the efficiency and accuracy of musculoskeletal MRI interpretation in real-world clinical workflows.",
    "summary": "",
    "translation": "OrthoDiffusion：一种用于肌肉骨骼MRI解读的可泛化多任务扩散基础模型",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学影像（肌肉骨骼MRI）的特定领域应用，属于明确的无关主题“Medical, Biology, Chemistry, Physics or other domain-specific applications”。虽然涉及扩散模型技术，但标题未表明其在推荐系统、搜索或广告领域的潜在应用，完全不符合当前关注的技术领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20739v1": {
    "title": "PyVision-RL: Forging Open Agentic Vision Models via RL",
    "url": "https://www.alphaxiv.org/abs/2602.20739v1",
    "arxiv_id": "2602.20739v1",
    "authors": "Shitian Zhao, Shaoheng Lin, Ming Li, Haoquan Zhang, Wenshuo Peng, Kaipeng Zhang, Chen Wei",
    "categories": "cs.AI, cs.CV",
    "pub_date": "2026-02-24 10:08:33",
    "ori_summary": "Reinforcement learning for agentic multimodal models often suffers from interaction collapse, where models learn to reduce tool usage and multi-turn reasoning, limiting the benefits of agentic behavior. We introduce PyVision-RL, a reinforcement learning framework for open-weight multimodal models that stabilizes training and sustains interaction. Our approach combines an oversampling-filtering-ranking rollout strategy with an accumulative tool reward to prevent collapse and encourage multi-turn tool use. Using a unified training pipeline, we develop PyVision-Image and PyVision-Video for image and video understanding. For video reasoning, PyVision-Video employs on-demand context construction, selectively sampling task-relevant frames during reasoning to significantly reduce visual token usage. Experiments show strong performance and improved efficiency, demonstrating that sustained interaction and on-demand visual processing are critical for scalable multimodal agents.",
    "summary": "",
    "translation": "PyVision-RL：通过强化学习锻造开放智能视觉模型",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及视觉模型（Vision Models）和强化学习（RL），但未明确说明与推荐系统、搜索或广告的关联。虽然强化学习在推荐系统中可能有应用，但标题聚焦于视觉模型而非推荐系统/搜索/广告的核心问题。根据您的关注点，这属于不相关的主题（视觉论文无明确相关性）。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20731v1": {
    "title": "Communication-Inspired Tokenization for Structured Image Representations",
    "url": "https://www.alphaxiv.org/abs/2602.20731v1",
    "arxiv_id": "2602.20731v1",
    "authors": "Aram Davtyan, Yusuf Sahin, Yasaman Haghighi, Sebastian Stapf, Pablo Acuaviva, Alexandre Alahi, Paolo Favaro",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2026-02-24 09:53:50",
    "ori_summary": "Discrete image tokenizers have emerged as a key component of modern vision and multimodal systems, providing a sequential interface for transformer-based architectures. However, most existing approaches remain primarily optimized for reconstruction and compression, often yielding tokens that capture local texture rather than object-level semantic structure. Inspired by the incremental and compositional nature of human communication, we introduce COMmunication inspired Tokenization (COMiT), a framework for learning structured discrete visual token sequences. COMiT constructs a latent message within a fixed token budget by iteratively observing localized image crops and recurrently updating its discrete representation. At each step, the model integrates new visual information while refining and reorganizing the existing token sequence. After several encoding iterations, the final message conditions a flow-matching decoder that reconstructs the full image. Both encoding and decoding are implemented within a single transformer model and trained end-to-end using a combination of flow-matching reconstruction and semantic representation alignment losses. Our experiments demonstrate that while semantic alignment provides grounding, attentive sequential tokenization is critical for inducing interpretable, object-centric token structure and substantially improving compositional generalization and relational reasoning over prior methods.",
    "summary": "",
    "translation": "面向结构化图像表征的通信启发式分词方法",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及图像表征和分词技术，属于计算机视觉领域，与RecSys/Search/Ads的核心关注点没有直接关联。虽然结构化表征和分词技术在某些多模态系统中可能作为辅助组件，但论文标题未表明其在推荐、搜索或广告中的具体应用潜力，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20725v1": {
    "title": "Bridging Physically Based Rendering and Diffusion Models with Stochastic Differential Equation",
    "url": "https://www.alphaxiv.org/abs/2602.20725v1",
    "arxiv_id": "2602.20725v1",
    "authors": "Junwei Shu, Wenjie Liu, Changgu Chen, Hantang Liu, Yang Li, Changbo Wang",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 09:44:12",
    "ori_summary": "Diffusion-based image generators excel at producing realistic content from text or image conditions, but they offer only limited explicit control over low-level, physically grounded shading and material properties. In contrast, physically based rendering (PBR) offers fine-grained physical control but lacks prompt-driven flexibility. Although these two paradigms originate from distinct communities, both share a common evolution -- from noisy observations to clean images. In this paper, we propose a unified stochastic formulation that bridges Monte Carlo rendering and diffusion-based generative modeling. First, a general stochastic differential equation (SDE) formulation for Monte Carlo integration under the Central Limit Theorem is modeled. Through instantiation via physically based path tracing, we convert it into a physically grounded SDE representation. Moreover, we provide a systematic analysis of how the physical characteristics of path tracing can be extended to existing diffusion models from the perspective of noise variance. Extensive experiments across multiple tasks show that our method can exert physically grounded control over diffusion-generated results, covering tasks such as rendering and material editing.",
    "summary": "",
    "translation": "基于随机微分方程连接物理渲染与扩散模型",
    "relevance_score": 1,
    "reasoning": "该论文涉及计算机图形学中的物理渲染和生成模型中的扩散模型，属于纯粹的视觉/图形领域研究，与推荐系统、搜索或广告的核心技术没有直接关联。虽然扩散模型是生成模型的一种，但论文聚焦于渲染技术结合，没有展示在推荐/搜索/广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20721v1": {
    "title": "CleanStyle: Plug-and-Play Style Conditioning Purification for Text-to-Image Stylization",
    "url": "https://www.alphaxiv.org/abs/2602.20721v1",
    "arxiv_id": "2602.20721v1",
    "authors": "Xiaoman Feng, Mingkun Lei, Yang Wang, Dingwen Fu, Chi Zhang",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 09:33:05",
    "ori_summary": "Style transfer in diffusion models enables controllable visual generation by injecting the style of a reference image. However, recent encoder-based methods, while efficient and tuning-free, often suffer from content leakage, where semantic elements from the style image undesirably appear in the output, impairing prompt fidelity and stylistic consistency. In this work, we introduce CleanStyle, a plug-and-play framework that filters out content-related noise from the style embedding without retraining. Motivated by empirical analysis, we observe that such leakage predominantly stems from the tail components of the style embedding, which are isolated via Singular Value Decomposition (SVD). To address this, we propose CleanStyleSVD (CS-SVD), which dynamically suppresses tail components using a time-aware exponential schedule, providing clean, style-preserving conditional embeddings throughout the denoising process. Furthermore, we present Style-Specific Classifier-Free Guidance (SS-CFG), which reuses the suppressed tail components to construct style-aware unconditional inputs. Unlike conventional methods that use generic negative embeddings (e.g., zero vectors), SS-CFG introduces targeted negative signals that reflect style-specific but prompt-irrelevant visual elements. This enables the model to effectively suppress these distracting patterns during generation, thereby improving prompt fidelity and enhancing the overall visual quality of stylized outputs. Our approach is lightweight, interpretable, and can be seamlessly integrated into existing encoder-based diffusion models without retraining. Extensive experiments demonstrate that CleanStyle substantially reduces content leakage, improves stylization quality and improves prompt alignment across a wide range of style references and prompts.",
    "summary": "",
    "translation": "CleanStyle：即插即用的文本到图像风格化中的风格条件净化",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本到图像生成的风格化技术，属于纯粹的AIGC/内容生成领域。虽然涉及条件控制技术，但论文标题明确指向图像生成应用，与推荐系统、搜索或广告中的排名、检索、用户建模等核心问题没有直接关联，也不属于能够应用于这些领域的使能技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20718v1": {
    "title": "Monocular Endoscopic Tissue 3D Reconstruction with Multi-Level Geometry Regularization",
    "url": "https://www.alphaxiv.org/abs/2602.20718v1",
    "arxiv_id": "2602.20718v1",
    "authors": "Yangsen Chen, Hao Wang",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 09:29:36",
    "ori_summary": "Reconstructing deformable endoscopic tissues is crucial for achieving robot-assisted surgery. However, 3D Gaussian Splatting-based approaches encounter challenges in achieving consistent tissue surface reconstruction, while existing NeRF-based methods lack real-time rendering capabilities. In pursuit of both smooth deformable surfaces and real-time rendering, we introduce a novel approach based on 3D Gaussian Splatting. Specifically, we introduce surface-aware reconstruction, initially employing a Sign Distance Field-based method to construct a mesh, subsequently utilizing this mesh to constrain the Gaussian Splatting reconstruction process. Furthermore, to ensure the generation of physically plausible deformations, we incorporate local rigidity and global non-rigidity restrictions to guide Gaussian deformation, tailored for the highly deformable nature of soft endoscopic tissue. Based on 3D Gaussian Splatting, our proposed method delivers a fast rendering process and smooth surface appearances. Quantitative and qualitative analysis against alternative methodologies shows that our approach achieves solid reconstruction quality in both textures and geometries.",
    "summary": "",
    "translation": "基于多级几何正则化的单目内窥镜组织三维重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学内窥镜成像和3D重建技术，属于医学/生物领域的特定应用。虽然涉及3D视觉，但论文明确针对内窥镜组织重建这一医疗场景，与推荐系统、搜索或广告领域没有直接关联，也不涉及LLM、Transformer架构或异构数据处理等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20709v1": {
    "title": "Onboard-Targeted Segmentation of Straylight in Space Camera Sensors",
    "url": "https://www.alphaxiv.org/abs/2602.20709v1",
    "arxiv_id": "2602.20709v1",
    "authors": "Riccardo Gallon, Fabian Schiemenz, Alessandra Menicucci, Eberhard Gill",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-24 09:15:13",
    "ori_summary": "This study details an artificial intelligence (AI)-based methodology for the semantic segmentation of space camera faults. Specifically, we address the segmentation of straylight effects induced by solar presence around the camera's Field of View (FoV). Anomalous images are sourced from our published dataset. Our approach emphasizes generalization across diverse flare textures, leveraging pre-training on a public dataset (Flare7k++) including flares in various non-space contexts to mitigate the scarcity of realistic space-specific data. A DeepLabV3 model with MobileNetV3 backbone performs the segmentation task. The model design targets deployment in spacecraft resource-constrained hardware. Finally, based on a proposed interface between our model and the onboard navigation pipeline, we develop custom metrics to assess the model's performance in the system-level context.",
    "summary": "",
    "translation": "空间相机传感器杂散光星上目标分割",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及空间相机传感器中的杂散光分割技术，属于计算机视觉在航天领域的特定应用。这与推荐系统、搜索、广告或相关使能技术（如LLM、Transformer架构）无直接关联，也不涉及异构数据统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20700v1": {
    "title": "NGL-Prompter: Training-Free Sewing Pattern Estimation from a Single Image",
    "url": "https://www.alphaxiv.org/abs/2602.20700v1",
    "arxiv_id": "2602.20700v1",
    "authors": "Anna Badalyan, Pratheba Selvaraju, Giorgio Becherini, Omid Taheri, Victoria Fernandez Abrevaya, Michael Black",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 09:01:11",
    "ori_summary": "Estimating sewing patterns from images is a practical approach for creating high-quality 3D garments. Due to the lack of real-world pattern-image paired data, prior approaches fine-tune large vision language models (VLMs) on synthetic garment datasets generated by randomly sampling from a parametric garment model GarmentCode. However, these methods often struggle to generalize to in-the-wild images, fail to capture real-world correlations between garment parts, and are typically restricted to single-layer outfits. In contrast, we observe that VLMs are effective at describing garments in natural language, yet perform poorly when asked to directly regress GarmentCode parameters from images. To bridge this gap, we propose NGL (Natural Garment Language), a novel intermediate language that restructures GarmentCode into a representation more understandable to language models. Leveraging this language, we introduce NGL-Prompter, a training-free pipeline that queries large VLMs to extract structured garment parameters, which are then deterministically mapped to valid GarmentCode. We evaluate our method on the Dress4D, CloSe and a newly collected dataset of approximately 5,000 in-the-wild fashion images. Our approach achieves state-of-the-art performance on standard geometry metrics and is strongly preferred in both human and GPT-based perceptual evaluations compared to existing baselines. Furthermore, NGL-prompter can recover multi-layer outfits whereas competing methods focus mostly on single-layer garments, highlighting its strong generalization to real-world images even with occluded parts. These results demonstrate that accurate sewing pattern reconstruction is possible without costly model training. Our code and data will be released for research use.",
    "summary": "",
    "translation": "NGL-Prompter：基于单张图像的免训练缝纫图案估计",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及计算机视觉中的图像到图案估计任务，属于特定领域（服装/缝纫）的视觉应用。根据用户设定的无关主题标准，该论文属于'Purely Vision、3D Vision, Graphic or Speech papers without clear relevance to RecSys/Search/Ads'类别，与推荐系统、搜索或广告的核心技术进展、LLM技术应用、Transformer架构改进或异构数据统一建模均无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20689v1": {
    "title": "MatchED: Crisp Edge Detection Using End-to-End, Matching-based Supervision",
    "url": "https://www.alphaxiv.org/abs/2602.20689v1",
    "arxiv_id": "2602.20689v1",
    "authors": "Bedrettin Cetinkaya, Sinan Kalkan, Emre Akbas",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 08:45:49",
    "ori_summary": "Generating crisp, i.e., one-pixel-wide, edge maps remains one of the fundamental challenges in edge detection, affecting both traditional and learning-based methods. To obtain crisp edges, most existing approaches rely on two hand-crafted post-processing algorithms, Non-Maximum Suppression (NMS) and skeleton-based thinning, which are non-differentiable and hinder end-to-end optimization. Moreover, all existing crisp edge detection methods still depend on such post-processing to achieve satisfactory results. To address this limitation, we propose \\MethodLPP, a lightweight, only $\\sim$21K additional parameters, and plug-and-play matching-based supervision module that can be appended to any edge detection model for joint end-to-end learning of crisp edges. At each training iteration, \\MethodLPP performs one-to-one matching between predicted and ground-truth edges based on spatial distance and confidence, ensuring consistency between training and testing protocols. Extensive experiments on four popular datasets demonstrate that integrating \\MethodLPP substantially improves the performance of existing edge detection models. In particular, \\MethodLPP increases the Average Crispness (AC) metric by up to 2--4$\\times$ compared to baseline models. Under the crispness-emphasized evaluation (CEval), \\MethodLPP further boosts baseline performance by up to 20--35\\% in ODS and achieves similar gains in OIS and AP, achieving SOTA performance that matches or surpasses standard post-processing for the first time. Code is available at https://cvpr26-matched.github.io.",
    "summary": "",
    "translation": "MatchED：基于端到端匹配监督的清晰边缘检测",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉中的边缘检测任务，属于纯粹的视觉技术研究。虽然边缘检测在图像处理中有应用，但论文标题没有表明与推荐系统、搜索或广告有任何直接或间接的联系，也没有涉及LLM、Transformer架构或异构数据建模等当前关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20685v1": {
    "title": "RAYNOVA: 3D-Geometry-Free Auto-Regressive Driving World Modeling with Unified Spatio-Temporal Representation",
    "url": "https://www.alphaxiv.org/abs/2602.20685v1",
    "arxiv_id": "2602.20685v1",
    "authors": "Yichen Xie, Chensheng Peng, Mazen Abdelfattah, Yihan Hu, Jiezhi Yang, Eric Higgins, Ryan Brigden, Masayoshi Tomizuka, Wei Zhan",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 08:41:40",
    "ori_summary": "World foundation models aim to simulate the evolution of the real world with physically plausible behavior. Unlike prior methods that handle spatial and temporal correlations separately, we propose RAYNOVA, a geometry-free world model that employs a dual-causal autoregressive framework. It follows both scale-wise and temporal topological orders in the autoregressive process, and leverages global attention for unified 4D spatio-temporal reasoning. Different from existing works that impose strong 3D geometric priors, RAYNOVA constructs an isotropic spatio-temporal representation across views, frames, and scales based on relative Plücker-ray positional encoding, enabling robust generalization to diverse camera setups and ego motions. We further introduce a recurrent training paradigm to alleviate distribution drift in long-horizon video generation. RAYNOVA achieves state-of-the-art multi-view video generation results on nuScenes, while offering higher throughput and strong controllability under diverse input conditions, generalizing to novel views and camera configurations without explicit 3D scene representation. Our code will be released at http://yichen928.github.io/raynova.",
    "summary": "",
    "translation": "RAYNOVA：基于统一时空表示的无3D几何自回归驾驶世界建模",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于3D几何、自动驾驶世界建模和时空表示，属于纯粹的自动驾驶/计算机视觉领域。虽然涉及时空建模技术，但缺乏与推荐系统、搜索或广告领域的明确关联或潜在应用场景，完全属于被排除的'Purely Vision、3D Vision, Graphic或Speech papers without clear relevance to RecSys/Search/Ads'类别。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20672v1": {
    "title": "BBQ-to-Image: Numeric Bounding Box and Qolor Control in Large-Scale Text-to-Image Models",
    "url": "https://www.alphaxiv.org/abs/2602.20672v1",
    "arxiv_id": "2602.20672v1",
    "authors": "Eliran Kachlon, Alexander Visheratin, Nimrod Sarid, Tal Hacham, Eyal Gutflaish, Saar Huberman, Hezi Zisman, David Ruppin, Ron Mokady",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 08:22:42",
    "ori_summary": "Text-to-image models have rapidly advanced in realism and controllability, with recent approaches leveraging long, detailed captions to support fine-grained generation. However, a fundamental parametric gap remains: existing models rely on descriptive language, whereas professional workflows require precise numeric control over object location, size, and color. In this work, we introduce BBQ, a large-scale text-to-image model that directly conditions on numeric bounding boxes and RGB triplets within a unified structured-text framework. We obtain precise spatial and chromatic control by training on captions enriched with parametric annotations, without architectural modifications or inference-time optimization. This also enables intuitive user interfaces such as object dragging and color pickers, replacing ambiguous iterative prompting with precise, familiar controls. Across comprehensive evaluations, BBQ achieves strong box alignment and improves RGB color fidelity over state-of-the-art baselines. More broadly, our results support a new paradigm in which user intent is translated into an intermediate structured language, consumed by a flow-based transformer acting as a renderer and naturally accommodating numeric parameters.",
    "summary": "",
    "translation": "BBQ转图像：大规模文本到图像模型中的数值边界框与颜色控制",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本到图像生成中的边界框和颜色控制，属于纯粹的视觉内容生成领域。虽然涉及大规模模型，但论文内容与推荐系统、搜索或广告中的排名、检索、个性化等核心任务没有直接关联，也不涉及将异构数据作为不同模态进行统一建模的思路。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20673v1": {
    "title": "GA-Drive: Geometry-Appearance Decoupled Modeling for Free-viewpoint Driving Scene Generatio",
    "url": "https://www.alphaxiv.org/abs/2602.20673v1",
    "arxiv_id": "2602.20673v1",
    "authors": "Hao Zhang, Lue Fan, Qitai Wang, Wenbo Li, Zehuan Wu, Lewei Lu, Zhaoxiang Zhang, Hongsheng Li",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 08:22:42",
    "ori_summary": "A free-viewpoint, editable, and high-fidelity driving simulator is crucial for training and evaluating end-to-end autonomous driving systems. In this paper, we present GA-Drive, a novel simulation framework capable of generating camera views along user-specified novel trajectories through Geometry-Appearance Decoupling and Diffusion-Based Generation. Given a set of images captured along a recorded trajectory and the corresponding scene geometry, GA-Drive synthesizes novel pseudo-views using geometry information. These pseudo-views are then transformed into photorealistic views using a trained video diffusion model. In this way, we decouple the geometry and appearance of scenes. An advantage of such decoupling is its support for appearance editing via state-of-the-art video-to-video editing techniques, while preserving the underlying geometry, enabling consistent edits across both original and novel trajectories. Extensive experiments demonstrate that GA-Drive substantially outperforms existing methods in terms of NTA-IoU, NTL-IoU, and FID scores.",
    "summary": "",
    "translation": "GA-Drive：面向自由视点驾驶场景生成的几何-外观解耦建模",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及计算机视觉中的3D场景生成技术，属于视觉领域而非推荐系统、搜索或广告的核心范畴。虽然解耦建模思想可能对处理异构数据有启发，但论文明确聚焦于驾驶场景的视觉生成，缺乏与推荐/搜索/广告系统的直接关联或明确的应用潜力说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20666v1": {
    "title": "BoxSplitGen: A Generative Model for 3D Part Bounding Boxes in Varying Granularity",
    "url": "https://www.alphaxiv.org/abs/2602.20666v1",
    "arxiv_id": "2602.20666v1",
    "authors": "Juil Koo, Wei-Tung Lin, Chanho Park, Chanhyeok Park, Minhyuk Sung",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 08:15:25",
    "ori_summary": "Human creativity follows a perceptual process, moving from abstract ideas to finer details during creation. While 3D generative models have advanced dramatically, models specifically designed to assist human imagination in 3D creation -- particularly for detailing abstractions from coarse to fine -- have not been explored. We propose a framework that enables intuitive and interactive 3D shape generation by iteratively splitting bounding boxes to refine the set of bounding boxes. The main technical components of our framework are two generative models: the box-splitting generative model and the box-to-shape generative model. The first model, named BoxSplitGen, generates a collection of 3D part bounding boxes with varying granularity by iteratively splitting coarse bounding boxes. It utilizes part bounding boxes created through agglomerative merging and learns the reverse of the merging process -- the splitting sequences. The model consists of two main components: the first learns the categorical distribution of the box to be split, and the second learns the distribution of the two new boxes, given the set of boxes and the indication of which box to split. The second model, the box-to-shape generative model, is trained by leveraging the 3D shape priors learned by an existing 3D diffusion model while adapting the model to incorporate bounding box conditioning. In our experiments, we demonstrate that the box-splitting generative model outperforms token prediction models and the inpainting approach with an unconditional diffusion model. Also, we show that our box-to-shape model, based on a state-of-the-art 3D diffusion model, provides superior results compared to a previous model.",
    "summary": "",
    "translation": "BoxSplitGen：一种用于不同粒度三维部件边界框的生成模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D视觉中的边界框生成，属于纯粹的计算机视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。标题中提到的“生成模型”和“不同粒度”可能涉及生成式AI，但具体应用局限于3D部件分割，没有表明在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20664v1": {
    "title": "AnimeAgent: Is the Multi-Agent via Image-to-Video models a Good Disney Storytelling Artist?",
    "url": "https://www.alphaxiv.org/abs/2602.20664v1",
    "arxiv_id": "2602.20664v1",
    "authors": "Hailong Yan, Shice Liu, Tao Wang, Xiangtao Zhang, Yijie Zhong, Jinwei Chen, Le Zhang, Bo Li",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 08:14:24",
    "ori_summary": "Custom Storyboard Generation (CSG) aims to produce high-quality, multi-character consistent storytelling. Current approaches based on static diffusion models, whether used in a one-shot manner or within multi-agent frameworks, face three key limitations: (1) Static models lack dynamic expressiveness and often resort to \"copy-paste\" pattern. (2) One-shot inference cannot iteratively correct missing attributes or poor prompt adherence. (3) Multi-agents rely on non-robust evaluators, ill-suited for assessing stylized, non-realistic animation. To address these, we propose AnimeAgent, the first Image-to-Video (I2V)-based multi-agent framework for CSG. Inspired by Disney's \"Combination of Straight Ahead and Pose to Pose\" workflow, AnimeAgent leverages I2V's implicit motion prior to enhance consistency and expressiveness, while a mixed subjective-objective reviewer enables reliable iterative refinement. We also collect a human-annotated CSG benchmark with ground-truth. Experiments show AnimeAgent achieves SOTA performance in consistency, prompt fidelity, and stylization.",
    "summary": "",
    "translation": "AnimeAgent：基于图像到视频模型的多智能体是否是一位优秀的迪士尼故事讲述艺术家？",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于使用图像到视频模型进行迪士尼风格动画故事创作，这属于AIGC/内容生成领域，与您的核心关注点（推荐系统、搜索、广告中的LLM应用、Transformer架构进展等）完全无关。标题中提到的“多智能体”可能指技术实现方式，但整体应用场景是纯粹的创意内容生成，而非推荐、搜索或广告中的技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20658v1": {
    "title": "Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video",
    "url": "https://www.alphaxiv.org/abs/2602.20658v1",
    "arxiv_id": "2602.20658v1",
    "authors": "Mohammad Sadra Rajabi, Aanuoluwapo Ojelade, Sunwook Kim, Maury A. Nussbaum",
    "categories": "cs.CV, cs.AI, cs.HC, cs.LG",
    "pub_date": "2026-02-24 08:01:49",
    "ori_summary": "Manual lifting tasks are a major contributor to work-related musculoskeletal disorders, and effective ergonomic risk assessment is essential for quantifying physical exposure and informing ergonomic interventions. The Revised NIOSH Lifting Equation (RNLE) is a widely used ergonomic risk assessment tool for lifting tasks that relies on six task variables, including horizontal (H) and vertical (V) hand distances; such distances are typically obtained through manual measurement or specialized sensing systems and are difficult to use in real-world environments. We evaluated the feasibility of using innovative vision-language models (VLMs) to non-invasively estimate H and V from RGB video streams. Two multi-stage VLM-based pipelines were developed: a text-guided detection-only pipeline and a detection-plus-segmentation pipeline. Both pipelines used text-guided localization of task-relevant regions of interest, visual feature extraction from those regions, and transformer-based temporal regression to estimate H and V at the start and end of a lift. For a range of lifting tasks, estimation performance was evaluated using leave-one-subject-out validation across the two pipelines and seven camera view conditions. Results varied significantly across pipelines and camera view conditions, with the segmentation-based, multi-view pipeline consistently yielding the smallest errors, achieving mean absolute errors of approximately 6-8 cm when estimating H and 5-8 cm when estimating V. Across pipelines and camera view configurations, pixel-level segmentation reduced estimation error by approximately 20-30% for H and 35-40% for V relative to the detection-only pipeline. These findings support the feasibility of VLM-based pipelines for video-based estimation of RNLE distance parameters.",
    "summary": "",
    "translation": "基于视觉语言模型的手动举升任务人机工程学评估：从RGB视频估计水平和垂直手部距离",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于人机工程学评估和物理动作分析，属于医疗/生物力学领域应用。虽然提到了Vision-Language Models，但应用场景是手动举升任务的距离估计，与推荐系统、搜索或广告的核心技术需求无直接关联。该研究属于明确的无关主题（医疗/生物力学应用），没有任何RecSys/Search/Ads的潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20653v1": {
    "title": "SD4R: Sparse-to-Dense Learning for 3D Object Detection with 4D Radar",
    "url": "https://www.alphaxiv.org/abs/2602.20653v1",
    "arxiv_id": "2602.20653v1",
    "authors": "Xiaokai Bai, Jiahao Cheng, Songkai Wang, Yixuan Luo, Lianqing Zheng, Xiaohan Zhang, Si-Yuan Cao, Hui-Liang Shen",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 07:57:19",
    "ori_summary": "4D radar measurements offer an affordable and weather-robust solution for 3D perception. However, the inherent sparsity and noise of radar point clouds present significant challenges for accurate 3D object detection, underscoring the need for effective and robust point clouds densification. Despite recent progress, existing densification methods often fail to address the extreme sparsity of 4D radar point clouds and exhibit limited robustness when processing scenes with a small number of points. In this paper, we propose SD4R, a novel framework that transforms sparse radar point clouds into dense representations. SD4R begins by utilizing a foreground point generator (FPG) to mitigate noise propagation and produce densified point clouds. Subsequently, a logit-query encoder (LQE) enhances conventional pillarization, resulting in robust feature representations. Through these innovations, our SD4R demonstrates strong capability in both noise reduction and foreground point densification. Extensive experiments conducted on the publicly available View-of-Delft dataset demonstrate that SD4R achieves state-of-the-art performance. Source code is available at https://github.com/lancelot0805/SD4R.",
    "summary": "",
    "translation": "SD4R：面向4D雷达三维目标检测的稀疏到密集学习",
    "relevance_score": 1,
    "reasoning": "该论文专注于4D雷达的3D目标检测，属于纯粹的计算机视觉/自动驾驶领域。虽然涉及稀疏到密集的学习方法，但其核心应用场景（自动驾驶感知）与推荐系统、搜索或广告领域没有直接关联，也不属于Transformer架构、LLM技术或异构数据统一建模的相关研究。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20650v1": {
    "title": "Dataset Color Quantization: A Training-Oriented Framework for Dataset-Level Compression",
    "url": "https://www.alphaxiv.org/abs/2602.20650v1",
    "arxiv_id": "2602.20650v1",
    "authors": "Chenyue Yu, Lingao Xiao, Jinhong Deng, Ivor W. Tsang, Yang He",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-24 07:53:58",
    "ori_summary": "Large-scale image datasets are fundamental to deep learning, but their high storage demands pose challenges for deployment in resource-constrained environments. While existing approaches reduce dataset size by discarding samples, they often ignore the significant redundancy within each image -- particularly in the color space. To address this, we propose Dataset Color Quantization (DCQ), a unified framework that compresses visual datasets by reducing color-space redundancy while preserving information crucial for model training. DCQ achieves this by enforcing consistent palette representations across similar images, selectively retaining semantically important colors guided by model perception, and maintaining structural details necessary for effective feature learning. Extensive experiments across CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K show that DCQ significantly improves training performance under aggressive compression, offering a scalable and robust solution for dataset-level storage reduction. Code is available at \\href{https://github.com/he-y/Dataset-Color-Quantization}{https://github.com/he-y/Dataset-Color-Quantization}.",
    "summary": "",
    "translation": "数据集颜色量化：一种面向训练的数据集级压缩框架",
    "relevance_score": 2,
    "reasoning": "该论文标题聚焦于数据集压缩技术，特别是颜色量化方法，这属于通用数据预处理或存储优化范畴。虽然压缩技术可能间接影响推荐/搜索系统的数据处理效率，但论文未明确涉及Transformer架构、LLM技术、多模态建模或直接应用于推荐/搜索/广告的核心算法，与当前关注的五大方向关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20636v1": {
    "title": "SurgAtt-Tracker: Online Surgical Attention Tracking via Temporal Proposal Reranking and Motion-Aware Refinement",
    "url": "https://www.alphaxiv.org/abs/2602.20636v1",
    "arxiv_id": "2602.20636v1",
    "authors": "Rulin Zhou, Guankun Wang, An Wang, Yujie Ma, Lixin Ouyang, Bolin Cui, Junyan Li, Chaowei Zhu, Mingyang Li, Ming Chen, Xiaopin Zhong, Peng Lu, Jiankun Wang, Xianming Liu, Hongliang Ren",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-24 07:30:51",
    "ori_summary": "Accurate and stable field-of-view (FoV) guidance is critical for safe and efficient minimally invasive surgery, yet existing approaches often conflate visual attention estimation with downstream camera control or rely on direct object-centric assumptions. In this work, we formulate surgical attention tracking as a spatio-temporal learning problem and model surgeon focus as a dense attention heatmap, enabling continuous and interpretable frame-wise FoV guidance. We propose SurgAtt-Tracker, a holistic framework that robustly tracks surgical attention by exploiting temporal coherence through proposal-level reranking and motion-aware refinement, rather than direct regression. To support systematic training and evaluation, we introduce SurgAtt-1.16M, a large-scale benchmark with a clinically grounded annotation protocol that enables comprehensive heatmap-based attention analysis across procedures and institutions. Extensive experiments on multiple surgical datasets demonstrate that SurgAtt-Tracker consistently achieves state-of-the-art performance and strong robustness under occlusion, multi-instrument interference, and cross-domain settings. Beyond attention tracking, our approach provides a frame-wise FoV guidance signal that can directly support downstream robotic FoV planning and automatic camera control.",
    "summary": "",
    "translation": "SurgAtt-Tracker：通过时序提案重排序与运动感知精细化实现在线手术注意力追踪",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向手术场景中的视觉注意力追踪，属于计算机视觉在医疗领域的特定应用。虽然涉及时序建模和注意力机制，但其核心是医疗手术分析，与推荐系统、搜索或广告的技术焦点完全无关，也不符合任何指定的技术趋势类别。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20632v1": {
    "title": "Boosting Instance Awareness via Cross-View Correlation with 4D Radar and Camera for 3D Object Detection",
    "url": "https://www.alphaxiv.org/abs/2602.20632v1",
    "arxiv_id": "2602.20632v1",
    "authors": "Xiaokai Bai, Lianqing Zheng, Si-Yuan Cao, Xiaohan Zhang, Zhe Wu, Beinan Yu, Fang Wang, Jie Bai, Hui-Liang Shen",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 07:25:53",
    "ori_summary": "4D millimeter-wave radar has emerged as a promising sensing modality for autonomous driving due to its robustness and affordability. However, its sparse and weak geometric cues make reliable instance activation difficult, limiting the effectiveness of existing radar-camera fusion paradigms. BEV-level fusion offers global scene understanding but suffers from weak instance focus, while perspective-level fusion captures instance details but lacks holistic context. To address these limitations, we propose SIFormer, a scene-instance aware transformer for 3D object detection using 4D radar and camera. SIFormer first suppresses background noise during view transformation through segmentation- and depth-guided localization. It then introduces a cross-view activation mechanism that injects 2D instance cues into BEV space, enabling reliable instance awareness under weak radar geometry. Finally, a transformer-based fusion module aggregates complementary image semantics and radar geometry for robust perception. As a result, with the aim of enhancing instance awareness, SIFormer bridges the gap between the two paradigms, combining their complementary strengths to address inherent sparse nature of radar and improve detection accuracy. Experiments demonstrate that SIFormer achieves state-of-the-art performance on View-of-Delft, TJ4DRadSet and NuScenes datasets. Source code is available at github.com/shawnnnkb/SIFormer.",
    "summary": "",
    "translation": "通过4D雷达与相机跨视图相关性增强实例感知以实现3D目标检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域的多模态传感器融合（4D雷达与相机）和3D目标检测技术，属于纯粹的计算机视觉/自动驾驶研究方向。论文内容与推荐系统、搜索、广告等核心业务领域无直接关联，也不涉及LLM、Transformer架构改进或异构数据统一建模等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20630v1": {
    "title": "From Pairs to Sequences: Track-Aware Policy Gradients for Keypoint Detection",
    "url": "https://www.alphaxiv.org/abs/2602.20630v1",
    "arxiv_id": "2602.20630v1",
    "authors": "Yepeng Liu, Hao Li, Liwen Yang, Fangzhen Li, Xudi Ge, Yuliang Gu, kuang Gao, Bing Wang, Guang Chen, Hangjun Ye, Yongchao Xu",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 07:24:25",
    "ori_summary": "Keypoint-based matching is a fundamental component of modern 3D vision systems, such as Structure-from-Motion (SfM) and SLAM. Most existing learning-based methods are trained on image pairs, a paradigm that fails to explicitly optimize for the long-term trackability of keypoints across sequences under challenging viewpoint and illumination changes. In this paper, we reframe keypoint detection as a sequential decision-making problem. We introduce TraqPoint, a novel, end-to-end Reinforcement Learning (RL) framework designed to optimize the \\textbf{Tra}ck-\\textbf{q}uality (Traq) of keypoints directly on image sequences. Our core innovation is a track-aware reward mechanism that jointly encourages the consistency and distinctiveness of keypoints across multiple views, guided by a policy gradient method. Extensive evaluations on sparse matching benchmarks, including relative pose estimation and 3D reconstruction, demonstrate that TraqPoint significantly outperforms some state-of-the-art (SOTA) keypoint detection and description methods.",
    "summary": "",
    "translation": "从点对到序列：基于轨迹感知策略梯度的关键点检测",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及计算机视觉中的关键点检测和强化学习技术，属于纯粹的视觉处理领域。虽然提到了策略梯度（强化学习的一种方法），但论文聚焦于视觉关键点检测任务，与推荐系统、搜索或广告的核心技术（如排序、召回、用户建模等）没有直接关联，也不属于Transformer架构改进或LLM应用范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20627v1": {
    "title": "Object-Scene-Camera Decomposition and Recomposition for Data-Efficient Monocular 3D Object Detection",
    "url": "https://www.alphaxiv.org/abs/2602.20627v1",
    "arxiv_id": "2602.20627v1",
    "authors": "Zhaonian Kuang, Rui Ding, Meng Yang, Xinhu Zheng, Gang Hua",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2026-02-24 07:22:58",
    "ori_summary": "Monocular 3D object detection (M3OD) is intrinsically ill-posed, hence training a high-performance deep learning based M3OD model requires a humongous amount of labeled data with complicated visual variation from diverse scenes, variety of objects and camera poses.However, we observe that, due to strong human bias, the three independent entities, i.e., object, scene, and camera pose, are always tightly entangled when an image is captured to construct training data. More specifically, specific 3D objects are always captured in particular scenes with fixed camera poses, and hence lacks necessary diversity. Such tight entanglement induces the challenging issues of insufficient utilization and overfitting to uniform training data. To mitigate this, we propose an online object-scene-camera decomposition and recomposition data manipulation scheme to more efficiently exploit the training data. We first fully decompose training images into textured 3D object point models and background scenes in an efficient computation and storage manner. We then continuously recompose new training images in each epoch by inserting the 3D objects into the freespace of the background scenes, and rendering them with perturbed camera poses from textured 3D point representation. In this way, the refreshed training data in all epochs can cover the full spectrum of independent object, scene, and camera pose combinations. This scheme can serve as a plug-and-play component to boost M3OD models, working flexibly with both fully and sparsely supervised settings. In the sparsely-supervised setting, objects closest to the ego-camera for all instances are sparsely annotated. We then can flexibly increase the annotated objects to control annotation cost. For validation, our method is widely applied to five representative M3OD models and evaluated on both the KITTI and the more complicated Waymo datasets.",
    "summary": "",
    "translation": "面向数据高效的单目3D目标检测的对象-场景-相机分解与重组",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的单目3D目标检测，属于纯粹的视觉领域研究，没有明确涉及推荐系统、搜索或广告应用。标题中提到的分解与重组技术是针对视觉数据处理的，与我的关注点中的异构数据处理（如用户序列和上下文特征）没有直接关联，也不属于LLM、Transformer架构或直接应用于RecSys/Search/Ads的技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20618v1": {
    "title": "RecoverMark: Robust Watermarking for Localization and Recovery of Manipulated Faces",
    "url": "https://www.alphaxiv.org/abs/2602.20618v1",
    "arxiv_id": "2602.20618v1",
    "authors": "Haonan An, Xiaohui Ye, Guang Hua, Yihang Tao, Hangcheng Cao, Xiangyu Yu, Yuguang Fang",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 07:11:40",
    "ori_summary": "The proliferation of AI-generated content has facilitated sophisticated face manipulation, severely undermining visual integrity and posing unprecedented challenges to intellectual property. In response, a common proactive defense leverages fragile watermarks to detect, localize, or even recover manipulated regions. However, these methods always assume an adversary unaware of the embedded watermark, overlooking their inherent vulnerability to watermark removal attacks. Furthermore, this fragility is exacerbated in the commonly used dual-watermark strategy that adds a robust watermark for image ownership verification, where mutual interference and limited embedding capacity reduce the fragile watermark's effectiveness. To address the gap, we propose RecoverMark, a watermarking framework that achieves robust manipulation localization, content recovery, and ownership verification simultaneously. Our key insight is twofold. First, we exploit a critical real-world constraint: an adversary must preserve the background's semantic consistency to avoid visual detection, even if they apply global, imperceptible watermark removal attacks. Second, using the image's own content (face, in this paper) as the watermark enhances extraction robustness. Based on these insights, RecoverMark treats the protected face content itself as the watermark and embeds it into the surrounding background. By designing a robust two-stage training paradigm with carefully crafted distortion layers that simulate comprehensive potential attacks and a progressive training strategy, RecoverMark achieves a robust watermark embedding in no fragile manner for image manipulation localization, recovery, and image IP protection simultaneously. Extensive experiments demonstrate the proposed RecoverMark's robustness against both seen and unseen attacks and its generalizability to in-distribution and out-of-distribution data.",
    "summary": "",
    "translation": "RecoverMark：用于人脸篡改定位与恢复的鲁棒水印技术",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的人脸水印和篡改恢复技术，属于纯粹的视觉应用。虽然水印技术可能在某些场景下与内容安全相关，但论文标题明确指向人脸处理这一特定视觉任务，与推荐系统、搜索、广告的核心技术（如排序、召回、用户建模、Transformer架构改进等）无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20616v1": {
    "title": "Knowing the Unknown: Interpretable Open-World Object Detection via Concept Decomposition Model",
    "url": "https://www.alphaxiv.org/abs/2602.20616v1",
    "arxiv_id": "2602.20616v1",
    "authors": "Xueqiang Lv, Shizhou Zhang, Yinghui Xing, Di Xu, Peng Wang, Yanning Zhang",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-02-24 07:08:47",
    "ori_summary": "Open-world object detection (OWOD) requires incrementally detecting known categories while reliably identifying unknown objects. Existing methods primarily focus on improving unknown recall, yet overlook interpretability, often leading to known-unknown confusion and reduced prediction reliability. This paper aims to make the entire OWOD framework interpretable, enabling the detector to truly \"knowing the unknown\". To this end, we propose a concept-driven InterPretable OWOD framework(IPOW) by introducing a Concept Decomposition Model (CDM) for OWOD, which explicitly decomposes the coupled RoI features in Faster R-CNN into discriminative, shared, and background concepts. Discriminative concepts identify the most discriminative features to enlarge the distances between known categories, while shared and background concepts, due to their strong generalization ability, can be readily transferred to detect unknown categories. Leveraging the interpretable framework, we identify that known-unknown confusion arises when unknown objects fall into the discriminative space of known classes. To address this, we propose Concept-Guided Rectification (CGR) to further resolve such confusion. Extensive experiments show that IPOW significantly improves unknown recall while mitigating confusion, and provides concept-level interpretability for both known and unknown predictions.",
    "summary": "",
    "translation": "知晓未知：通过概念分解模型实现可解释的开放世界目标检测",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的开放世界目标检测和可解释性，属于纯粹的视觉研究范畴。虽然涉及概念分解和开放世界识别，但缺乏明确的推荐系统、搜索或广告应用场景，与当前关注的LLM技术、Transformer架构进展或异构数据统一建模等方向关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20608v1": {
    "title": "VAGNet: Grounding 3D Affordance from Human-Object Interactions in Videos",
    "url": "https://www.alphaxiv.org/abs/2602.20608v1",
    "arxiv_id": "2602.20608v1",
    "authors": "Aihua Mao, Kaihang Huang, Yong-Jin Liu, Chee Seng Chan, Ying He",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 07:00:38",
    "ori_summary": "3D object affordance grounding aims to identify regions on 3D objects that support human-object interaction (HOI), a capability essential to embodied visual reasoning. However, most existing approaches rely on static visual or textual cues, neglecting that affordances are inherently defined by dynamic actions. As a result, they often struggle to localize the true contact regions involved in real interactions. We take a different perspective. Humans learn how to use objects by observing and imitating actions, not just by examining shapes. Motivated by this intuition, we introduce video-guided 3D affordance grounding, which leverages dynamic interaction sequences to provide functional supervision. To achieve this, we propose VAGNet, a framework that aligns video-derived interaction cues with 3D structure to resolve ambiguities that static cues cannot address. To support this new setting, we introduce PVAD, the first HOI video-3D pairing affordance dataset, providing functional supervision unavailable in prior works. Extensive experiments on PVAD show that VAGNet achieves state-of-the-art performance, significantly outperforming static-based baselines. The code and dataset will be open publicly.",
    "summary": "",
    "translation": "VAGNet：基于视频中人物-物体交互的三维功能属性定位",
    "relevance_score": 2,
    "reasoning": "该论文主要研究计算机视觉中的3D场景理解和物体功能属性识别，属于纯粹的视觉/3D视觉领域。虽然涉及人类交互，但缺乏与推荐系统、搜索或广告领域的直接联系或潜在应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20597v1": {
    "title": "Interaction-aware Representation Modeling with Co-occurrence Consistency for Egocentric Hand-Object Parsing",
    "url": "https://www.alphaxiv.org/abs/2602.20597v1",
    "arxiv_id": "2602.20597v1",
    "authors": "Yuejiao Su, Yi Wang, Lei Yao, Yawen Cui, Lap-Pui Chau",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 06:39:18",
    "ori_summary": "A fine-grained understanding of egocentric human-environment interactions is crucial for developing next-generation embodied agents. One fundamental challenge in this area involves accurately parsing hands and active objects. While transformer-based architectures have demonstrated considerable potential for such tasks, several key limitations remain unaddressed: 1) existing query initialization mechanisms rely primarily on semantic cues or learnable parameters, demonstrating limited adaptability to changing active objects across varying input scenes; 2) previous transformer-based methods utilize pixel-level semantic features to iteratively refine queries during mask generation, which may introduce interaction-irrelevant content into the final embeddings; and 3) prevailing models are susceptible to \"interaction illusion\", producing physically inconsistent predictions. To address these issues, we propose an end-to-end Interaction-aware Transformer (InterFormer), which integrates three key components, i.e., a Dynamic Query Generator (DQG), a Dual-context Feature Selector (DFS), and the Conditional Co-occurrence (CoCo) loss. The DQG explicitly grounds query initialization in the spatial dynamics of hand-object contact, enabling targeted generation of interaction-aware queries for hands and various active objects. The DFS fuses coarse interactive cues with semantic features, thereby suppressing interaction-irrelevant noise and emphasizing the learning of interactive relationships. The CoCo loss incorporates hand-object relationship constraints to enhance physical consistency in prediction. Our model achieves state-of-the-art performance on both the EgoHOS and the challenging out-of-distribution mini-HOI4D datasets, demonstrating its effectiveness and strong generalization ability. Code and models are publicly available at https://github.com/yuggiehk/InterFormer.",
    "summary": "",
    "translation": "基于共现一致性的交互感知表示建模用于第一人称手-物体解析",
    "relevance_score": 2,
    "reasoning": "该论文主要关注第一人称视角下的手-物体交互解析，这属于计算机视觉中的特定任务。虽然涉及交互建模，但其应用场景（第一人称手-物体解析）与推荐系统、搜索或广告的核心领域没有直接关联，也不属于能够应用于这些领域的LLM或Transformer基础技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20584v1": {
    "title": "Long-Term Multi-Session 3D Reconstruction Under Substantial Appearance Change",
    "url": "https://www.alphaxiv.org/abs/2602.20584v1",
    "arxiv_id": "2602.20584v1",
    "authors": "Beverley Gorry, Tobias Fischer, Michael Milford, Alejandro Fontan",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2026-02-24 06:12:51",
    "ori_summary": "Long-term environmental monitoring requires the ability to reconstruct and align 3D models across repeated site visits separated by months or years. However, existing Structure-from-Motion (SfM) pipelines implicitly assume near-simultaneous image capture and limited appearance change, and therefore fail when applied to long-term monitoring scenarios such as coral reef surveys, where substantial visual and structural change is common. In this paper, we show that the primary limitation of current approaches lies in their reliance on post-hoc alignment of independently reconstructed sessions, which is insufficient under large temporal appearance change. We address this limitation by enforcing cross-session correspondences directly within a joint SfM reconstruction. Our approach combines complementary handcrafted and learned visual features to robustly establish correspondences across large temporal gaps, enabling the reconstruction of a single coherent 3D model from imagery captured years apart, where standard independent and joint SfM pipelines break down. We evaluate our method on long-term coral reef datasets exhibiting significant real-world change, and demonstrate consistent joint reconstruction across sessions in cases where existing methods fail to produce coherent reconstructions. To ensure scalability to large datasets, we further restrict expensive learned feature matching to a small set of likely cross-session image pairs identified via visual place recognition, which reduces computational cost and improves alignment robustness.",
    "summary": "",
    "translation": "在显著外观变化下的长期多会话三维重建",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向计算机视觉中的3D重建技术，专注于处理外观变化和多会话场景。根据用户列出的不相关主题，明确排除了'Purely Vision、3D Vision, Graphic or Speech papers without clear relevance to RecSys/Search/Ads'。标题中没有任何元素表明与推荐系统、搜索或广告的直接或潜在应用联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20583v1": {
    "title": "PropFly: Learning to Propagate via On-the-Fly Supervision from Pre-trained Video Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2602.20583v1",
    "arxiv_id": "2602.20583v1",
    "authors": "Wonyong Seo, Jaeho Moon, Jaehyup Lee, Soo Ye Kim, Munchurl Kim",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 06:11:08",
    "ori_summary": "Propagation-based video editing enables precise user control by propagating a single edited frame into following frames while maintaining the original context such as motion and structures. However, training such models requires large-scale, paired (source and edited) video datasets, which are costly and complex to acquire. Hence, we propose the PropFly, a training pipeline for Propagation-based video editing, relying on on-the-Fly supervision from pre-trained video diffusion models (VDMs) instead of requiring off-the-shelf or precomputed paired video editing datasets. Specifically, our PropFly leverages one-step clean latent estimations from intermediate noised latents with varying Classifier-Free Guidance (CFG) scales to synthesize diverse pairs of 'source' (low-CFG) and 'edited' (high-CFG) latents on-the-fly. The source latent serves as structural information of the video, while the edited latent provides the target transformation for learning propagation. Our pipeline enables an additional adapter attached to the pre-trained VDM to learn to propagate edits via Guidance-Modulated Flow Matching (GMFM) loss, which guides the model to replicate the target transformation. Our on-the-fly supervision ensures the model to learn temporally consistent and dynamic transformations. Extensive experiments demonstrate that our PropFly significantly outperforms the state-of-the-art methods on various video editing tasks, producing high-quality editing results.",
    "summary": "",
    "translation": "PropFly：通过预训练视频扩散模型的即时监督学习传播",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及视频生成和扩散模型，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然扩散模型是生成模型的一种，但论文专注于视频生成这一特定应用，没有明确展示其在异构数据处理、序列建模或推荐相关任务中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20577v1": {
    "title": "Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion",
    "url": "https://www.alphaxiv.org/abs/2602.20577v1",
    "arxiv_id": "2602.20577v1",
    "authors": "Jiaru Zhang, Manav Gagvani, Can Cui, Juntong Peng, Ruqi Zhang, Ziran Wang",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 05:59:10",
    "ori_summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged as promising candidates for end-to-end autonomous driving. However, these models typically face challenges in inference latency, action precision, and explainability. Existing autoregressive approaches struggle with slow token-by-token generation, while prior diffusion-based planners often rely on verbose, general-purpose language tokens that lack explicit geometric structure. In this work, we propose Masked Vision-Language-Action Diffusion for Autonomous Driving (MVLAD-AD), a novel framework designed to bridge the gap between efficient planning and semantic explainability via a masked vision-language-action diffusion model. Unlike methods that force actions into the language space, we introduce a discrete action tokenization strategy that constructs a compact codebook of kinematically feasible waypoints from real-world driving distributions. Moreover, we propose geometry-aware embedding learning to ensure that embeddings in the latent space approximate physical geometric metrics. Finally, an action-priority decoding strategy is introduced to prioritize trajectory generation. Extensive experiments on nuScenes and derived benchmarks demonstrate that MVLAD-AD achieves superior efficiency and outperforms state-of-the-art autoregressive and diffusion baselines in planning precision, while providing high-fidelity and explainable reasoning.",
    "summary": "",
    "translation": "基于掩码视觉-语言-动作扩散的高效可解释端到端自动驾驶",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域，属于纯粹的视觉/机器人应用，与推荐系统、搜索或广告的核心技术栈无直接关联。虽然标题包含'Vision-Language'，但其应用场景（自动驾驶）和核心方法（扩散模型用于动作规划）均不符合当前关注的任何技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20575v1": {
    "title": "An interactive enhanced driving dataset for autonomous driving",
    "url": "https://www.alphaxiv.org/abs/2602.20575v1",
    "arxiv_id": "2602.20575v1",
    "authors": "Haojie Feng, Peizhi Zhang, Mengjie Tian, Xinrui Zhang, Zhuoren Li, Junpeng Huang, Xiurong Wang, Junfan Zhu, Jianzhou Wang, Dongxiao Yin, Lu Xiong",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 05:57:18",
    "ori_summary": "The evolution of autonomous driving towards full automation demands robust interactive capabilities; however, the development of Vision-Language-Action (VLA) models is constrained by the sparsity of interactive scenarios and inadequate multimodal alignment in existing data. To this end, this paper proposes the Interactive Enhanced Driving Dataset (IEDD). We develop a scalable pipeline to mine million-level interactive segments from naturalistic driving data based on interactive trajectories, and design metrics to quantify the interaction processes. Furthermore, the IEDD-VQA dataset is constructed by generating synthetic Bird's Eye View (BEV) videos where semantic actions are strictly aligned with structured language. Benchmark results evaluating ten mainstream Vision Language Models (VLMs) are provided to demonstrate the dataset's reuse value in assessing and fine-tuning the reasoning capabilities of autonomous driving models.",
    "summary": "",
    "translation": "一个用于自动驾驶的交互式增强驾驶数据集",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向自动驾驶领域，属于您指定的无关主题中的“其他领域特定应用”。虽然数据集创建可能涉及数据处理技术，但标题没有显示与推荐系统、搜索、广告或相关使能技术（如Transformer架构、LLM应用）的任何直接联系。自动驾驶数据集通常专注于车辆感知和控制，与您的核心关注领域不匹配。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20569v1": {
    "title": "AIForge-Doc: A Benchmark for Detecting AI-Forged Tampering in Financial and Form Documents",
    "url": "https://www.alphaxiv.org/abs/2602.20569v1",
    "arxiv_id": "2602.20569v1",
    "authors": "Jiaqi Wu, Yuchen Zhou, Muduo Xu, Zisheng Liang, Simiao Ren, Jiayu Xue, Meige Yang, Siying Chen, Jingheng Huan",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 05:37:35",
    "ori_summary": "We present AIForge-Doc, the first dedicated benchmark targeting exclusively diffusion-model-based inpainting in financial and form documents with pixel-level annotation. Existing document forgery datasets rely on traditional digital editing tools (e.g., Adobe Photoshop, GIMP), creating a critical gap: state-of-the-art detectors are blind to the rapidly growing threat of AI-forged document fraud. AIForge-Doc addresses this gap by systematically forging numeric fields in real-world receipt and form images using two AI inpainting APIs -- Gemini 2.5 Flash Image and Ideogram v2 Edit -- yielding 4,061 forged images from four public document datasets (CORD, WildReceipt, SROIE, XFUND) across nine languages, annotated with pixel-precise tampered-region masks in DocTamper-compatible format. We benchmark three representative detectors -- TruFor, DocTamper, and a zero-shot GPT-4o judge -- and find that all existing methods degrade substantially: TruFor achieves AUC=0.751 (zero-shot, out-of-distribution) vs. AUC=0.96 on NIST16; DocTamper achieves AUC=0.563 vs. AUC=0.98 in-distribution, with pixel-level IoU=0.020; GPT-4o achieves only 0.509 -- essentially at chance -- confirming that AI-forged values are indistinguishable to automated detectors and VLMs. These results demonstrate that AIForge-Doc represents a qualitatively new and unsolved challenge for document forensics.",
    "summary": "",
    "translation": "AIForge-Doc：一个用于检测金融和表单文档中AI伪造篡改的基准",
    "relevance_score": 1,
    "reasoning": "该论文专注于文档伪造检测，属于安全/隐私领域，与您关注的推荐系统、搜索、广告核心进展以及相关使能技术无关。论文标题明确涉及金融文档的AI伪造检测，这完全属于您列出的不相关主题（安全、隐私、指纹等），没有任何部分与您的当前研究重点相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20566v1": {
    "title": "BFA++: Hierarchical Best-Feature-Aware Token Prune for Multi-View Vision Language Action Model",
    "url": "https://www.alphaxiv.org/abs/2602.20566v1",
    "arxiv_id": "2602.20566v1",
    "authors": "Haosheng Li, Weixin Mao, Zihan Lan, Hongwei Xiong, Hongan Wang, Chenyang Si, Ziwei Liu, Xiaoming Deng, Hua Chen",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2026-02-24 05:31:52",
    "ori_summary": "Vision-Language-Action (VLA) models have achieved significant breakthroughs by leveraging Large Vision Language Models (VLMs) to jointly interpret instructions and visual inputs. However, the substantial increase in visual tokens, particularly from multi-view inputs, poses serious challenges to real-time robotic manipulation. Existing acceleration techniques for VLMs, such as token pruning, often result in degraded performance when directly applied to VLA models, as they overlook the relationships between different views and fail to account for the dynamic and task-specific characteristics of robotic operation. To address this, we propose BFA++, a dynamic token pruning framework designed specifically for VLA models. BFA++ introduces a hierarchical pruning strategy guided by two-level importance predictors: an intra-view predictor highlights task-relevant regions within each image to suppress spatial noise, while an inter-view predictor identifies critical camera views throughout different manipulation phases to reduce cross-view redundancy. This design enables efficient token selection while preserving essential visual cues, resulting in improved computational efficiency and higher manipulation success rates. Evaluations on the RoboTwin benchmark and real-world robotic tasks demonstrate that BFA++ consistently outperforms existing methods. BFA++ improves the success rate by about 10% on both the π0 and RDT models, achieving speedup of 1.8X and 1.5X, respectively. Our results highlight that context-sensitive and task-aware token pruning serves as a more effective strategy than full visual processing, enabling faster inference and improved manipulation accuracy in real-world robotic systems.",
    "summary": "",
    "translation": "BFA++：面向多视图视觉语言动作模型的分层最佳特征感知令牌剪枝",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及多视图视觉语言动作模型和令牌剪枝技术，主要属于视觉-语言多模态领域，与纯粹的推荐系统、搜索或广告的直接应用关联较弱。虽然令牌剪枝技术可能提升模型效率，但论文未明确提及在推荐/搜索/广告场景的应用潜力，且多视图视觉语言动作模型更偏向视觉和动作理解，而非文本或序列数据处理。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20556v1": {
    "title": "WildGHand: Learning Anti-Perturbation Gaussian Hand Avatars from Monocular In-the-Wild Videos",
    "url": "https://www.alphaxiv.org/abs/2602.20556v1",
    "arxiv_id": "2602.20556v1",
    "authors": "Hanhui Li, Xuan Huang, Wanquan Liu, Yuhao Cheng, Long Chen, Yiqiang Yan, Xiaodan Liang, Chenqiang Gao",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 05:14:05",
    "ori_summary": "Despite recent progress in 3D hand reconstruction from monocular videos, most existing methods rely on data captured in well-controlled environments and therefore degrade in real-world settings with severe perturbations, such as hand-object interactions, extreme poses, illumination changes, and motion blur. To tackle these issues, we introduce WildGHand, an optimization-based framework that enables self-adaptive 3D Gaussian splatting on in-the-wild videos and produces high-fidelity hand avatars. WildGHand incorporates two key components: (i) a dynamic perturbation disentanglement module that explicitly represents perturbations as time-varying biases on 3D Gaussian attributes during optimization, and (ii) a perturbation-aware optimization strategy that generates per-frame anisotropic weighted masks to guide optimization. Together, these components allow the framework to identify and suppress perturbations across both spatial and temporal dimensions. We further curate a dataset of monocular hand videos captured under diverse perturbations to benchmark in-the-wild hand avatar reconstruction. Extensive experiments on this dataset and two public datasets demonstrate that WildGHand achieves state-of-the-art performance and substantially improves over its base model across multiple metrics (e.g., up to a $15.8\\%$ relative gain in PSNR and a $23.1\\%$ relative reduction in LPIPS). Our implementation and dataset are available at https://github.com/XuanHuang0/WildGHand.",
    "summary": "",
    "translation": "WildGHand：从单目野外视频中学习抗扰动高斯手部化身",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的手部建模和三维重建，属于纯粹的视觉领域研究。标题表明其研究内容涉及手部姿态估计、三维重建和抗扰动技术，这些与推荐系统、搜索或广告的核心技术栈没有直接关联。论文没有展示任何在推荐、搜索或广告领域的潜在应用，也不涉及Transformer架构、LLM技术或异构数据处理等当前关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20551v1": {
    "title": "CAD-Prompted SAM3: Geometry-Conditioned Instance Segmentation for Industrial Objects",
    "url": "https://www.alphaxiv.org/abs/2602.20551v1",
    "arxiv_id": "2602.20551v1",
    "authors": "Zhenran Tang, Rohan Nagabhirava, Changliu Liu",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 05:10:22",
    "ori_summary": "Verbal-prompted segmentation is inherently limited by the expressiveness of natural language and struggles with uncommon, instance-specific, or difficult-to-describe objects: scenarios frequently encountered in manufacturing and 3D printing environments. While image exemplars provide an alternative, they primarily encode appearance cues such as color and texture, which are often unrelated to a part's geometric identity. In industrial settings, a single component may be produced in different materials, finishes, or colors, making appearance-based prompting unreliable. In contrast, such objects are typically defined by precise CAD models that capture their canonical geometry. We propose a CAD-prompted segmentation framework built on SAM3 that uses canonical multi-view renderings of a CAD model as prompt input. The rendered views provide geometry-based conditioning independent of surface appearance. The model is trained using synthetic data generated from mesh renderings in simulation under diverse viewpoints and scene contexts. Our approach enables single-stage, CAD-prompted mask prediction, extending promptable segmentation to objects that cannot be robustly described by language or appearance alone.",
    "summary": "",
    "translation": "CAD提示的SAM3：面向工业对象的几何条件实例分割",
    "relevance_score": 1,
    "reasoning": "该论文专注于工业对象的计算机视觉实例分割，属于纯粹的视觉领域研究。虽然提到了CAD（计算机辅助设计）和几何条件，但这与推荐系统、搜索或广告的核心技术（如用户行为建模、内容理解、排序算法等）没有直接关联。论文没有涉及任何可能应用于RecSys/Search/Ads的LLM技术、Transformer架构进展或异构数据处理方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20550v1": {
    "title": "The Finite Primitive Basis Theorem for Computational Imaging: Formal Foundations of the OperatorGraph Representation",
    "url": "https://www.alphaxiv.org/abs/2602.20550v1",
    "arxiv_id": "2602.20550v1",
    "authors": "Chengshuai Yang",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 05:07:06",
    "ori_summary": "Computational imaging forward models, from coded aperture spectral cameras to MRI scanners, are traditionally implemented as monolithic, modality-specific codes. We prove that every forward model in a broad, precisely defined operator class Cimg (encompassing clinical, scientific, and industrial imaging modalities, both linear and nonlinear) admits an epsilon-approximate representation as a typed directed acyclic graph (DAG) whose nodes are drawn from a library of exactly 11 canonical primitives: Propagate, Modulate, Project, Encode, Convolve, Accumulate, Detect, Sample, Disperse, Scatter, and Transform. We call this the Finite Primitive Basis Theorem. The proof is constructive: we provide an algorithm that, given any H in Cimg, produces a DAG G with relative operator error at most epsilon and graph complexity within prescribed bounds. We further prove that the library is minimal: removing any single primitive causes at least one modality to lose its epsilon-approximate representation. A systematic analysis of nonlinearities in imaging physics shows they fall into two structural categories: pointwise scalar functions (handled by Transform) and self-consistent iterations (unrolled into existing linear primitives). Empirical validation on 31 linear modalities confirms eimg below 0.01 with at most 5 nodes and depth 5, and we provide constructive DAG decompositions for 9 additional nonlinear modalities. These results establish mathematical foundations for the Physics World Model (PWM) framework.",
    "summary": "",
    "translation": "计算成像的有限原始基定理：OperatorGraph表示的形式化基础",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于计算成像领域的数学基础理论，涉及算子图表示的形式化基础。这属于计算机视觉中的特定数学理论方向，与推荐系统、搜索或广告的核心技术、LLM应用、Transformer架构或异构数据建模均无直接关联。即使考虑潜在应用，计算成像主要面向医学成像、遥感等领域，而非RecSys/Search/Ads场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20549v1": {
    "title": "Sample-efficient evidence estimation of score based priors for model selection",
    "url": "https://www.alphaxiv.org/abs/2602.20549v1",
    "arxiv_id": "2602.20549v1",
    "authors": "Frederic Wang, Katherine L. Bouman",
    "categories": "cs.LG, cs.CV, stat.ME",
    "pub_date": "2026-02-24 05:06:46",
    "ori_summary": "The choice of prior is central to solving ill-posed imaging inverse problems, making it essential to select one consistent with the measurements $y$ to avoid severe bias. In Bayesian inverse problems, this could be achieved by evaluating the model evidence $p(y \\mid M)$ under different models $M$ that specify the prior and then selecting the one with the highest value. Diffusion models are the state-of-the-art approach to solving inverse problems with a data-driven prior; however, directly computing the model evidence with respect to a diffusion prior is intractable. Furthermore, most existing model evidence estimators require either many pointwise evaluations of the unnormalized prior density or an accurate clean prior score. We propose \\method, an estimator of the model evidence of a diffusion prior by integrating over the time-marginals of posterior sampling methods. Our method leverages the large amount of intermediate samples naturally obtained during the reverse diffusion sampling process to obtain an accurate estimation of the model evidence using only a handful of posterior samples (e.g., 20). We also demonstrate how to implement our estimator in tandem with recent diffusion posterior sampling methods. Empirically, our estimator matches the model evidence when it can be computed analytically, and it is able to both select the correct diffusion model prior and diagnose prior misfit under different highly ill-conditioned, non-linear inverse problems, including a real-world black hole imaging problem.",
    "summary": "",
    "translation": "基于评分先验的模型选择中样本高效证据估计",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及贝叶斯统计中的模型选择与证据估计，属于理论统计方法研究。虽然提到了'样本高效'这一计算效率概念，但未明确涉及Transformer架构、LLM技术或推荐/搜索/广告系统的具体应用场景，与当前关注的四大技术方向均无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20548v1": {
    "title": "Robust Spiking Neural Networks Against Adversarial Attacks",
    "url": "https://www.alphaxiv.org/abs/2602.20548v1",
    "arxiv_id": "2602.20548v1",
    "authors": "Shuai Wang, Malu Zhang, Yulin Jiang, Dehao Zhang, Ammar Belatreche, Yu Liang, Yimeng Shan, Zijian Zhou, Yang Yang, Haizhou Li",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 05:06:12",
    "ori_summary": "Spiking Neural Networks (SNNs) represent a promising paradigm for energy-efficient neuromorphic computing due to their bio-plausible and spike-driven characteristics. However, the robustness of SNNs in complex adversarial environments remains significantly constrained. In this study, we theoretically demonstrate that those threshold-neighboring spiking neurons are the key factors limiting the robustness of directly trained SNNs. We find that these neurons set the upper limits for the maximum potential strength of adversarial attacks and are prone to state-flipping under minor disturbances. To address this challenge, we propose a Threshold Guarding Optimization (TGO) method, which comprises two key aspects. First, we incorporate additional constraints into the loss function to move neurons' membrane potentials away from their thresholds. It increases SNNs' gradient sparsity, thereby reducing the theoretical upper bound of adversarial attacks. Second, we introduce noisy spiking neurons to transition the neuronal firing mechanism from deterministic to probabilistic, decreasing their state-flipping probability due to minor disturbances. Extensive experiments conducted in standard adversarial scenarios prove that our method significantly enhances the robustness of directly trained SNNs. These findings pave the way for advancing more reliable and secure neuromorphic computing in real-world applications.",
    "summary": "",
    "translation": "针对对抗性攻击的鲁棒性脉冲神经网络",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于脉冲神经网络对抗性攻击的鲁棒性，这属于神经网络安全领域，与您关注的推荐系统、搜索、广告的核心进展、LLM技术、Transformer架构或异构数据统一建模等方向无直接关联。对抗性攻击防御属于安全范畴，已被明确列为不相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20543v1": {
    "title": "Beyond Human Performance: A Vision-Language Multi-Agent Approach for Quality Control in Pharmaceutical Manufacturing",
    "url": "https://www.alphaxiv.org/abs/2602.20543v1",
    "arxiv_id": "2602.20543v1",
    "authors": "Subhra Jyoti Mandal, Lara Rachidi, Puneet Jain, Matthieu Duvinage, Sander W. Timmer",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 04:48:05",
    "ori_summary": "Colony-forming unit (CFU) detection is critical in pharmaceutical manufacturing, serving as a key component of Environmental Monitoring programs and ensuring compliance with stringent quality standards. Manual counting is labor-intensive and error-prone, while deep learning (DL) approaches, though accurate, remain vulnerable to sample quality variations and artifacts. Building on our earlier CNN-based framework (Beznik et al., 2020), we evaluated YOLOv5, YOLOv7, and YOLOv8 for CFU detection; however, these achieved only 97.08 percent accuracy, insufficient for pharmaceutical-grade requirements. A custom Detectron2 model trained on GSK's dataset of over 50,000 Petri dish images achieved 99 percent detection rate with 2 percent false positives and 0.6 percent false negatives. Despite high validation accuracy, Detectron2 performance degrades on outlier cases including contaminated plates, plastic artifacts, or poor optical clarity. To address this, we developed a multi-agent framework combining DL with vision-language models (VLMs). The VLM agent first classifies plates as valid or invalid. For valid samples, both DL and VLM agents independently estimate colony counts. When predictions align within 5 percent, results are automatically recorded in Postgres and SAP; otherwise, samples are routed for expert review. Expert feedback enables continuous retraining and self-improvement. Initial DL-based automation reduced human verification by 50 percent across vaccine manufacturing sites. With VLM integration, this increased to 85 percent, delivering significant operational savings. The proposed system provides a scalable, auditable, and regulation-ready solution for microbiological quality control, advancing automation in biopharmaceutical production.",
    "summary": "",
    "translation": "超越人类性能：基于视觉-语言多智能体方法的药品生产质量控制",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于药品制造领域的质量控制应用，属于医疗/生物领域特定应用，完全在无关主题列表中。虽然提到了视觉-语言模型，但上下文是药品制造而非推荐/搜索/广告中的异构数据处理，没有展示任何与推荐系统、搜索或广告相关的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20539v1": {
    "title": "Progressive Per-Branch Depth Optimization for DEFOM-Stereo and SAM3 Joint Analysis in UAV Forestry Applications",
    "url": "https://www.alphaxiv.org/abs/2602.20539v1",
    "arxiv_id": "2602.20539v1",
    "authors": "Yida Lin, Bing Xue, Mengjie Zhang, Sam Schofield, Richard Green",
    "categories": "eess.IV, cs.CV",
    "pub_date": "2026-02-24 04:37:18",
    "ori_summary": "Accurate per-branch 3D reconstruction is a prerequisite for autonomous UAV-based tree pruning; however, dense disparity maps from modern stereo matchers often remain too noisy for individual branch analysis in complex forest canopies. This paper introduces a progressive pipeline integrating DEFOM-Stereo foundation-model disparity estimation, SAM3 instance segmentation, and multi-stage depth optimization to deliver robust per-branch point clouds. Starting from a naive baseline, we systematically identify and resolve three error families through successive refinements. Mask boundary contamination is first addressed through morphological erosion and subsequently refined via a skeleton-preserving variant to safeguard thin-branch topology. Segmentation inaccuracy is then mitigated using LAB-space Mahalanobis color validation coupled with cross-branch overlap arbitration. Finally, depth noise - the most persistent error source - is initially reduced by outlier removal and median filtering, before being superseded by a robust five-stage scheme comprising MAD global detection, spatial density consensus, local MAD filtering, RGB-guided filtering, and adaptive bilateral filtering. Evaluated on 1920x1080 stereo imagery of Radiata pine (Pinus radiata) acquired with a ZED Mini camera (63 mm baseline) from a UAV in Canterbury, New Zealand, the proposed pipeline reduces the average per-branch depth standard deviation by 82% while retaining edge fidelity. The result is geometrically coherent 3D point clouds suitable for autonomous pruning tool positioning. All code and processed data are publicly released to facilitate further UAV forestry research.",
    "summary": "",
    "translation": "面向无人机林业应用的DEFOM-Stereo与SAM3联合分析中的渐进式逐分支深度优化",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向无人机林业应用，属于特定领域应用（林业），与RecSys/搜索/广告的核心技术无关。标题中的DEFOM-Stereo和SAM3联合分析暗示了计算机视觉或3D视觉技术，且没有表明这些技术会应用于推荐、搜索或广告系统。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20537v1": {
    "title": "PFGNet: A Fully Convolutional Frequency-Guided Peripheral Gating Network for Efficient Spatiotemporal Predictive Learning",
    "url": "https://www.alphaxiv.org/abs/2602.20537v1",
    "arxiv_id": "2602.20537v1",
    "authors": "Xinyong Cai, Changbin Sun, Yong Wang, Hongyu Yang, Yuankai Wu",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 04:31:12",
    "ori_summary": "Spatiotemporal predictive learning (STPL) aims to forecast future frames from past observations and is essential across a wide range of applications. Compared with recurrent or hybrid architectures, pure convolutional models offer superior efficiency and full parallelism, yet their fixed receptive fields limit their ability to adaptively capture spatially varying motion patterns. Inspired by biological center-surround organization and frequency-selective signal processing, we propose PFGNet, a fully convolutional framework that dynamically modulates receptive fields through pixel-wise frequency-guided gating. The core Peripheral Frequency Gating (PFG) block extracts localized spectral cues and adaptively fuses multi-scale large-kernel peripheral responses with learnable center suppression, effectively forming spatially adaptive band-pass filters. To maintain efficiency, all large kernels are decomposed into separable 1D convolutions ($1 \\times k$ followed by $k \\times 1$), reducing per-channel computational cost from $O(k^2)$ to $O(2k)$. PFGNet enables structure-aware spatiotemporal modeling without recurrence or attention. Experiments on Moving MNIST, TaxiBJ, Human3.6M, and KTH show that PFGNet delivers SOTA or near-SOTA forecasting performance with substantially fewer parameters and FLOPs. Our code is available at https://github.com/fhjdqaq/PFGNet.",
    "summary": "",
    "translation": "PFGNet：一种用于高效时空预测学习的全卷积频率引导外围门控网络",
    "relevance_score": 2,
    "reasoning": "该论文主要关注时空预测学习的卷积网络架构改进，属于计算机视觉和时序预测领域。虽然时空预测在推荐系统中可能有潜在应用（如用户行为序列预测），但论文标题未明确提及推荐、搜索或广告应用，也未涉及Transformer架构、LLM技术或VLM类比等核心关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20531v1": {
    "title": "A Lightweight Vision-Language Fusion Framework for Predicting App Ratings from User Interfaces and Metadata",
    "url": "https://www.alphaxiv.org/abs/2602.20531v1",
    "arxiv_id": "2602.20531v1",
    "authors": "Azrin Sultana, Firoz Ahmed",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 04:17:50",
    "ori_summary": "App ratings are among the most significant indicators of the quality, usability, and overall user satisfaction of mobile applications. However, existing app rating prediction models are largely limited to textual data or user interface (UI) features, overlooking the importance of jointly leveraging UI and semantic information. To address these limitations, this study proposes a lightweight vision--language framework that integrates both mobile UI and semantic information for app rating prediction. The framework combines MobileNetV3 to extract visual features from UI layouts and DistilBERT to extract textual features. These multimodal features are fused through a gated fusion module with Swish activations, followed by a multilayer perceptron (MLP) regression head. The proposed model is evaluated using mean absolute error (MAE), root mean square error (RMSE), mean squared error (MSE), coefficient of determination (R2), and Pearson correlation. After training for 20 epochs, the model achieves an MAE of 0.1060, an RMSE of 0.1433, an MSE of 0.0205, an R2 of 0.8529, and a Pearson correlation of 0.9251. Extensive ablation studies further demonstrate the effectiveness of different combinations of visual and textual encoders. Overall, the proposed lightweight framework provides valuable insights for developers and end users, supports sustainable app development, and enables efficient deployment on edge devices.",
    "summary": "",
    "translation": "一种用于从用户界面和元数据预测应用评分的轻量级视觉-语言融合框架",
    "relevance_score": 3,
    "reasoning": "该论文涉及视觉-语言融合，可能类比异构数据处理，但直接应用于应用评分预测，与推荐/搜索/广告的核心排序任务关联较弱。视觉-语言融合技术有潜力用于处理推荐系统中的多模态数据（如商品图片与文本描述），但论文的具体应用场景（应用评分预测）偏离了核心领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20520v1": {
    "title": "How Do Inpainting Artifacts Propagate to Language?",
    "url": "https://www.alphaxiv.org/abs/2602.20520v1",
    "arxiv_id": "2602.20520v1",
    "authors": "Pratham Yashwante, Davit Abrahamyan, Shresth Grover, Sukruth Rao",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-24 03:46:33",
    "ori_summary": "We study how visual artifacts introduced by diffusion-based inpainting affect language generation in vision-language models. We use a two-stage diagnostic setup in which masked image regions are reconstructed and then provided to captioning models, enabling controlled comparisons between captions generated from original and reconstructed inputs. Across multiple datasets, we analyze the relationship between reconstruction fidelity and downstream caption quality. We observe consistent associations between pixel-level and perceptual reconstruction metrics and both lexical and semantic captioning performance. Additional analysis of intermediate visual representations and attention patterns shows that inpainting artifacts lead to systematic, layer-dependent changes in model behavior. Together, these results provide a practical diagnostic framework for examining how visual reconstruction quality influences language generation in multimodal systems.",
    "summary": "",
    "translation": "修复伪影如何传播至语言？",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于图像修复伪影对语言的影响，属于视觉-语言交叉领域，但未明确涉及推荐系统、搜索或广告的应用。尽管提及“语言”，但核心是视觉伪影分析，属于纯粹的视觉或AIGC内容生成范畴，与当前关注的异构数据统一建模、Transformer架构进展或LLM在推荐/搜索/广告的直接应用无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20511v1": {
    "title": "Leveraging Causal Reasoning Method for Explaining Medical Image Segmentation Models",
    "url": "https://www.alphaxiv.org/abs/2602.20511v1",
    "arxiv_id": "2602.20511v1",
    "authors": "Limai Jiang, Ruitao Xie, Bokai Yang, Huazhen Huang, Juan He, Yufu Huo, Zikai Wang, Yang Wei, Yunpeng Cai",
    "categories": "cs.CV",
    "pub_date": "2026-02-24 03:26:27",
    "ori_summary": "Medical image segmentation plays a vital role in clinical decision-making, enabling precise localization of lesions and guiding interventions. Despite significant advances in segmentation accuracy, the black-box nature of most deep models has raised growing concerns about their trustworthiness in high-stakes medical scenarios. Current explanation techniques have primarily focused on classification tasks, leaving the segmentation domain relatively underexplored. We introduced an explanation model for segmentation task which employs the causal inference framework and backpropagates the average treatment effect (ATE) into a quantification metric to determine the influence of input regions, as well as network components, on target segmentation areas. Through comparison with recent segmentation explainability techniques on two representative medical imaging datasets, we demonstrated that our approach provides more faithful explanations than existing approaches. Furthermore, we carried out a systematic causal analysis of multiple foundational segmentation models using our method, which reveals significant heterogeneity in perceptual strategies across different models, and even between different inputs for the same model. Suggesting the potential of our method to provide notable insights for optimizing segmentation models. Our code can be found at https://github.com/lcmmai/PdCR.",
    "summary": "",
    "translation": "利用因果推理方法解释医学图像分割模型",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医学图像分割模型的解释性，属于医学领域的特定应用。根据用户列出的无关主题，医学、生物学、化学、物理学等特定领域应用应被排除。该论文与推荐系统、搜索、广告或相关使能技术（如Transformer架构、LLM应用、异构数据建模）均无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  }
}