{
  "2602.20135v1": {
    "title": "KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration",
    "url": "https://www.alphaxiv.org/abs/2602.20135v1",
    "arxiv_id": "2602.20135v1",
    "authors": "Mohammad Amanlou, Erfan Shafiee Moghaddam, Yasaman Amou Jafari, Mahdi Noori, Farhan Farsi, Behnam Bahrak",
    "categories": "cs.CL, cs.AI, cs.IR",
    "pub_date": "2026-02-23 18:46:27",
    "ori_summary": "With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.",
    "summary": "",
    "translation": "KNIGHT：基于知识图谱驱动的自适应难度校准多项选择题生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注知识图谱驱动的多项选择题生成，属于特定领域的NLP应用，与推荐系统、搜索或广告的核心技术进展、LLM基础技术、Transformer架构改进或直接应用无关。虽然知识图谱在推荐系统中有所应用，但该论文聚焦于问题生成这一具体任务，缺乏对RecSys/Search/Ads领域的明确应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20122v1": {
    "title": "NanoKnow: How to Know What Your Language Model Knows",
    "url": "https://www.alphaxiv.org/abs/2602.20122v1",
    "arxiv_id": "2602.20122v1",
    "authors": "Lingwei Gu, Nour Jedidi, Jimmy Lin",
    "categories": "cs.CL, cs.AI, cs.IR, cs.LG",
    "pub_date": "2026-02-23 18:37:49",
    "ori_summary": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat's pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow's utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.",
    "summary": "",
    "translation": "NanoKnow：如何了解你的语言模型知道什么",
    "relevance_score": 2,
    "reasoning": "该论文标题聚焦于评估语言模型的知识边界，属于LLM评估范畴，与您关注的推荐/搜索/广告系统核心进展、LLM技术应用或Transformer架构改进无直接关联。虽然了解模型知识边界可能间接影响应用可靠性，但论文本身并未明确指向推荐/搜索/广告领域的实际应用或技术改进。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20093v1": {
    "title": "ManCAR: Manifold-Constrained Latent Reasoning with Adaptive Test-Time Computation for Sequential Recommendation",
    "url": "https://www.alphaxiv.org/abs/2602.20093v1",
    "arxiv_id": "2602.20093v1",
    "authors": "Kun Yang, Yuxuan Zhu, Yazhe Chen, Siyao Zheng, Bangyang Hong, Kangle Wu, Yabo Ni, Anxiang Zeng, Cong Fu, Hui Li",
    "categories": "cs.IR",
    "pub_date": "2026-02-23 18:02:50",
    "ori_summary": "Sequential recommendation increasingly employs latent multi-step reasoning to enhance test-time computation. Despite empirical gains, existing approaches largely drive intermediate reasoning states via target-dominant objectives without imposing explicit feasibility constraints. This results in latent drift, where reasoning trajectories deviate into implausible regions. We argue that effective recommendation reasoning should instead be viewed as navigation on a collaborative manifold rather than free-form latent refinement. To this end, we propose ManCAR (Manifold-Constrained Adaptive Reasoning), a principled framework that grounds reasoning within the topology of a global interaction graph. ManCAR constructs a local intent prior from the collaborative neighborhood of a user's recent actions, represented as a distribution over the item simplex. During training, the model progressively aligns its latent predictive distribution with this prior, forcing the reasoning trajectory to remain within the valid manifold. At test time, reasoning proceeds adaptively until the predictive distribution stabilizes, avoiding over-refinement. We provide a variational interpretation of ManCAR to theoretically validate its drift-prevention and adaptive test-time stopping mechanisms. Experiments on seven benchmarks demonstrate that ManCAR consistently outperforms state-of-the-art baselines, achieving up to a 46.88% relative improvement w.r.t. NDCG@10. Our code is available at https://github.com/FuCongResearchSquad/ManCAR.",
    "summary": "论文研究序列推荐中潜在多步推理的轨迹漂移问题，核心思想是将推理视为在全局交互图拓扑定义的协作流形上的导航，通过构建局部意图先验并强制预测分布对齐来约束推理轨迹的可行性。",
    "translation": "ManCAR：基于流形约束的潜在推理与自适应测试时计算的序列推荐",
    "relevance_score": 9,
    "reasoning": "该论文直接针对序列推荐这一核心推荐系统领域，属于'核心领域进展'范畴。其提出的流形约束潜在推理方法可能涉及Transformer架构的改进或高效推理技术，与'赋能Transformer技术'相关。自适应测试时计算技术对大规模推荐系统的部署效率有直接影响，具有明确的实用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对序列推荐的核心问题，提出在协作流形上进行约束推理的创新方法，并引入自适应测试时计算机制，与关注领域高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.20001v1": {
    "title": "FairFS: Addressing Deep Feature Selection Biases for Recommender System",
    "url": "https://www.alphaxiv.org/abs/2602.20001v1",
    "arxiv_id": "2602.20001v1",
    "authors": "Xianquan Wang, Zhaocheng Du, Jieming Zhu, Qinglin Jia, Zhenhua Dong, Kai Zhang",
    "categories": "cs.IR, cs.LG",
    "pub_date": "2026-02-23 16:08:32",
    "ori_summary": "Large-scale online marketplaces and recommender systems serve as critical technological support for e-commerce development. In industrial recommender systems, features play vital roles as they carry information for downstream models. Accurate feature importance estimation is critical because it helps identify the most useful feature subsets from thousands of feature candidates for online services. Such selection enables improved online performance while reducing computational cost. To address feature selection problems in deep learning, trainable gate-based and sensitivity-based methods have been proposed and proven effective in industrial practice. However, through the analysis of real-world cases, we identified three bias issues that cause feature importance estimation to rely on partial model layers, samples, or gradients, ultimately leading to inaccurate importance estimation. We refer to these as layer bias, baseline bias, and approximation bias. To mitigate these issues, we propose FairFS, a fair and accurate feature selection algorithm. FairFS regularizes feature importance estimated across all nonlinear transformation layers to address layer bias. It also introduces a smooth baseline feature close to the classifier decision boundary and adopts an aggregated approximation method to alleviate baseline and approximation biases. Extensive experiments demonstrate that FairFS effectively mitigates these biases and achieves state-of-the-art feature selection performance.",
    "summary": "",
    "translation": "FairFS：解决推荐系统中深度特征选择偏差",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及公平性（Fairness），这属于明确排除的无关主题。虽然提到了推荐系统和特征选择，但核心焦点是公平性而非技术架构、LLM应用或Transformer进展。根据指令要求，公平性、隐私、伦理等非技术主题应完全排除。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19990v1": {
    "title": "A Context-Aware Knowledge Graph Platform for Stream Processing in Industrial IoT",
    "url": "https://www.alphaxiv.org/abs/2602.19990v1",
    "arxiv_id": "2602.19990v1",
    "authors": "Monica Marconi Sciarroni, Emanuele Storti",
    "categories": "cs.DB, cs.DC, cs.IR",
    "pub_date": "2026-02-23 15:55:32",
    "ori_summary": "Industrial IoT ecosystems bring together sensors, machines and smart devices operating collaboratively across industrial environments. These systems generate large volumes of heterogeneous, high-velocity data streams that require interoperable, secure and contextually aware management. Most of the current stream management architectures, however, still rely on syntactic integration mechanisms, which result in limited flexibility, maintainability and interpretability in complex Industry 5.0 scenarios. This work proposes a context-aware semantic platform for data stream management that unifies heterogeneous IoT/IoE data sources through a Knowledge Graph enabling formal representation of devices, streams, agents, transformation pipelines, roles and rights. The model supports flexible data gathering, composable stream processing pipelines, and dynamic role-based data access based on agents' contexts, relying on Apache Kafka and Apache Flink for real-time processing, while SPARQL and SWRL-based reasoning provide context-dependent stream discovery. Experimental evaluations demonstrate the effectiveness of combining semantic models, context-aware reasoning and distributed stream processing to enable interoperable data workflows for Industry 5.0 environments.",
    "summary": "",
    "translation": "面向工业物联网流处理的情境感知知识图谱平台",
    "relevance_score": 2,
    "reasoning": "该论文主要关注工业物联网(IIoT)领域的流处理和知识图谱应用，属于特定领域应用而非核心推荐系统、搜索或广告技术。虽然知识图谱技术有时用于推荐系统的知识增强，但该论文明确限定在工业物联网场景，缺乏对RecSys/Search/Ads领域的直接或潜在应用说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19987v1": {
    "title": "Counterfactual Understanding via Retrieval-aware Multimodal Modeling for Time-to-Event Survival Prediction",
    "url": "https://www.alphaxiv.org/abs/2602.19987v1",
    "arxiv_id": "2602.19987v1",
    "authors": "Ha-Anh Hoang Nguyen, Tri-Duc Phan Le, Duc-Hoang Pham, Huy-Son Nguyen, Cam-Van Thi Nguyen, Duc-Trong Le, Hoang-Quynh Le",
    "categories": "cs.LG, cs.IR",
    "pub_date": "2026-02-23 15:53:25",
    "ori_summary": "This paper tackles the problem of time-to-event counterfactual survival prediction, aiming to optimize individualized survival outcomes in the presence of heterogeneity and censored data. We propose CURE, a framework that advances counterfactual survival modeling via comprehensive multimodal embedding and latent subgroup retrieval. CURE integrates clinical, paraclinical, demographic, and multi-omics information, which are aligned and fused through cross-attention mechanisms. Complex multi-omics signals can be adaptively refined using a mixture-of-experts architecture, emphasizing the most informative omics components. Building upon this representation, CURE implicitly retrieves patient-specific latent subgroups that capture both baseline survival dynamics and treatment-dependent variations. Experimental results on METABRIC and TCGA-LUAD datasets demonstrate that proposed CURE model consistently outperforms strong baselines in survival analysis, evaluated using the Time-dependent Concordance Index ($C^{td}$) and Integrated Brier Score (IBS). These findings highlight the potential of CURE to enhance multimodal understanding and serve as a foundation for future treatment recommendation models. All code and related resources are publicly available to facilitate the reproducibility https://github.com/L2R-UET/CURE.",
    "summary": "",
    "translation": "基于检索感知多模态建模的反事实理解用于事件时间生存预测",
    "relevance_score": 2,
    "reasoning": "该论文涉及多模态建模和反事实理解，可能属于医疗或生物统计领域的事件时间生存预测，与推荐系统、搜索或广告的核心技术无直接关联。虽然提及多模态建模，但未明确指向异构数据处理或推荐/搜索/广告应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19961v1": {
    "title": "Unlocking Multimodal Document Intelligence: From Current Triumphs to Future Frontiers of Visual Document Retrieval",
    "url": "https://www.alphaxiv.org/abs/2602.19961v1",
    "arxiv_id": "2602.19961v1",
    "authors": "Yibo Yan, Jiahao Huo, Guanbo Feng, Mingdong Ou, Yi Cao, Xin Zou, Shuliang Liu, Yuanhuiyi Lyu, Yu Huang, Jungang Li, Kening Zheng, Xu Zheng, Philip S. Yu, James Kwok, Xuming Hu",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2026-02-23 15:27:41",
    "ori_summary": "With the rapid proliferation of multimodal information, Visual Document Retrieval (VDR) has emerged as a critical frontier in bridging the gap between unstructured visually rich data and precise information acquisition. Unlike traditional natural image retrieval, visual documents exhibit unique characteristics defined by dense textual content, intricate layouts, and fine-grained semantic dependencies. This paper presents the first comprehensive survey of the VDR landscape, specifically through the lens of the Multimodal Large Language Model (MLLM) era. We begin by examining the benchmark landscape, and subsequently dive into the methodological evolution, categorizing approaches into three primary aspects: multimodal embedding models, multimodal reranker models, and the integration of Retrieval-Augmented Generation (RAG) and Agentic systems for complex document intelligence. Finally, we identify persistent challenges and outline promising future directions, aiming to provide a clear roadmap for future multimodal document intelligence.",
    "summary": "",
    "translation": "解锁多模态文档智能：从当前成就到视觉文档检索的未来前沿",
    "relevance_score": 3,
    "reasoning": "该论文标题主要关注视觉文档检索，属于多模态信息检索领域，与搜索有一定相关性。然而，它更侧重于文档图像处理而非推荐系统或广告的核心排名问题，且未明确涉及LLM或Transformer架构的进展，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19778v1": {
    "title": "Enhancing Automatic Chord Recognition via Pseudo-Labeling and Knowledge Distillation",
    "url": "https://www.alphaxiv.org/abs/2602.19778v1",
    "arxiv_id": "2602.19778v1",
    "authors": "Nghia Phan, Rong Jin, Gang Liu, Xiao Dong",
    "categories": "cs.SD, cs.IR, cs.LG, cs.MM",
    "pub_date": "2026-02-23 12:32:53",
    "ori_summary": "Automatic Chord Recognition (ACR) is constrained by the scarcity of aligned chord labels, as well-aligned annotations are costly to acquire. At the same time, open-weight pre-trained models are currently more accessible than their proprietary training data. In this work, we present a two-stage training pipeline that leverages pre-trained models together with unlabeled audio. The proposed method decouples training into two stages. In the first stage, we use a pre-trained BTC model as a teacher to generate pseudo-labels for over 1,000 hours of diverse unlabeled audio and train a student model solely on these pseudo-labels. In the second stage, the student is continually trained on ground-truth labels as they become available, with selective knowledge distillation (KD) from the teacher applied as a regularizer to prevent catastrophic forgetting of the representations learned in the first stage. In our experiments, two models (BTC, 2E1D) were used as students. In stage 1, using only pseudo-labels, the BTC student achieves over 98% of the teacher's performance, while the 2E1D model achieves about 96% across seven standard mir_eval metrics. After a single training run for both students in stage 2, the resulting BTC student model surpasses the traditional supervised learning baseline by 2.5% and the original pre-trained teacher model by 1.55% on average across all metrics. And the resulting 2E1D student model improves from the traditional supervised learning baseline by 3.79% on average and achieves almost the same performance as the teacher. Both cases show the large gains on rare chord qualities.",
    "summary": "",
    "translation": "通过伪标签与知识蒸馏增强自动和弦识别",
    "relevance_score": 1,
    "reasoning": "该论文专注于音乐领域的自动和弦识别，属于音频信号处理任务，与推荐系统、搜索或广告的核心技术领域无直接关联。伪标签和知识蒸馏虽然是通用技术，但论文的应用场景（音乐分析）与当前关注的RecSys/Search/Ads领域没有明确的潜在应用联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19728v1": {
    "title": "GrIT: Group Informed Transformer for Sequential Recommendation",
    "url": "https://www.alphaxiv.org/abs/2602.19728v1",
    "arxiv_id": "2602.19728v1",
    "authors": "Adamya Shyam, Venkateswara Rao Kagita, Bharti Rana, Vikas Kumar",
    "categories": "cs.IR",
    "pub_date": "2026-02-23 11:26:11",
    "ori_summary": "Sequential recommender systems aim to predict a user's future interests by extracting temporal patterns from their behavioral history. Existing approaches typically employ transformer-based architectures to process long sequences of user interactions, capturing preference shifts by modeling temporal relationships between items. However, these methods often overlook the influence of group-level features that capture the collective behavior of similar users. We hypothesize that explicitly modeling temporally evolving group features alongside individual user histories can significantly enhance next-item recommendation. Our approach introduces latent group representations, where each user's affiliation to these groups is modeled through learnable, time-varying membership weights. The membership weights at each timestep are computed by modeling shifts in user preferences through their interaction history, where we incorporate both short-term and long-term user preferences. We extract a set of statistical features that capture the dynamics of user behavior and further refine them through a series of transformations to produce the final drift-aware membership weights. A group-based representation is derived by weighting latent group embeddings with the learned membership scores. This representation is integrated with the user's sequential representation within the transformer block to jointly capture personal and group-level temporal dynamics, producing richer embeddings that lead to more accurate, context-aware recommendations. We validate the effectiveness of our approach through extensive experiments on five benchmark datasets, where it consistently outperforms state-of-the-art sequential recommendation methods.",
    "summary": "论文研究如何提升序列推荐系统对用户兴趣动态建模的准确性。核心思想是提出一种分组感知Transformer，通过可学习的时变成员权重将用户与潜在群体关联，并将演化中的群体级表示与用户序列表示在Transformer中融合，以联合捕捉个人和群体层面的时序动态。",
    "translation": "GrIT：面向序列推荐的组信息Transformer",
    "relevance_score": 9,
    "reasoning": "该论文直接属于序列推荐的核心领域进展，标题明确表明这是针对序列推荐的Transformer架构改进。通过引入组信息机制，这属于Transformer架构效率或注意力机制方面的技术进步，在推荐系统中具有直接应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文通过引入时变群体特征增强Transformer序列推荐，直接对应'核心领域进展'和'直接LLM应用'焦点，方法创新且应用明确。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.19711v1": {
    "title": "A Three-stage Neuro-symbolic Recommendation Pipeline for Cultural Heritage Knowledge Graphs",
    "url": "https://www.alphaxiv.org/abs/2602.19711v1",
    "arxiv_id": "2602.19711v1",
    "authors": "Krzysztof Kutt, Elżbieta Sroka, Oleksandra Ishchuk, Luiz do Valle Miranda",
    "categories": "cs.IR, cs.DL, cs.HC",
    "pub_date": "2026-02-23 11:02:13",
    "ori_summary": "The growing volume of digital cultural heritage resources highlights the need for advanced recommendation methods capable of interpreting semantic relationships between heterogeneous data entities. This paper presents a complete methodology for implementing a hybrid recommendation pipeline integrating knowledge-graph embeddings, approximate nearest-neighbour search, and SPARQL-driven semantic filtering. The work is evaluated on the JUHMP (Jagiellonian University Heritage Metadata Portal) knowledge graph developed within the CHExRISH project, which at the time of experimentation contained ${\\approx}3.2$M RDF triples describing people, events, objects, and historical relations affiliated with the Jagiellonian University (Kraków, PL). We evaluate four embedding families (TransE, ComplEx, ConvE, CompGCN) and perform hyperparameter selection for ComplEx and HNSW. Then, we present and evaluate the final three-stage neuro-symbolic recommender. Despite sparse and heterogeneous metadata, the approach produces useful and explainable recommendations, which were also proven with expert evaluation.",
    "summary": "该论文研究如何为文化遗产知识图谱中的异构数据实体提供语义感知的推荐。核心方法是设计了一个三阶段神经符号推荐管道，将知识图谱嵌入、近似最近邻搜索和SPARQL驱动的语义过滤相结合，以生成可解释的推荐结果。",
    "translation": "面向文化遗产知识图谱的三阶段神经符号推荐流程",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及推荐系统（RecSys）的核心领域进展，提出了一种结合神经与符号方法的推荐流程，这是推荐系统架构的重要创新。虽然应用于文化遗产领域，但其三阶段神经符号方法（可能涉及知识图谱推理、序列建模等）对处理异构数据、用户序列和上下文特征具有通用价值，可类比VLM处理多模态数据的思路，适用于更广泛的推荐场景。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出了一种结合神经符号方法的推荐系统管道，直接针对推荐系统领域的核心问题，并整合了知识图谱嵌入和语义过滤技术，与推荐系统核心进展和异构数据处理高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.19702v1": {
    "title": "DReX: An Explainable Deep Learning-based Multimodal Recommendation Framework",
    "url": "https://www.alphaxiv.org/abs/2602.19702v1",
    "arxiv_id": "2602.19702v1",
    "authors": "Adamya Shyam, Venkateswara Rao Kagita, Bharti Rana, Vikas Kumar",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2026-02-23 10:52:20",
    "ori_summary": "Multimodal recommender systems leverage diverse data sources, such as user interactions, content features, and contextual information, to address challenges like cold-start and data sparsity. However, existing methods often suffer from one or more key limitations: processing different modalities in isolation, requiring complete multimodal data for each interaction during training, or independent learning of user and item representations. These factors contribute to increased complexity and potential misalignment between user and item embeddings. To address these challenges, we propose DReX, a unified multimodal recommendation framework that incrementally refines user and item representations by leveraging interaction-level features from multimodal feedback. Our model employs gated recurrent units to selectively integrate these fine-grained features into global representations. This incremental update mechanism provides three key advantages: (1) simultaneous modeling of both nuanced interaction details and broader preference patterns, (2) eliminates the need for separate user and item feature extraction processes, leading to enhanced alignment in their learned representation, and (3) inherent robustness to varying or missing modalities. We evaluate the performance of the proposed approach on three real-world datasets containing reviews and ratings as interaction modalities. By considering review text as a modality, our approach automatically generates interpretable keyword profiles for both users and items, which supplement the recommendation process with interpretable preference indicators. Experiment results demonstrate that our approach outperforms state-of-the-art methods across all evaluated datasets.",
    "summary": "论文研究多模态推荐系统中用户与物品表示对齐及模态缺失的挑战。核心方法是设计统一框架，使用门控循环单元增量整合交互级多模态特征，同步建模细粒度交互细节与全局偏好模式，实现鲁棒的多模态表示学习。",
    "translation": "DReX：一种可解释的基于深度学习的多模态推荐框架",
    "relevance_score": 9,
    "reasoning": "该论文直接属于'核心领域进展'和'直接LLM应用'范畴，聚焦于推荐系统中的深度学习与多模态技术。虽然未明确提及LLM，但多模态推荐框架与'异构数据的VLM类比'高度相关，且可解释性对搜索/推荐/广告的实际部署至关重要。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出统一多模态推荐框架DReX，通过门控循环单元增量整合细粒度交互特征，直接解决多模态推荐中的表示对齐和模态缺失问题，与关注领域高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.19698v1": {
    "title": "Iconographic Classification and Content-Based Recommendation for Digitized Artworks",
    "url": "https://www.alphaxiv.org/abs/2602.19698v1",
    "arxiv_id": "2602.19698v1",
    "authors": "Krzysztof Kutt, Maciej Baczyński",
    "categories": "cs.DL, cs.AI, cs.CV, cs.IR",
    "pub_date": "2026-02-23 10:44:27",
    "ori_summary": "We present a proof-of-concept system that automates iconographic classification and content-based recommendation of digitized artworks using the Iconclass vocabulary and selected artificial intelligence methods. The prototype implements a four-stage workflow for classification and recommendation, which integrates YOLOv8 object detection with algorithmic mappings to Iconclass codes, rule-based inference for abstract meanings, and three complementary recommenders (hierarchical proximity, IDF-weighted overlap, and Jaccard similarity). Although more engineering is still needed, the evaluation demonstrates the potential of this solution: Iconclass-aware computer vision and recommendation methods can accelerate cataloging and enhance navigation in large heritage repositories. The key insight is to let computer vision propose visible elements and to use symbolic structures (Iconclass hierarchy) to reach meaning.",
    "summary": "",
    "translation": "数字化艺术作品的图像学分类与基于内容的推荐",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及基于内容的推荐系统，这属于推荐系统（RecSys）的核心领域。然而，其具体应用场景（数字化艺术作品）非常狭窄且特定，缺乏通用性，与当前关注的搜索、推荐或广告领域的广泛技术进展关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19549v1": {
    "title": "Sculpting the Vector Space: Towards Efficient Multi-Vector Visual Document Retrieval via Prune-then-Merge Framework",
    "url": "https://www.alphaxiv.org/abs/2602.19549v1",
    "arxiv_id": "2602.19549v1",
    "authors": "Yibo Yan, Mingdong Ou, Yi Cao, Xin Zou, Jiahao Huo, Shuliang Liu, James Kwok, Xuming Hu",
    "categories": "cs.CL, cs.CV, cs.IR",
    "pub_date": "2026-02-23 06:45:19",
    "ori_summary": "Visual Document Retrieval (VDR), which aims to retrieve relevant pages within vast corpora of visually-rich documents, is of significance in current multimodal retrieval applications. The state-of-the-art multi-vector paradigm excels in performance but suffers from prohibitive overhead, a problem that current efficiency methods like pruning and merging address imperfectly, creating a difficult trade-off between compression rate and feature fidelity. To overcome this dilemma, we introduce Prune-then-Merge, a novel two-stage framework that synergizes these complementary approaches. Our method first employs an adaptive pruning stage to filter out low-information patches, creating a refined, high-signal set of embeddings. Subsequently, a hierarchical merging stage compresses this pre-filtered set, effectively summarizing semantic content without the noise-induced feature dilution seen in single-stage methods. Extensive experiments on 29 VDR datasets demonstrate that our framework consistently outperforms existing methods, significantly extending the near-lossless compression range and providing robust performance at high compression ratios.",
    "summary": "",
    "translation": "雕琢向量空间：通过剪枝-合并框架实现高效多向量视觉文档检索",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视觉文档检索中的多向量表示效率优化，属于计算机视觉与信息检索交叉领域。虽然涉及检索技术，但其核心是视觉文档处理而非推荐/搜索/广告系统，且未明确涉及LLM、Transformer架构或异构数据统一建模等当前关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19543v1": {
    "title": "Hyper-KGGen: A Skill-Driven Knowledge Extractor for High-Quality Knowledge Hypergraph Generation",
    "url": "https://www.alphaxiv.org/abs/2602.19543v1",
    "arxiv_id": "2602.19543v1",
    "authors": "Rizhuo Huang, Yifan Feng, Rundong Xue, Shihui Ying, Jun-Hai Yong, Chuan Shi, Shaoyi Du, Yue Gao",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2026-02-23 06:32:00",
    "ori_summary": "Knowledge hypergraphs surpass traditional binary knowledge graphs by encapsulating complex $n$-ary atomic facts, providing a more comprehensive paradigm for semantic representation. However, constructing high-quality hypergraphs remains challenging due to the \\textit{scenario gap}: generic extractors struggle to generalize across diverse domains with specific jargon, while existing methods often fail to balance structural skeletons with fine-grained details. To bridge this gap, we propose \\textbf{Hyper-KGGen}, a skill-driven framework that reformulates extraction as a dynamic skill-evolving process. First, Hyper-KGGen employs a \\textit{coarse-to-fine} mechanism to systematically decompose documents, ensuring full-dimensional coverage from binary links to complex hyperedges. Crucially, it incorporates an \\textit{adaptive skill acquisition} module that actively distills domain expertise into a Global Skill Library. This is achieved via a stability-based feedback loop, where extraction stability serves as a relative reward signal to induce high-quality skills from unstable traces and missed predictions. Additionally, we present \\textbf{HyperDocRED}, a rigorously annotated benchmark for document-level knowledge hypergraph extraction. Experiments demonstrate that Hyper-KGGen significantly outperforms strong baselines, validating that evolved skills provide substantially richer guidance than static few-shot examples in multi-scenario settings.",
    "summary": "",
    "translation": "Hyper-KGGen：一种面向技能驱动的高质量知识超图生成知识提取器",
    "relevance_score": 2,
    "reasoning": "该论文主要关注知识超图生成，属于知识图谱构建领域。虽然知识图谱在推荐系统中可作为辅助信息源，但论文标题未明确指向推荐、搜索或广告应用，也未涉及LLM、Transformer架构或异构数据统一建模等当前关注的核心技术。其潜在应用过于间接，与当前关注点匹配度较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20133v1": {
    "title": "AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization",
    "url": "https://www.alphaxiv.org/abs/2602.20133v1",
    "arxiv_id": "2602.20133v1",
    "authors": "Mert Cemri, Shubham Agrawal, Akshat Gupta, Shu Liu, Audrey Cheng, Qiuyang Mang, Ashwin Naren, Lutfi Eren Erdogan, Koushik Sen, Matei Zaharia, Alex Dimakis, Ion Stoica",
    "categories": "cs.NE, cs.AI, cs.CL",
    "pub_date": "2026-02-23 18:45:31",
    "ori_summary": "The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. We introduce AdaEvolve, a framework that reformulates LLM-driven evolution as a hierarchical adaptive optimization problem. AdaEvolve uses an \"accumulated improvement signal\" to unify decisions across three levels: Local Adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; Global Adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and Meta-Guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. We demonstrate that AdaEvolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems.",
    "summary": "该论文研究LLM驱动的进化搜索中静态调度导致的资源浪费问题，核心思想是将LLM进化重构为分层自适应优化问题，通过局部适应、全局适应和元引导三个层次动态调整探索强度与资源分配。",
    "translation": "AdaEvolve：基于自适应大语言模型的零阶优化方法",
    "relevance_score": 7,
    "reasoning": "该论文属于'赋能LLM技术'范畴，提出了一种结合LLM的自适应零阶优化方法。零阶优化在推荐/搜索系统中具有潜在应用价值，可用于黑盒系统优化、超参数调优或处理不可微目标函数，例如优化复杂用户参与度指标或A/B测试策略。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出了一种基于LLM的自适应零阶优化框架，通过动态资源分配和元引导机制提升搜索效率，直接适用于算法设计和系统优化等推荐/搜索相关领域。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.20130v1": {
    "title": "To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering",
    "url": "https://www.alphaxiv.org/abs/2602.20130v1",
    "arxiv_id": "2602.20130v1",
    "authors": "Zaifu Zhan, Min Zeng, Shuang Zhou, Yiran Song, Xiaoyi Chen, Yu Hou, Yifan Wu, Yang Ruan, Rui Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-02-23 18:42:50",
    "ori_summary": "Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy. Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and PubMedQA. Metrics included accuracy, total generated tokens, and inference time. Results: Selective CoT reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\\leq$4\\%). In some model-task pairs, it achieved both higher accuracy and greater efficiency than standard CoT. Compared with fixed-length CoT, Selective CoT reached similar or superior accuracy at substantially lower computational cost. Discussion: Selective CoT dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability. Conclusion: Selective CoT provides a simple, model-agnostic, and cost-effective approach for medical QA, aligning reasoning effort with question complexity to enhance real-world deployability of LLM-based clinical systems.",
    "summary": "",
    "translation": "推理与否：医疗问答中的选择性思维链方法",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医疗问答领域，这属于明确的无关主题（Medical domain-specific applications）。虽然提到了思维链（Chain-of-Thought）这一LLM技术，但核心应用场景是医疗问答而非推荐系统、搜索或广告。论文没有显示出对RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20092v1": {
    "title": "BabyLM Turns 4: Call for Papers for the 2026 BabyLM Workshop",
    "url": "https://www.alphaxiv.org/abs/2602.20092v1",
    "arxiv_id": "2602.20092v1",
    "authors": "Leshem Choshen, Ryan Cotterell, Mustafa Omer Gul, Jaap Jumelet, Tal Linzen, Aaron Mueller, Suchir Salhan, Raj Sanjay Shah, Alex Warstadt, Ethan Gotlieb Wilcox",
    "categories": "cs.CL",
    "pub_date": "2026-02-23 18:02:23",
    "ori_summary": "BabyLM aims to dissolve the boundaries between cognitive modeling and language modeling. We call for both workshop papers and for researchers to join the 4th BabyLM competition. As in previous years, we call for participants in the data-efficient pretraining challenge in the general track. This year, we also offer a new track: Multilingual. We also call for papers outside the competition in any relevant areas. These include training efficiency, cognitively plausible research, weak model evaluation, and more.",
    "summary": "",
    "translation": "BabyLM 四岁：2026年 BabyLM 研讨会征稿启事",
    "relevance_score": 1,
    "reasoning": "该标题仅涉及一个特定研讨会（BabyLM Workshop）的征稿通知，未提及任何具体的技术内容、模型架构、算法或应用。它不涉及推荐系统、搜索、广告领域的核心进展、LLM 技术趋势、Transformer 架构改进、LLM 的直接应用，或受 VLM 启发的异构数据建模思想。因此，与用户关注的焦点完全不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20091v1": {
    "title": "How Retrieved Context Shapes Internal Representations in RAG",
    "url": "https://www.alphaxiv.org/abs/2602.20091v1",
    "arxiv_id": "2602.20091v1",
    "authors": "Samuel Yeh, Sharon Li",
    "categories": "cs.CL",
    "pub_date": "2026-02-23 18:02:04",
    "ori_summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by conditioning generation on retrieved external documents, but the effect of retrieved context is often non-trivial. In realistic retrieval settings, the retrieved document set often contains a mixture of documents that vary in relevance and usefulness. While prior work has largely examined these phenomena through output behavior, little is known about how retrieved context shapes the internal representations that mediate information integration in RAG. In this work, we study RAG through the lens of latent representations. We systematically analyze how different types of retrieved documents affect the hidden states of LLMs, and how these internal representation shifts relate to downstream generation behavior. Across four question-answering datasets and three LLMs, we analyze internal representations under controlled single- and multi-document settings. Our results reveal how context relevancy and layer-wise processing influence internal representations, providing explanations on LLMs output behaviors and insights for RAG system design.",
    "summary": "该论文研究检索增强生成中检索上下文对大型语言模型内部表示的影响。核心方法是通过系统分析不同类型检索文档如何改变LLM隐藏状态，揭示上下文相关性和分层处理如何影响内部表示，为理解信息整合机制提供新视角。",
    "translation": "检索到的上下文如何塑造RAG中的内部表示",
    "relevance_score": 8,
    "reasoning": "该论文探讨检索增强生成（RAG）中检索上下文对模型内部表示的影响，这直接属于'直接LLM应用'范畴，因为RAG是LLM在搜索和推荐系统中的关键技术。理解上下文如何塑造内部表示有助于优化检索策略和模型架构，提升搜索相关性排序和个性化推荐质量。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文研究RAG系统中检索上下文如何影响LLM内部表示，直接关联检索系统设计，对理解LLM信息整合机制有重要价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.20065v1": {
    "title": "Multilingual Large Language Models do not comprehend all natural languages to equal degrees",
    "url": "https://www.alphaxiv.org/abs/2602.20065v1",
    "arxiv_id": "2602.20065v1",
    "authors": "Natalia Moskvina, Raquel Montero, Masaya Yoshida, Ferdy Hubers, Paolo Morosi, Walid Irhaymi, Jin Yan, Tamara Serrano, Elena Pagliarini, Fritz Günther, Evelina Leivada",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-02-23 17:22:46",
    "ori_summary": "Large Language Models (LLMs) play a critical role in how humans access information. While their core use relies on comprehending written requests, our understanding of this ability is currently limited, because most benchmarks evaluate LLMs in high-resource languages predominantly spoken by Western, Educated, Industrialised, Rich, and Democratic (WEIRD) communities. The default assumption is that English is the best-performing language for LLMs, while smaller, low-resource languages are linked to less reliable outputs, even in multilingual, state-of-the-art models. To track variation in the comprehension abilities of LLMs, we prompt 3 popular models on a language comprehension task across 12 languages, representing the Indo-European, Afro-Asiatic, Turkic, Sino-Tibetan, and Japonic language families. Our results suggest that the models exhibit remarkable linguistic accuracy across typologically diverse languages, yet they fall behind human baselines in all of them, albeit to different degrees. Contrary to what was expected, English is not the best-performing language, as it was systematically outperformed by several Romance languages, even lower-resource ones. We frame the results by discussing the role of several factors that drive LLM performance, such as tokenization, language distance from Spanish and English, size of training data, and data origin in high- vs. low-resource languages and WEIRD vs. non-WEIRD communities.",
    "summary": "",
    "translation": "多语言大语言模型对不同自然语言的理解程度并不相等",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多语言LLM在不同语言上的能力差异评估，属于纯粹的NLP评估基准研究。虽然涉及LLM技术，但论文焦点是语言能力评估而非模型架构改进或具体应用，且未明确讨论其在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20052v1": {
    "title": "Entropy in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.20052v1",
    "arxiv_id": "2602.20052v1",
    "authors": "Marco Scharringhausen",
    "categories": "cs.CL",
    "pub_date": "2026-02-23 17:02:45",
    "ori_summary": "In this study, the output of large language models (LLM) is considered an information source generating an unlimited sequence of symbols drawn from a finite alphabet. Given the probabilistic nature of modern LLMs, we assume a probabilistic model for these LLMs, following a constant random distribution and the source itself thus being stationary. We compare this source entropy (per word) to that of natural language (written or spoken) as represented by the Open American National Corpus (OANC). Our results indicate that the word entropy of such LLMs is lower than the word entropy of natural speech both in written or spoken form. The long-term goal of such studies is to formalize the intuitions of information and uncertainty in large language training to assess the impact of training an LLM from LLM generated training data. This refers to texts from the world wide web in particular.",
    "summary": "该论文研究大型语言模型输出序列的信息熵特性问题，其核心思想是将LLM视为概率信息源，通过比较其词级熵与自然语言语料库的熵，量化LLM生成文本的不确定性差异。",
    "translation": "大语言模型中的熵",
    "relevance_score": 8,
    "reasoning": "该标题直接涉及大语言模型（LLMs）的核心特性——熵，这属于“赋能LLM技术”范畴。熵的概念在LLMs中与模型不确定性、预测置信度和信息理论密切相关，这些特性对推荐系统、搜索和广告中的排序置信度校准、多样性控制以及探索-利用权衡等关键问题具有直接应用潜力。",
    "rerank_relevance_score": 4,
    "rerank_reasoning": "该论文研究LLM输出熵与自然语言熵的对比，属于核心LLM技术的基础分析，但未直接涉及推荐、搜索或广告的具体应用或架构创新。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.20042v1": {
    "title": "Position: General Alignment Has Hit a Ceiling; Edge Alignment Must Be Taken Seriously",
    "url": "https://www.alphaxiv.org/abs/2602.20042v1",
    "arxiv_id": "2602.20042v1",
    "authors": "Han Bao, Yue Huang, Xiaoda Wang, Zheyuan Zhang, Yujun Zhou, Carl Yang, Xiangliang Zhang, Yanfang Ye",
    "categories": "cs.CL",
    "pub_date": "2026-02-23 16:51:43",
    "ori_summary": "Large language models are being deployed in complex socio-technical systems, which exposes limits in current alignment practice. We take the position that the dominant paradigm of General Alignment, which compresses diverse human values into a single scalar reward, reaches a structural ceiling in settings with conflicting values, plural stakeholders, and irreducible uncertainty. These failures follow from the mathematics and incentives of scalarization and lead to \\textbf{structural} value flattening, \\textbf{normative} representation loss, and \\textbf{cognitive} uncertainty blindness. We introduce Edge Alignment as a distinct approach in which systems preserve multi dimensional value structure, support plural and democratic representation, and incorporate epistemic mechanisms for interaction and clarification. To make this approach practical, we propose seven interdependent pillars organized into three phases. We identify key challenges in data collection, training objectives, and evaluation, outlining complementary technical and governance directions. Taken together, these measures reframe alignment as a lifecycle problem of dynamic normative governance rather than as a single instance optimization task.",
    "summary": "",
    "translation": "观点：通用对齐已触及天花板；边缘对齐必须认真对待",
    "relevance_score": 1,
    "reasoning": "该标题讨论的是LLM对齐问题，属于纯粹的LLM中心话题，与推荐系统、搜索或广告的核心进展、应用或使能技术没有直接关联。标题中提到的'通用对齐'和'边缘对齐'属于LLM安全、伦理或评估范畴，这些都是明确列出的不相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20040v1": {
    "title": "AgenticSum: An Agentic Inference-Time Framework for Faithful Clinical Text Summarization",
    "url": "https://www.alphaxiv.org/abs/2602.20040v1",
    "arxiv_id": "2602.20040v1",
    "authors": "Fahmida Liza Piya, Rahmatollah Beheshti",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-02-23 16:49:37",
    "ori_summary": "Large language models (LLMs) offer substantial promise for automating clinical text summarization, yet maintaining factual consistency remains challenging due to the length, noise, and heterogeneity of clinical documentation. We present AgenticSum, an inference-time, agentic framework that separates context selection, generation, verification, and targeted correction to reduce hallucinated content. The framework decomposes summarization into coordinated stages that compress task-relevant context, generate an initial draft, identify weakly supported spans using internal attention grounding signals, and selectively revise flagged content under supervisory control. We evaluate AgenticSum on two public datasets, using reference-based metrics, LLM-as-a-judge assessment, and human evaluation. Across various measures, AgenticSum demonstrates consistent improvements compared to vanilla LLMs and other strong baselines. Our results indicate that structured, agentic design with targeted correction offers an effective inference time solution to improve clinical note summarization using LLMs.",
    "summary": "",
    "translation": "AgenticSum：一种用于忠实临床文本摘要的智能体化推理时框架",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于临床文本摘要，属于医疗领域的特定应用，这直接落入'Irrelevant Topics'中的'Medical, Biology, Chemistry, Physics or other domain-specific applications'。虽然提到了'Agentic'框架，但核心应用场景（临床文本）与推荐系统、搜索或广告无关，因此不具备相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20020v1": {
    "title": "gencat: Generative computerized adaptive testing",
    "url": "https://www.alphaxiv.org/abs/2602.20020v1",
    "arxiv_id": "2602.20020v1",
    "authors": "Wanyong Feng, Andrew Lan",
    "categories": "cs.CL",
    "pub_date": "2026-02-23 16:28:46",
    "ori_summary": "Existing computerized Adaptive Testing (CAT) frameworks are typically built on predicting the correctness of a student response to a question. Although effective, this approach fails to leverage textual information in questions and responses, especially for open-ended questions. In this work, we propose GENCAT (\\textbf{GEN}erative \\textbf{CAT}), a novel CAT framework that leverages Large Language Models for knowledge estimate and question selection. First, we develop a Generative Item Response Theory (GIRT) model that enables us to estimate student knowledge from their open-ended responses and predict responses to unseen questions. We train the model in a two-step process, first via Supervised Fine-Tuning and then via preference optimization for knowledge-response alignment. Second, we introduce three question selection algorithms that leverage the generative capabilities of the GIRT model, based on the uncertainty, linguistic diversity, and information of sampled student responses. Third, we conduct experiments on two real-world programming datasets and demonstrate that GENCAT outperforms existing CAT baselines, achieving an AUC improvement of up to 4.32\\% in the key early testing stages.",
    "summary": "",
    "translation": "gencat：生成式计算机自适应测试",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及教育技术领域的计算机自适应测试，属于特定领域应用（教育评估），与推荐系统、搜索或广告的核心技术进展、LLM应用、Transformer架构改进或异构数据统一建模均无直接关联。生成式测试方法可能涉及内容生成，但属于AIGC在教育领域的应用，不在当前关注范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20017v1": {
    "title": "QUIETT: Query-Independent Table Transformation for Robust Reasoning",
    "url": "https://www.alphaxiv.org/abs/2602.20017v1",
    "arxiv_id": "2602.20017v1",
    "authors": "Gaurav Najpande, Tampu Ravi Kumar, Manan Roy Choudhury, Neha Valeti, Yanjie Fu, Vivek Gupta",
    "categories": "cs.CL",
    "pub_date": "2026-02-23 16:23:49",
    "ori_summary": "Real-world tables often exhibit irregular schemas, heterogeneous value formats, and implicit relational structure, which degrade the reliability of downstream table reasoning and question answering. Most existing approaches address these issues in a query-dependent manner, entangling table cleanup with reasoning and thus limiting generalization. We introduce QuIeTT, a query-independent table transformation framework that preprocesses raw tables into a single SQL-ready canonical representation before any test-time queries are observed. QuIeTT performs lossless schema and value normalization, exposes implicit relations, and preserves full provenance via raw table snapshots. By decoupling table transformation from reasoning, QuIeTT enables cleaner, more reliable, and highly efficient querying without modifying downstream models. Experiments on four benchmarks, WikiTQ, HiTab, NQ-Table, and SequentialQA show consistent gains across models and reasoning paradigms, with particularly strong improvements on a challenge set of structurally diverse, unseen questions.",
    "summary": "",
    "translation": "QUIETT：用于鲁棒推理的查询无关表转换",
    "relevance_score": 3,
    "reasoning": "该论文关注表转换和推理，可能涉及数据处理技术，但标题未明确表明与推荐系统、搜索或广告的直接关联。查询无关的表转换可能适用于数据预处理，但缺乏明确的Transformer架构、LLM应用或异构数据建模的焦点，使其与当前关注点的相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19991v1": {
    "title": "Cross-lingual Matryoshka Representation Learning across Speech and Text",
    "url": "https://www.alphaxiv.org/abs/2602.19991v1",
    "arxiv_id": "2602.19991v1",
    "authors": "Yaya Sy, Dioula Doucouré, Christophe Cerisara, Irina Illina",
    "categories": "cs.CL",
    "pub_date": "2026-02-23 15:57:16",
    "ori_summary": "Speakers of under-represented languages face both a language barrier, as most online knowledge is in a few dominant languages, and a modality barrier, since information is largely text-based while many languages are primarily oral. We address this for French-Wolof by training the first bilingual speech-text Matryoshka embedding model, enabling efficient retrieval of French text from Wolof speech queries without relying on a costly ASR-translation pipelines. We introduce large-scale data curation pipelines and new benchmarks, compare modeling strategies, and show that modality fusion within a frozen text Matryoshka model performs best. Although trained only for retrieval, the model generalizes well to other tasks, such as speech intent detection, indicating the learning of general semantic representations. Finally, we analyze cost-accuracy trade-offs across Matryoshka dimensions and ranks, showing that information is concentrated only in a few components, suggesting potential for efficiency improvements.",
    "summary": "",
    "translation": "跨语言套娃表示学习：跨越语音与文本",
    "relevance_score": 1,
    "reasoning": "该论文专注于跨语言语音与文本的表示学习，属于多模态学习范畴，但未明确涉及推荐系统、搜索或广告领域的应用。虽然套娃表示学习（Matryoshka Representation Learning）在表示压缩方面有潜力，但论文标题未表明其与异构数据处理（如用户序列和上下文特征）或Transformer架构改进的直接关联，因此与当前关注点相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19969v1": {
    "title": "ReAttn: Improving Attention-based Re-ranking via Attention Re-weighting",
    "url": "https://www.alphaxiv.org/abs/2602.19969v1",
    "arxiv_id": "2602.19969v1",
    "authors": "Yuxing Tian, Fengran Mo, Weixu Zhang, Yiyan Qi, Jian-Yun Nie",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-02-23 15:30:52",
    "ori_summary": "The strong capabilities of recent Large Language Models (LLMs) have made them highly effective for zero-shot re-ranking task. Attention-based re-ranking methods, which derive relevance scores directly from attention weights, offer an efficient and interpretable alternative to generation-based re-ranking methods. However, they still face two major limitations. First, attention signals are highly concentrated a small subset of tokens within a few documents, making others indistinguishable. Second, attention often overemphasizes phrases lexically similar to the query, yielding biased rankings that irrelevant documents with mere lexical resemblance are regarded as relevant. In this paper, we propose \\textbf{ReAttn}, a post-hoc re-weighting strategy for attention-based re-ranking methods. It first compute the cross-document IDF weighting to down-weight attention on query-overlapping tokens that frequently appear across the candidate documents, reducing lexical bias and emphasizing distinctive terms. It then employs entropy-based regularization to mitigate over-concentrated attention, encouraging a more balanced distribution across informative tokens. Both adjustments operate directly on existing attention weights without additional training or supervision. Extensive experiments demonstrate the effectiveness of our method.",
    "summary": "论文研究注意力机制在基于LLM的零样本重排任务中的两个关键缺陷：注意力过度集中于少数文档的少量标记，以及过度强调与查询词汇相似的短语导致排序偏差。核心方法是提出ReAttn后处理重加权策略，通过跨文档IDF加权降低高频重叠词汇的注意力权重以减少词汇偏见，并利用熵正则化缓解注意力过度集中问题，从而改进现有注意力权重而不需要额外训练。",
    "translation": "ReAttn：通过注意力重加权改进基于注意力的重排序",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及注意力机制改进，属于'Enabling Transformer Tech'中的新注意力机制类别。注意力重加权技术可应用于搜索和推荐系统中的重排序阶段，通过优化注意力权重来提升排序质量，具有明确的RecSys/Search应用潜力。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM在搜索重排任务中的注意力机制缺陷提出改进方法，属于核心领域进展和直接LLM应用的交叉点，与搜索领域高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.19948v1": {
    "title": "Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming",
    "url": "https://www.alphaxiv.org/abs/2602.19948v1",
    "arxiv_id": "2602.19948v1",
    "authors": "Ian Steenstra, Paola Pedrelli, Weiyan Shi, Stacy Marsella, Timothy W. Bickmore",
    "categories": "cs.CL, cs.AI, cs.CY, cs.HC, cs.MA",
    "pub_date": "2026-02-23 15:17:18",
    "ori_summary": "Large Language Models (LLMs) are increasingly utilized for mental health support; however, current safety benchmarks often fail to detect the complex, longitudinal risks inherent in therapeutic dialogue. We introduce an evaluation framework that pairs AI psychotherapists with simulated patient agents equipped with dynamic cognitive-affective models and assesses therapy session simulations against a comprehensive quality of care and risk ontology. We apply this framework to a high-impact test case, Alcohol Use Disorder, evaluating six AI agents (including ChatGPT, Gemini, and Character.AI) against a clinically-validated cohort of 15 patient personas representing diverse clinical phenotypes. Our large-scale simulation (N=369 sessions) reveals critical safety gaps in the use of AI for mental health support. We identify specific iatrogenic risks, including the validation of patient delusions (\"AI Psychosis\") and failure to de-escalate suicide risk. Finally, we validate an interactive data visualization dashboard with diverse stakeholders, including AI engineers and red teamers, mental health professionals, and policy experts (N=9), demonstrating that this framework effectively enables stakeholders to audit the \"black box\" of AI psychotherapy. These findings underscore the critical safety risks of AI-provided mental health support and the necessity of simulation-based clinical red teaming before deployment.",
    "summary": "",
    "translation": "评估大型语言模型在心理健康支持中的风险：一种自动化临床人工智能红队测试框架",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于心理健康领域的临床AI风险评估和红队测试，这属于医学/生物学特定领域应用，与您的关注点无关。论文讨论的是LLM在医疗场景中的风险，而非在推荐系统、搜索或广告中的技术应用或架构进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19919v1": {
    "title": "Janus-Q: End-to-End Event-Driven Trading via Hierarchical-Gated Reward Modeling",
    "url": "https://www.alphaxiv.org/abs/2602.19919v1",
    "arxiv_id": "2602.19919v1",
    "authors": "Xiang Li, Zikai Wei, Yiyan Qi, Wanyun Zhou, Xiang Liu, Penglei Sun, Yongqi Zhang, Xiaowen Chu",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2026-02-23 14:58:51",
    "ori_summary": "Financial market movements are often driven by discrete financial events conveyed through news, whose impacts are heterogeneous, abrupt, and difficult to capture under purely numerical prediction objectives. These limitations have motivated growing interest in using textual information as the primary source of trading signals in learning-based systems. Two key challenges hinder existing approaches: (1) the absence of large-scale, event-centric datasets that jointly model news semantics and statistically grounded market reactions, and (2) the misalignment between language model reasoning and financially valid trading behavior under dynamic market conditions. To address these challenges, we propose Janus-Q, an end-to-end event-driven trading framework that elevates financial news events from auxiliary signals to primary decision units. Janus-Q unifies event-centric data construction and model optimization under a two-stage paradigm. Stage I focuses on event-centric data construction, building a large-scale financial news event dataset comprising 62,400 articles annotated with 10 fine-grained event types, associated stocks, sentiment labels, and event-driven cumulative abnormal return (CAR). Stage II performs decision-oriented fine-tuning, combining supervised learning with reinforcement learning guided by a Hierarchical Gated Reward Model (HGRM), which explicitly captures trade-offs among multiple trading objectives. Extensive experiments demonstrate that Janus-Q achieves more consistent, interpretable, and profitable trading decisions than market indices and LLM baselines, improving the Sharpe Ratio by up to 102.0% while increasing direction accuracy by over 17.5% compared to the strongest competing strategies.",
    "summary": "",
    "translation": "Janus-Q：通过分层门控奖励建模实现端到端事件驱动交易",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及金融交易领域的强化学习应用，与推荐系统、搜索或广告的核心技术焦点无关。虽然提到了分层门控奖励建模，但这是针对特定金融交易场景的，没有显示出在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19895v1": {
    "title": "DSDR: Dual-Scale Diversity Regularization for Exploration in LLM Reasoning",
    "url": "https://www.alphaxiv.org/abs/2602.19895v1",
    "arxiv_id": "2602.19895v1",
    "authors": "Zhongwei Wan, Yun Shen, Zhihao Dou, Donghao Zhou, Yu Zhang, Xin Wang, Hui Shen, Jing Xiong, Chaofan Tao, Zixuan Zhong, Peizhou Huang, Mi Zhang",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2026-02-23 14:37:01",
    "ori_summary": "Reinforcement learning with verifiers (RLVR) is a central paradigm for improving large language model (LLM) reasoning, yet existing methods often suffer from limited exploration. Policies tend to collapse onto a few reasoning patterns and prematurely stop deep exploration, while conventional entropy regularization introduces only local stochasticity and fails to induce meaningful path-level diversity, leading to weak and unstable learning signals in group-based policy optimization. We propose DSDR, a Dual-Scale Diversity Regularization reinforcement learning framework that decomposes diversity in LLM reasoning into global and coupling components. Globally, DSDR promotes diversity among correct reasoning trajectories to explore distinct solution modes. Locally, it applies a length-invariant, token-level entropy regularization restricted to correct trajectories, preventing entropy collapse within each mode while preserving correctness. The two scales are coupled through a global-to-local allocation mechanism that emphasizes local regularization for more distinctive correct trajectories. We provide theoretical support showing that DSDR preserves optimal correctness under bounded regularization, sustains informative learning signals in group-based optimization, and yields a principled global-to-local coupling rule. Experiments on multiple reasoning benchmarks demonstrate consistent improvements in accuracy and pass@k, highlighting the importance of dual-scale diversity for deep exploration in RLVR. Code is available at https://github.com/SUSTechBruce/DSDR.",
    "summary": "该论文研究强化学习验证器范式下LLM推理探索不足的问题，其核心思想是将推理路径多样性分解为全局模式多样性与局部令牌级随机性，并通过一种耦合机制进行协调，以促进对多种正确解决方案的深度探索。",
    "translation": "DSDR：用于大语言模型推理探索的双尺度多样性正则化",
    "relevance_score": 6,
    "reasoning": "该论文涉及LLM推理中的探索机制，属于核心LLM技术进展（Enabling LLM Tech）。双尺度多样性正则化技术可能应用于搜索和推荐系统中的查询理解、结果多样性优化，或广告中的创意多样性探索，通过增强LLM的推理探索能力来改善这些领域的决策质量。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出了一种用于增强LLM推理探索的双尺度多样性正则化强化学习框架，直接属于“直接LLM应用”和“核心LLM技术进展”范畴，对提升搜索/推荐系统中基于LLM的推理与决策组件有潜在价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.19883v1": {
    "title": "Denotational Semantics for ODRL: Knowledge-Based Constraint Conflict Detection",
    "url": "https://www.alphaxiv.org/abs/2602.19883v1",
    "arxiv_id": "2602.19883v1",
    "authors": "Daham Mustafa, Diego Collarana, Yixin Peng, Rafiqul Haque, Christoph Lange-Bever, Christoph Quix, Stephan Decker",
    "categories": "cs.CL, cs.LO",
    "pub_date": "2026-02-23 14:28:13",
    "ori_summary": "ODRL's six set-based operators -- isA, isPartOf, hasPart, isAnyOf, isAllOf, isNoneOf -- depend on external domain knowledge that the W3C specification leaves unspecified. Without it, every cross-dataspace policy comparison defaults to Unknown. We present a denotational semantics that maps each ODRL constraint to the set of knowledge-base concepts satisfying it. Conflict detection reduces to denotation intersection under a three-valued verdict -- Conflict, Compatible, or Unknown -- that is sound under incomplete knowledge. The framework covers all three ODRL composition modes (and, or, xone) and all three semantic domains arising in practice: taxonomic (class subsumption), mereological (part-whole containment), and nominal (identity). For cross-dataspace interoperability, we define order-preserving alignments between knowledge bases and prove two guarantees: conflicts are preserved across different KB standards, and unmapped concepts degrade gracefully to Unknown -- never to false conflicts. A runtime soundness theorem ensures that design-time verdicts hold for all execution contexts. The encoding stays within the decidable EPR fragment of first-order logic. We validate it with 154 benchmarks across six knowledge base families (GeoNames, ISO 3166, W3C DPV, a GDPR-derived taxonomy, BCP 47, and ISO 639-3) and four structural KBs targeting adversarial edge cases. Both the Vampire theorem prover and the Z3 SMT solver agree on all 154 verdicts. A key finding is that exclusive composition (xone) requires strictly stronger KB axioms than conjunction or disjunction: open-world semantics blocks exclusivity even when positive evidence appears to satisfy exactly one branch.",
    "summary": "",
    "translation": "ODRL的指称语义：基于知识的约束冲突检测",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及ODRL（开放数字权利语言）的形式语义和约束冲突检测，属于特定领域的形式化方法研究。这与搜索、推荐或广告系统的核心进展、LLM技术、Transformer架构或直接应用无关，也不涉及异构数据的统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19878v1": {
    "title": "Axis Decomposition for ODRL: Resolving Dimensional Ambiguity in Policy Constraints through Interval Semantics",
    "url": "https://www.alphaxiv.org/abs/2602.19878v1",
    "arxiv_id": "2602.19878v1",
    "authors": "Daham Mustafa, Diego Collarana, Yixin Peng, Rafiqul Haque, Christoph Lange-Bever, Christoph Quix, Stephan Decker",
    "categories": "cs.CL, cs.LO",
    "pub_date": "2026-02-23 14:24:46",
    "ori_summary": "Every ODRL 2.2 constraint compares a single scalar value: (leftOperand, operator, rightOperand). Five of ODRL's approximately 34 left operands, however, denote multi-dimensional quantities--image dimensions, canvas positions, geographic coordinates--whose specification text explicitly references multiple axes. For these operands, a single scalar constraint admits one interpretation per axis, making policy evaluation non-deterministic. We classify ODRL's left operands by value-domain structure (scalar, dimensional, concept-valued), grounded in the ODRL 2.2 specification text, and show that dimensional ambiguity is intrinsic to the constraint syntax. We present an axis-decomposition framework that refines each dimensional operand into axis-specific scalar operands and prove four properties: deterministic interpretation, AABB completeness, sound over-approximation under projection, and conservative extension. Conflict detection operates in two layers: per-axis verdicts are always decidable; box-level verdicts compose through Strong Kleene conjunction into a three-valued logic (Conflict, Compatible, Unknown). For ODRL's disjunctive (odrl:or) and exclusive-or (odrl:xone) logical constraints, where per-axis decomposition does not apply, the framework encodes coupled multi-axis conjectures directly. We instantiate the framework as the ODRL Spatial Axis Profile--15 axis-specific left operands for the five affected base terms--and evaluate it on 117 benchmark problems spanning nine categories across both TPTP FOF (Vampire) and SMT-LIB (Z3) encodings, achieving full concordance between provers. Benchmark scenarios are inspired by constraints arising in cultural heritage dataspaces such as Datenraum Kultur. All meta-theorems are mechanically verified in Isabelle/HOL.",
    "summary": "",
    "translation": "ODRL的轴分解：通过区间语义解决策略约束中的维度模糊性",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及ODRL（开放数字权利语言），这是一个与数字版权管理相关的技术标准，主要关注策略约束和语义解析。这属于安全、隐私或权利管理领域，与您关注的推荐系统、搜索、广告、LLM技术或Transformer架构等核心主题无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19855v1": {
    "title": "SHIELD: Semantic Heterogeneity Integrated Embedding for Latent Discovery in Clinical Trial Safety Signals",
    "url": "https://www.alphaxiv.org/abs/2602.19855v1",
    "arxiv_id": "2602.19855v1",
    "authors": "Francois Vandenhende, Anna Georgiou, Theodoros Psaras, Ellie Karekla",
    "categories": "cs.CL",
    "pub_date": "2026-02-23 13:55:36",
    "ori_summary": "We present SHIELD, a novel methodology for automated and integrated safety signal detection in clinical trials. SHIELD combines disproportionality analysis with semantic clustering of adverse event (AE) terms applied to MedDRA term embeddings. For each AE, the pipeline computes an information-theoretic disproportionality measure (Information Component) with effect size derived via empirical Bayesian shrinkage. A utility matrix is constructed by weighting semantic term-term similarities by signal magnitude, followed by spectral embedding and clustering to identify groups of related AEs. Resulting clusters are annotated with syndrome-level summary labels using large language models, yielding a coherent, data-driven representation of treatment-associated safety profiles in the form of a network graph and hierarchical tree. We implement the SHIELD framework in the context of a single-arm incidence summary, to compare two treatment arms or for the detection of any treatment effect in a multi-arm trial. We illustrate its ability to recover known safety signals and generate interpretable, cluster-based summaries in a real clinical trial example. This work bridges statistical signal detection with modern natural language processing to enhance safety assessment and causal interpretation in clinical trials.",
    "summary": "",
    "translation": "SHIELD：用于临床试验安全信号潜在发现的语义异质性集成嵌入",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学/生物领域的临床试验安全信号分析，属于明确的医学领域应用。虽然涉及异质性数据集成和潜在发现，但其应用场景（临床试验）完全属于被排除的医学领域，与推荐系统、搜索或广告的核心技术进展无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19840v1": {
    "title": "SAMAS: A Spectrum-Guided Multi-Agent System for Achieving Style Fidelity in Literary Translation",
    "url": "https://www.alphaxiv.org/abs/2602.19840v1",
    "arxiv_id": "2602.19840v1",
    "authors": "Jingzhuo Wu, Jiajun Zhang, Keyan Jin, Dehua Ma, Junbo Wang",
    "categories": "cs.CL",
    "pub_date": "2026-02-23 13:40:44",
    "ori_summary": "Modern large language models (LLMs) excel at generating fluent and faithful translations. However, they struggle to preserve an author's unique literary style, often producing semantically correct but generic outputs. This limitation stems from the inability of current single-model and static multi-agent systems to perceive and adapt to stylistic variations. To address this, we introduce the Style-Adaptive Multi-Agent System (SAMAS), a novel framework that treats style preservation as a signal processing task. Specifically, our method quantifies literary style into a Stylistic Feature Spectrum (SFS) using the wavelet packet transform. This SFS serves as a control signal to dynamically assemble a tailored workflow of specialized translation agents based on the source text's structural patterns. Extensive experiments on translation benchmarks show that SAMAS achieves competitive semantic accuracy against strong baselines, primarily by leveraging its statistically significant advantage in style fidelity.",
    "summary": "",
    "translation": "SAMAS：一种用于实现文学翻译风格保真的频谱引导多智能体系统",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于文学翻译中的风格保真问题，属于纯粹的机器翻译/NLP应用领域。虽然涉及多智能体系统，但没有任何迹象表明与推荐系统、搜索或广告相关，也不涉及LLM/Transformer架构进展、异构数据建模或这些领域的技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19815v1": {
    "title": "Keyboards for the Endangered Idu Mishmi Language",
    "url": "https://www.alphaxiv.org/abs/2602.19815v1",
    "arxiv_id": "2602.19815v1",
    "authors": "Akhilesh Kakolu Ramarao",
    "categories": "cs.CL",
    "pub_date": "2026-02-23 13:13:40",
    "ori_summary": "We present a mobile and desktop keyboard suite for Idu Mishmi, an endangered Trans-Himalayan language spoken by approximately 11,000 people in Arunachal Pradesh, India. Although a Latin-based orthography was developed in 2018, no digital input tools existed to use it, forcing speakers into ad-hoc romanizations that cannot represent the full writing system. Our keyboards comprise two tools: (1) an Android mobile keyboard, published on the Google Play Store and actively used in teacher training programs, and (2) a Windows desktop keyboard currently undergoing community testing. Both tools support the complete Idu Mishmi character inventory, including schwa, retracted schwa, nasalized vowels, and accented forms. Both operate fully offline with zero network permissions, addressing connectivity constraints and data sovereignty concerns. We describe the design, implementation, and deployment as a replicable model for other endangered language communities.",
    "summary": "",
    "translation": "濒危伊杜米什米语键盘设计",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于为特定濒危语言设计键盘，属于语言学或人机交互领域，与推荐系统、搜索、广告、LLM技术或Transformer架构等核心关注点无直接关联。它不涉及任何可能应用于推荐、搜索或广告的技术进步或方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19743v1": {
    "title": "NILE: Formalizing Natural-Language Descriptions of Formal Languages",
    "url": "https://www.alphaxiv.org/abs/2602.19743v1",
    "arxiv_id": "2602.19743v1",
    "authors": "Tristan Kneisel, Marko Schmellenkamp, Fabian Vehlken, Thomas Zeume",
    "categories": "cs.FL, cs.CL, cs.LO",
    "pub_date": "2026-02-23 11:42:56",
    "ori_summary": "This paper explores how natural-language descriptions of formal languages can be compared to their formal representations and how semantic differences can be explained. This is motivated from educational scenarios where learners describe a formal language (presented, e.g., by a finite state automaton, regular expression, pushdown automaton, context-free grammar or in set notation) in natural language, and an educational support system has to (1) judge whether the natural-language description accurately describes the formal language, and to (2) provide explanations why descriptions are not accurate. To address this question, we introduce a representation language for formal languages, Nile, which is designed so that Nile expressions can mirror the syntactic structure of natural-language descriptions of formal languages. Nile is sufficiently expressive to cover a broad variety of formal languages, including all regular languages and fragments of context-free languages typically used in educational contexts. Generating Nile expressions that are syntactically close to natural-language descriptions then allows to provide explanations for inaccuracies in the descriptions algorithmically. In experiments on an educational data set, we show that LLMs can translate natural-language descriptions into equivalent, syntactically close Nile expressions with high accuracy - allowing to algorithmically provide explanations for incorrect natural-language descriptions. Our experiments also show that while natural-language descriptions can also be translated into regular expressions (but not context-free grammars), the expressions are often not syntactically close and thus not suitable for providing explanations.",
    "summary": "",
    "translation": "NILE：形式化自然语言对形式语言的描述",
    "relevance_score": 2,
    "reasoning": "该论文涉及自然语言与形式语言之间的映射，属于语言学和形式化方法的交叉领域。虽然标题提到“自然语言描述”，但核心是形式语言理论，与推荐系统、搜索或广告中的实际应用（如查询理解、特征工程或序列建模）没有直接关联。它可能对构建领域特定语言（DSL）有理论价值，但缺乏明确的RecSys/Search/Ads应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19643v1": {
    "title": "KGHaluBench: A Knowledge Graph-Based Hallucination Benchmark for Evaluating the Breadth and Depth of LLM Knowledge",
    "url": "https://www.alphaxiv.org/abs/2602.19643v1",
    "arxiv_id": "2602.19643v1",
    "authors": "Alex Robertson, Huizhi Liang, Mahbub Gani, Rohit Kumar, Srijith Rajamohan",
    "categories": "cs.CL",
    "pub_date": "2026-02-23 09:41:46",
    "ori_summary": "Large Language Models (LLMs) possess a remarkable capacity to generate persuasive and intelligible language. However, coherence does not equate to truthfulness, as the responses often contain subtle hallucinations. Existing benchmarks are limited by static and narrow questions, leading to limited coverage and misleading evaluations. We present KGHaluBench, a Knowledge Graph-based hallucination benchmark that assesses LLMs across the breadth and depth of their knowledge, providing a fairer and more comprehensive insight into LLM truthfulness. Our framework utilises the KG to dynamically construct challenging, multifaceted questions, whose difficulty is then statistically estimated to address popularity bias. Our automated verification pipeline detects abstentions and verifies the LLM's response at both conceptual and correctness levels to identify different types of hallucinations. We evaluate 25 frontier models, using novel accuracy and hallucination metrics. The results provide a more interpretable insight into the knowledge factors that cause hallucinations across different model sizes. KGHaluBench is publicly available to support future developments in hallucination mitigation.",
    "summary": "",
    "translation": "KGHaluBench：一个基于知识图谱的幻觉基准，用于评估大语言模型知识的广度与深度",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于LLM幻觉评估基准，这属于纯粹的NLP评估范畴，与您关注的推荐系统、搜索或广告领域的核心进展、LLM技术应用或Transformer架构改进无关。虽然知识图谱在推荐系统中可能有应用，但论文的核心是评估基准而非技术应用，因此相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19626v1": {
    "title": "Nacrith: Neural Lossless Compression via Ensemble Context Modeling and High-Precision CDF Coding",
    "url": "https://www.alphaxiv.org/abs/2602.19626v1",
    "arxiv_id": "2602.19626v1",
    "authors": "Roberto Tacconelli",
    "categories": "cs.IT, cs.CL",
    "pub_date": "2026-02-23 09:14:05",
    "ori_summary": "We present Nacrith, a lossless compression system that combines a 135M-parameter transformer language model (SmolLM2-135M) with an ensemble of lightweight online predictors and a 32-bit arithmetic coder. Beyond the base LLM-plus-arithmetic-coding paradigm, Nacrith introduces several contributions: (1) a CDF precision upgrade from 2^16 to 2^24 that eliminates ~75% of quantization overhead caused by minimum-probability floors in large vocabularies; (2) a token-level N-gram model for fast local predictions; (3) an adaptive log-space bias head correcting per-document LLM errors via online gradient descent; (4) confidence-based LLM skip for accelerating highly predictable tokens; (5) a hybrid binary format (NC06) extending neural compression to arbitrary binary files--to our knowledge a first among LLM-based compressors; (6) a llama.cpp inference backend achieving ~7x faster single-token decode than PyTorch; (7) parallel multi-GPU compression across up to 8 workers; and (8) native KV cache sliding window reducing per-slide cost by ~37x. The system requires only ~500 MB of GGUF weights and ~1.2 GB VRAM per worker, running on consumer GPUs. On alice29.txt (Canterbury Corpus, 152 KB), Nacrith achieves 0.918 bits per byte (bpb)--outperforming gzip by 3.1x, bzip2 by 2.5x, CMIX v21 by 44%, and ts_zip by 20%, while compressing below the 0th-, 1st-, and 2nd-order byte-level Shannon entropy bounds. On enwik8 (100 MB), Nacrith achieves 0.9389 bpb (11.74%), surpassing ts_zip (~1.11 bpb) by 15% and FineZip (1.024 bpb) by 8% despite using a 60x smaller model with no fine-tuning. An out-of-distribution evaluation on a document published after the model's training cutoff confirms these gains are not memorization artifacts, achieving 0.723 bpb on unseen text.",
    "summary": "",
    "translation": "Nacrith：基于集成上下文建模与高精度累积分布函数编码的神经无损压缩方法",
    "relevance_score": 2,
    "reasoning": "该论文主要研究神经无损压缩技术，属于通用数据压缩领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然压缩技术可能间接应用于数据传输或存储优化，但论文标题未表明其在推荐/搜索/广告领域的特定应用潜力，也不涉及LLM、Transformer架构或异构数据处理等当前关注的核心技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19612v1": {
    "title": "Anatomy of Unlearning: The Dual Impact of Fact Salience and Model Fine-Tuning",
    "url": "https://www.alphaxiv.org/abs/2602.19612v1",
    "arxiv_id": "2602.19612v1",
    "authors": "Borisiuk Anna, Andrey Savchenko, Alexander Panchecko, Elena Tutubalina",
    "categories": "cs.CL",
    "pub_date": "2026-02-23 08:58:48",
    "ori_summary": "Machine Unlearning (MU) enables Large Language Models (LLMs) to remove unsafe or outdated information. However, existing work assumes that all facts are equally forgettable and largely ignores whether the forgotten knowledge originates from pretraining or supervised fine-tuning (SFT). In this paper, we introduce DUAL (Dual Unlearning Evaluation across Training Stages), a benchmark of 28.6k Wikidata-derived triplets annotated with fact popularity using Wikipedia link counts and LLM-based salience scores. Our experiments show that pretrained and SFT models respond differently to unlearning. An SFT step on the forget data yields smoother forgetting, more stable tuning, and 10-50% higher retention, while direct unlearning on pretrained models remains unstable and prone to relearning or catastrophic forgetting.",
    "summary": "",
    "translation": "遗忘机制剖析：事实显著性与模型微调的双重影响",
    "relevance_score": 2,
    "reasoning": "该论文主要研究LLM中的遗忘机制，属于模型编辑和知识更新的技术范畴，与推荐/搜索/广告系统的核心进展或直接应用关联较弱。虽然涉及模型微调技术，但未明确讨论其在推荐/搜索/广告场景下的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19598v1": {
    "title": "Eye-Tracking-while-Reading: A Living Survey of Datasets with Open Library Support",
    "url": "https://www.alphaxiv.org/abs/2602.19598v1",
    "arxiv_id": "2602.19598v1",
    "authors": "Deborah N. Jakobi, David R. Reich, Paul Prasse, Jana M. Hofmann, Lena S. Bolliger, Lena A. Jäger",
    "categories": "cs.CL",
    "pub_date": "2026-02-23 08:40:50",
    "ori_summary": "Eye-tracking-while-reading corpora are a valuable resource for many different disciplines and use cases. Use cases range from studying the cognitive processes underlying reading to machine-learning-based applications, such as gaze-based assessments of reading comprehension. The past decades have seen an increase in the number and size of eye-tracking-while-reading datasets as well as increasing diversity with regard to the stimulus languages covered, the linguistic background of the participants, or accompanying psychometric or demographic data. The spread of data across different disciplines and the lack of data sharing standards across the communities lead to many existing datasets that cannot be easily reused due to a lack of interoperability. In this work, we aim at creating more transparency and clarity with regards to existing datasets and their features across different disciplines by i) presenting an extensive overview of existing datasets, ii) simplifying the sharing of newly created datasets by publishing a living overview online, https://dili-lab.github.io/datasets.html, presenting over 45 features for each dataset, and iii) integrating all publicly available datasets into the Python package pymovements which offers an eye-tracking datasets library. By doing so, we aim to strengthen the FAIR principles in eye-tracking-while-reading research and promote good scientific practices, such as reproducing and replicating studies.",
    "summary": "",
    "translation": "阅读时眼动追踪：具有开放库支持的数据集动态综述",
    "relevance_score": 1,
    "reasoning": "该论文主要关注眼动追踪数据集和开放库支持，属于特定领域的数据收集和工具开发，与推荐系统、搜索或广告的核心技术进展、LLM应用或Transformer架构改进无直接关联。眼动追踪可能在某些用户体验研究中应用，但论文标题未表明与当前关注的任何技术领域有明确联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19583v1": {
    "title": "DEEP: Docker-based Execution and Evaluation Platform",
    "url": "https://www.alphaxiv.org/abs/2602.19583v1",
    "arxiv_id": "2602.19583v1",
    "authors": "Sergio Gómez González, Miguel Domingo, Francisco Casacuberta",
    "categories": "cs.CL",
    "pub_date": "2026-02-23 08:08:57",
    "ori_summary": "Comparative evaluation of several systems is a recurrent task in researching. It is a key step before deciding which system to use for our work, or, once our research has been conducted, to demonstrate the potential of the resulting model. Furthermore, it is the main task of competitive, public challenges evaluation. Our proposed software (DEEP) automates both the execution and scoring of machine translation and optical character recognition models. Furthermore, it is easily extensible to other tasks. DEEP is prepared to receive dockerized systems, run them (extracting information at that same time), and assess hypothesis against some references. With this approach, evaluators can achieve a better understanding of the performance of each model. Moreover, the software uses a clustering algorithm based on a statistical analysis of the significance of the results yielded by each model, according to the evaluation metrics. As a result, evaluators are able to identify clusters of performance among the swarm of proposals and have a better understanding of the significance of their differences. Additionally, we offer a visualization web-app to ensure that the results can be adequately understood and interpreted. Finally, we present an exemplary case of use of DEEP.",
    "summary": "",
    "translation": "DEEP：基于Docker的执行与评估平台",
    "relevance_score": 1,
    "reasoning": "该论文标题描述了一个容器化执行平台，属于基础设施工具范畴，与推荐系统、搜索、广告的核心算法、架构或LLM应用无直接关联。它不涉及任何指定的技术焦点领域，包括Transformer架构、LLM技术或异构数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19569v1": {
    "title": "Temporal-Aware Heterogeneous Graph Reasoning with Multi-View Fusion for Temporal Question Answering",
    "url": "https://www.alphaxiv.org/abs/2602.19569v1",
    "arxiv_id": "2602.19569v1",
    "authors": "Wuzhenghong Wen, Bowen Zhou, Jinwen Huang, Xianjie Wu, Yuwei Sun, Su Pan, Liang Li, Jianting Liu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-02-23 07:36:36",
    "ori_summary": "Question Answering over Temporal Knowledge Graphs (TKGQA) has attracted growing interest for handling time-sensitive queries. However, existing methods still struggle with: 1) weak incorporation of temporal constraints in question representation, causing biased reasoning; 2) limited ability to perform explicit multi-hop reasoning; and 3) suboptimal fusion of language and graph representations. We propose a novel framework with temporal-aware question encoding, multi-hop graph reasoning, and multi-view heterogeneous information fusion. Specifically, our approach introduces: 1) a constraint-aware question representation that combines semantic cues from language models with temporal entity dynamics; 2) a temporal-aware graph neural network for explicit multi-hop reasoning via time-aware message passing; and 3) a multi-view attention mechanism for more effective fusion of question context and temporal graph knowledge. Experiments on multiple TKGQA benchmarks demonstrate consistent improvements over multiple baselines.",
    "summary": "该论文研究时序知识图谱问答中的三个核心问题：弱时间约束、有限多跳推理、次优语言-图表示融合。其核心方法是设计一个结合约束感知问题表示、时间感知图神经网络进行显式多跳推理、以及多视图注意力机制进行异构信息融合的统一框架。",
    "translation": "基于多视图融合的时序感知异构图推理用于时序问答",
    "relevance_score": 8,
    "reasoning": "该论文涉及异构图推理和多视图融合，与VLM类比处理异构数据的理念高度相关，可用于建模用户行为序列和上下文特征等不同模态。时序感知能力在推荐和搜索中至关重要，可用于处理用户兴趣演变和动态内容排序。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出了一种结合时间感知图神经网络与多视图注意力机制的框架，其核心方法（时间感知消息传递、异构信息融合）与推荐/搜索系统中处理时序用户行为和多模态数据的挑战直接相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.19548v1": {
    "title": "Beyond a Single Extractor: Re-thinking HTML-to-Text Extraction for LLM Pretraining",
    "url": "https://www.alphaxiv.org/abs/2602.19548v1",
    "arxiv_id": "2602.19548v1",
    "authors": "Jeffrey Li, Josh Gardner, Doug Kang, Fangping Shi, Karanjeet Singh, Chun-Liang Li, Herumb Shandilya, David Hall, Oncel Tuzel, Percy Liang, Ludwig Schmidt, Hadi Pour Ansari, Fartash Faghri",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2026-02-23 06:41:57",
    "ori_summary": "One of the first pre-processing steps for constructing web-scale LLM pretraining datasets involves extracting text from HTML. Despite the immense diversity of web content, existing open-source datasets predominantly apply a single fixed extractor to all webpages. In this work, we investigate whether this practice leads to suboptimal coverage and utilization of Internet data. We first show that while different extractors may lead to similar model performance on standard language understanding tasks, the pages surviving a fixed filtering pipeline can differ substantially. This suggests a simple intervention: by taking a Union over different extractors, we can increase the token yield of DCLM-Baseline by up to 71% while maintaining benchmark performance. We further show that for structured content such as tables and code blocks, extractor choice can significantly impact downstream task performance, with differences of up to 10 percentage points (p.p.) on WikiTQ and 3 p.p. on HumanEval.",
    "summary": "该论文研究网页HTML到文本提取方法对构建LLM预训练数据集的影响问题，核心思想是单一提取器会限制数据覆盖和利用，提出通过组合不同提取器的方法来提升数据多样性和下游任务性能。",
    "translation": "超越单一提取器：重新思考用于大语言模型预训练的HTML到文本提取",
    "relevance_score": 4,
    "reasoning": "该论文关注HTML到文本提取技术，这是LLM预训练中数据准备的关键环节，属于'Enabling LLM Tech'范畴。虽然不直接涉及推荐系统、搜索或广告，但改进的网页内容提取技术可以提升用于这些领域的LLM训练数据的质量和多样性，从而间接增强模型在相关任务上的表现。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文研究HTML到文本提取方法对LLM预训练数据质量的影响，属于LLM技术基础进步领域，直接关系到大规模推荐和搜索系统的数据预处理质量。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.19526v1": {
    "title": "How to Train Your Deep Research Agent? Prompt, Reward, and Policy Optimization in Search-R1",
    "url": "https://www.alphaxiv.org/abs/2602.19526v1",
    "arxiv_id": "2602.19526v1",
    "authors": "Yinuo Xu, Shuo Lu, Jianjie Cheng, Meng Wang, Qianlong Xie, Xingxing Wang, Ran He, Jian Liang",
    "categories": "cs.CL",
    "pub_date": "2026-02-23 05:33:17",
    "ori_summary": "Deep Research agents tackle knowledge-intensive tasks through multi-round retrieval and decision-oriented generation. While reinforcement learning (RL) has been shown to improve performance in this paradigm, its contributions remain underexplored. To fully understand the role of RL, we conduct a systematic study along three decoupled dimensions: prompt template, reward function, and policy optimization. Our study reveals that: 1) the Fast Thinking template yields greater stability and better performance than the Slow Thinking template used in prior work; 2) the F1-based reward underperforms the EM due to training collapse driven by answer avoidance; this can be mitigated by incorporating action-level penalties, ultimately surpassing EM; 3) REINFORCE outperforms PPO while requiring fewer search actions, whereas GRPO shows the poorest stability among policy optimization methods. Building on these insights, we then introduce Search-R1++, a strong baseline that improves the performance of Search-R1 from 0.403 to 0.442 (Qwen2.5-7B) and 0.289 to 0.331 (Qwen2.5-3B). We hope that our findings can pave the way for more principled and reliable RL training strategies in Deep Research systems.",
    "summary": "论文研究如何通过系统优化强化学习的三个关键维度（提示模板、奖励函数、策略优化）来训练深度研究智能体，以解决其在知识密集型搜索任务中的多轮检索和决策生成问题。核心思想是将强化学习训练分解为三个可独立优化的组件，通过对比分析不同设计选择（如Fast Thinking模板、基于EM的奖励函数、REINFORCE算法）来构建更稳定有效的训练策略。",
    "translation": "如何训练你的深度研究智能体？搜索-R1中的提示、奖励与策略优化",
    "relevance_score": 8,
    "reasoning": "该论文标题直接涉及搜索领域（Search-R1）中的智能体训练，涵盖提示、奖励和策略优化，这属于核心搜索领域进展。虽然未明确提及LLM，但提示工程和策略优化技术可直接应用于搜索系统的查询理解、结果排序和个性化推荐，符合直接LLM应用和核心领域进展的关注点。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文系统研究强化学习在深度研究智能体中的三大组件（提示模板、奖励函数、策略优化），直接针对搜索领域的决策生成任务，与用户关注的搜索系统核心进展和LLM直接应用高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.19517v1": {
    "title": "Classroom Final Exam: An Instructor-Tested Reasoning Benchmark",
    "url": "https://www.alphaxiv.org/abs/2602.19517v1",
    "arxiv_id": "2602.19517v1",
    "authors": "Chongyang Gao, Diji Yang, Shuyan Zhou, Xichen Yan, Luchuan Song, Shuo Li, Kezhen Chen",
    "categories": "cs.AI, cs.CE, cs.CL, cs.CV",
    "pub_date": "2026-02-23 05:17:41",
    "ori_summary": "We introduce \\CFE{} (\\textbf{C}lassroom \\textbf{F}inal \\textbf{E}xam), a multimodal benchmark for evaluating the reasoning capabilities of large language models across more than 20 STEM domains. \\CFE{} is curated from repeatedly used, authentic university homework and exam problems, together with reference solutions provided by course instructors. \\CFE{} presents a significant challenge even for frontier models: the newly released Gemini-3.1-pro-preview achieves an overall accuracy of 59.69\\%, while the second-best model, Gemini-3-flash-preview, reaches 55.46\\%, leaving considerable room for improvement. Beyond leaderboard results, we perform a diagnostic analysis by decomposing reference solutions into reasoning flows. We find that although frontier models can often answer intermediate sub-questions correctly, they struggle to reliably derive and maintain correct intermediate states throughout multi-step solutions. We further observe that model-generated solutions typically have more reasoning steps than those provided by the instructor, indicating suboptimal step efficiency and a higher risk of error accumulation. The data and code are available at https://github.com/Analogy-AI/CFE_Bench.",
    "summary": "",
    "translation": "课堂期末考试：一项经过教师测试的推理基准",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向教育领域的评估基准创建，属于纯粹的NLP评估基准主题，与推荐系统、搜索或广告的核心技术进展、LLM应用或Transformer架构改进均无直接关联。即使考虑LLM推理能力，该论文专注于特定教育场景的基准测试，缺乏对RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19509v1": {
    "title": "Pyramid MoA: A Probabilistic Framework for Cost-Optimized Anytime Inference",
    "url": "https://www.alphaxiv.org/abs/2602.19509v1",
    "arxiv_id": "2602.19509v1",
    "authors": "Arindam Khaled",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2026-02-23 04:47:47",
    "ori_summary": "Large Language Models (LLMs) face a persistent trade-off between inference cost and reasoning capability. While \"Oracle\" models (e.g., Llama-3-70B) achieve state-of-the-art accuracy, they are prohibitively expensive for high-volume deployment. Smaller models (e.g., 8B parameters) are cost-effective but struggle with complex tasks. In this work, we propose \"Pyramid MoA\", a hierarchical Mixture-of-Agents architecture that uses a lightweight Router to dynamically escalate queries only when necessary. By leveraging semantic agreement and confidence calibration among an ensemble of small models, our Router identifies \"hard\" problems with high precision. On the GSM8K benchmark, our system achieves 93.0% accuracy, effectively matching the Oracle baseline (98.0%) while reducing compute costs by 61%. We demonstrate that the system introduces negligible latency overhead (+0.82s) and allows for a tunable trade-off between performance and budget.",
    "summary": "该论文研究LLM推理成本与能力间的权衡问题，核心方法是构建分层专家混合架构，利用轻量级路由器基于语义一致性和置信度校准动态分配查询，仅在必要时调用大模型。",
    "translation": "金字塔MoA：一种面向成本优化任意时间推理的概率框架",
    "relevance_score": 8,
    "reasoning": "该论文涉及MoE（专家混合）架构的优化，属于'Enabling Transformer Tech'范畴，特别是效率改进方面。成本优化的推理框架可直接应用于推荐系统、搜索或广告中的大规模模型部署，通过动态调整计算资源来平衡精度与延迟。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的分层MoA架构通过动态路由机制优化LLM推理成本，直接关联Transformer效率提升和LLM应用部署的实际需求。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.19467v1": {
    "title": "Can Large Language Models Replace Human Coders? Introducing ContentBench",
    "url": "https://www.alphaxiv.org/abs/2602.19467v1",
    "arxiv_id": "2602.19467v1",
    "authors": "Michael Haman",
    "categories": "cs.CY, cs.AI, cs.CL",
    "pub_date": "2026-02-23 03:26:17",
    "ori_summary": "Can low-cost large language models (LLMs) take over the interpretive coding work that still anchors much of empirical content analysis? This paper introduces ContentBench, a public benchmark suite that helps answer this replacement question by tracking how much agreement low-cost LLMs achieve and what they cost on the same interpretive coding tasks. The suite uses versioned tracks that invite researchers to contribute new benchmark datasets. I report results from the first track, ContentBench-ResearchTalk v1.0: 1,000 synthetic, social-media-style posts about academic research labeled into five categories spanning praise, critique, sarcasm, questions, and procedural remarks. Reference labels are assigned only when three state-of-the-art reasoning models (GPT-5, Gemini 2.5 Pro, and Claude Opus 4.1) agree unanimously, and all final labels are checked by the author as a quality-control audit. Among the 59 evaluated models, the best low-cost LLMs reach roughly 97-99% agreement with these jury labels, far above GPT-3.5 Turbo, the model behind early ChatGPT and the initial wave of LLM-based text annotation. Several top models can code 50,000 posts for only a few dollars, pushing large-scale interpretive coding from a labor bottleneck toward questions of validation, reporting, and governance. At the same time, small open-weight models that run locally still struggle on sarcasm-heavy items (for example, Llama 3.2 3B reaches only 4% agreement on hard-sarcasm). ContentBench is released with data, documentation, and an interactive quiz at contentbench.github.io to support comparable evaluations over time and to invite community extensions.",
    "summary": "",
    "translation": "大型语言模型能否取代人类编码员？内容基准测试的引入",
    "relevance_score": 2,
    "reasoning": "该论文标题主要探讨LLM在编码任务中替代人类的能力，并引入了一个基准测试。这属于纯粹的LLM能力评估范畴，与RecSys/Search/Ads的核心技术进展、Transformer架构改进或LLM在推荐/搜索/广告中的直接应用没有明确关联。虽然基准测试可能涉及内容分析，但论文焦点似乎是通用编码能力评估，而非针对推荐/搜索/广告领域的特定应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19463v1": {
    "title": "PuppetChat: Fostering Intimate Communication through Bidirectional Actions and Micronarratives",
    "url": "https://www.alphaxiv.org/abs/2602.19463v1",
    "arxiv_id": "2602.19463v1",
    "authors": "Emma Jiren Wang, Siying Hu, Zhicong Lu",
    "categories": "cs.HC, cs.AI, cs.CL, cs.CY",
    "pub_date": "2026-02-23 03:17:27",
    "ori_summary": "As a primary channel for sustaining modern intimate relationships, instant messaging facilitates frequent connection across distances. However, today's tools often dilute care; they favor single tap reactions and vague emojis that do not support two way action responses, do not preserve the feeling that the exchange keeps going without breaking, and are weakly tied to who we are and what we share. To address this challenge, we present PuppetChat, a dyadic messaging prototype that restores this expressive depth through embodied interaction. PuppetChat uses a reciprocity aware recommender to encourage responsive actions and generates personalized micronarratives from user stories to ground interactions in personal history. Our 10-day field study with 11 dyads of close partners or friends revealed that this approach enhanced social presence, supported more expressive self disclosure, and sustained continuity and shared memories.",
    "summary": "",
    "translation": "PuppetChat：通过双向操作与微叙事促进亲密沟通",
    "relevance_score": 2,
    "reasoning": "该论文标题聚焦于促进亲密沟通的交互系统，涉及双向操作和微叙事技术，这属于人机交互或对话系统领域。虽然可能涉及LLM技术，但缺乏与推荐系统、搜索或广告领域的明确关联，且未提及任何核心架构进展或跨模态建模方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19455v1": {
    "title": "SenTSR-Bench: Thinking with Injected Knowledge for Time-Series Reasoning",
    "url": "https://www.alphaxiv.org/abs/2602.19455v1",
    "arxiv_id": "2602.19455v1",
    "authors": "Zelin He, Boran Han, Xiyuan Zhang, Shuai Zhang, Haotian Lin, Qi Zhu, Haoyang Fang, Danielle C. Maddix, Abdul Fatir Ansari, Akash Chandrayan, Abhinav Pradhan, Bernie Wang, Matthew Reimherr",
    "categories": "cs.LG, cs.AI, cs.CL, stat.ML",
    "pub_date": "2026-02-23 02:55:32",
    "ori_summary": "Time-series diagnostic reasoning is essential for many applications, yet existing solutions face a persistent gap: general reasoning large language models (GRLMs) possess strong reasoning skills but lack the domain-specific knowledge to understand complex time-series patterns. Conversely, fine-tuned time-series LLMs (TSLMs) understand these patterns but lack the capacity to generalize reasoning for more complicated questions. To bridge this gap, we propose a hybrid knowledge-injection framework that injects TSLM-generated insights directly into GRLM's reasoning trace, thereby achieving strong time-series reasoning with in-domain knowledge. As collecting data for knowledge injection fine-tuning is costly, we further leverage a reinforcement learning-based approach with verifiable rewards (RLVR) to elicit knowledge-rich traces without human supervision, then transfer such an in-domain thinking trace into GRLM for efficient knowledge injection. We further release SenTSR-Bench, a multivariate time-series-based diagnostic reasoning benchmark collected from real-world industrial operations. Across SenTSR-Bench and other public datasets, our method consistently surpasses TSLMs by 9.1%-26.1% and GRLMs by 7.9%-22.4%, delivering robust, context-aware time-series diagnostic insights.",
    "summary": "",
    "translation": "SenTSR-Bench：基于注入知识的时序推理基准",
    "relevance_score": 2,
    "reasoning": "该论文标题表明其专注于时序推理的基准测试，这属于时序分析领域而非推荐系统、搜索或广告的核心技术。虽然时序数据在用户行为分析中有应用，但论文强调的是推理基准而非直接的应用方法或架构创新，与当前关注的LLM技术、Transformer架构改进或直接应用相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19403v1": {
    "title": "Personalized Prediction of Perceived Message Effectiveness Using Large Language Model Based Digital Twins",
    "url": "https://www.alphaxiv.org/abs/2602.19403v1",
    "arxiv_id": "2602.19403v1",
    "authors": "Jasmin Han, Janardan Devkota, Joseph Waring, Amanda Luken, Felix Naughton, Roger Vilardaga, Jonathan Bricker, Carl Latkin, Meghan Moran, Yiqun Chen, Johannes Thrul",
    "categories": "cs.CL, stat.AP",
    "pub_date": "2026-02-23 00:32:23",
    "ori_summary": "Perceived message effectiveness (PME) by potential intervention end-users is important for selecting and optimizing personalized smoking cessation intervention messages for mobile health (mHealth) platform delivery. This study evaluates whether large language models (LLMs) can accurately predict PME for smoking cessation messages. We evaluated multiple models for predicting PME across three domains: content quality, coping support, and quitting support. The dataset comprised 3010 message ratings (5-point Likert scale) from 301 young adult smokers. We compared (1) supervised learning models trained on labeled data, (2) zero and few-shot LLMs prompted without task-specific fine-tuning, and (3) LLM-based digital twins that incorporate individual characteristics and prior PME histories to generate personalized predictions. Model performance was assessed on three held-out messages per participant using accuracy, Cohen's kappa, and F1. LLM-based digital twins outperformed zero and few-shot LLMs (12 percentage points on average) and supervised baselines (13 percentage points), achieving accuracies of 0.49 (content), 0.45 (coping), and 0.49 (quitting), with directional accuracies of 0.75, 0.66, and 0.70 on a simplified 3-point scale. Digital twin predictions showed greater dispersion across rating categories, indicating improved sensitivity to individual differences. Integrating personal profiles with LLMs captures person-specific differences in PME and outperforms supervised and zero and few-shot approaches. Improved PME prediction may enable more tailored intervention content in mHealth. LLM-based digital twins show potential for supporting personalization of mobile smoking cessation and other health behavior change interventions.",
    "summary": "论文研究如何为移动健康戒烟干预消息预测个性化感知有效性。核心方法是利用大型语言模型构建整合个人特征和历史反馈的数字孪生，以捕捉个体差异并生成个性化预测。",
    "translation": "基于大语言模型数字孪生的个性化感知信息有效性预测",
    "relevance_score": 8,
    "reasoning": "该论文直接应用LLM技术构建数字孪生来预测个性化感知信息有效性，属于'Direct LLM Applications'范畴，与推荐系统/搜索/广告中的个性化内容优化高度相关。数字孪生技术可用于模拟用户对广告或推荐内容的反应，帮助优化信息传递策略。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用LLM技术构建数字孪生进行个性化预测，完美契合“直接LLM应用”焦点，并为推荐/广告的个性化内容生成提供方法论类比。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.20161v1": {
    "title": "Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device",
    "url": "https://www.alphaxiv.org/abs/2602.20161v1",
    "arxiv_id": "2602.20161v1",
    "authors": "Abdelrahman Shaker, Ahmed Heakl, Jaseel Muhammad, Ritesh Thawkar, Omkar Thawakar, Senmao Li, Hisham Cholakkal, Ian Reid, Eric P. Xing, Salman Khan, Fahad Shahbaz Khan",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 18:59:58",
    "ori_summary": "Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/",
    "summary": "",
    "translation": "Mobile-O：移动设备上的统一多模态理解与生成",
    "relevance_score": 2,
    "reasoning": "该论文标题关注移动设备上的多模态统一模型，虽然涉及多模态技术（可能与VLM类比相关），但未明确提及推荐系统、搜索或广告应用。标题强调移动设备部署，主要属于边缘计算/设备端AI范畴，而非您关注的RecSys/Search/Ads核心领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20160v1": {
    "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2602.20160v1",
    "arxiv_id": "2602.20160v1",
    "authors": "Chen Wang, Hao Tan, Wang Yifan, Zhiqin Chen, Yuheng Liu, Kalyan Sunkavalli, Sai Bi, Lingjie Liu, Yiwei Hu",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 18:59:45",
    "ori_summary": "We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.",
    "summary": "",
    "translation": "tttLRM：面向长上下文与自回归三维重建的测试时训练",
    "relevance_score": 2,
    "reasoning": "该论文主要关注3D重建和测试时训练技术，属于计算机视觉领域。虽然提到了“长上下文”和“自回归”等与LLM相关的概念，但核心应用是3D重建，这与推荐系统、搜索或广告的直接相关性较弱。没有明确证据表明该技术可应用于异构数据处理或推荐系统架构。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20159v1": {
    "title": "A Very Big Video Reasoning Suite",
    "url": "https://www.alphaxiv.org/abs/2602.20159v1",
    "arxiv_id": "2602.20159v1",
    "authors": "Maijunxian Wang, Ruisi Wang, Juyi Lin, Ran Ji, Thaddäus Wiedemer, Qingying Gao, Dezhi Luo, Yaoyao Qian, Lianyu Huang, Zelong Hong, Jiahui Ge, Qianli Ma, Hang He, Yifan Zhou, Lingzi Guo, Lantao Mei, Jiachen Li, Hanwen Xing, Tianqi Zhao, Fengyuan Yu, Weihang Xiao, Yizheng Jiao, Jianheng Hou, Danyang Zhang, Pengcheng Xu, Boyang Zhong, Zehong Zhao, Gaoyun Fang, John Kitaoka, Yile Xu, Hua Xu, Kenton Blacutt, Tin Nguyen, Siyuan Song, Haoran Sun, Shaoyue Wen, Linyang He, Runming Wang, Yanzhi Wang, Mengyue Yang, Ziqiao Ma, Raphaël Millière, Freda Shi, Nuno Vasconcelos, Daniel Khashabi, Alan Yuille, Yilun Du, Ziming Liu, Bo Li, Dahua Lin, Ziwei Liu, Vikash Kumar, Yijiang Li, Lei Yang, Zhongang Cai, Hokin Deng",
    "categories": "cs.CV, cs.AI, cs.LG, cs.MM, cs.RO",
    "pub_date": "2026-02-23 18:59:41",
    "ori_summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .",
    "summary": "",
    "translation": "一个非常庞大的视频推理套件",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于视频推理，属于纯粹的视觉领域研究。根据您的关注点排除标准，纯粹视觉论文若没有明确展示与推荐系统、搜索或广告的相关性，则被视为不相关。标题中未提及任何可能应用于这些领域的跨模态建模或异构数据处理方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20157v1": {
    "title": "Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning",
    "url": "https://www.alphaxiv.org/abs/2602.20157v1",
    "arxiv_id": "2602.20157v1",
    "authors": "Zhongxiao Cong, Qitao Zhao, Minsik Jeon, Shubham Tulsiani",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 18:59:30",
    "ori_summary": "Current feed-forward 3D/4D reconstruction systems rely on dense geometry and pose supervision -- expensive to obtain at scale and particularly scarce for dynamic real-world scenes. We present Flow3r, a framework that augments visual geometry learning with dense 2D correspondences (`flow') as supervision, enabling scalable training from unlabeled monocular videos. Our key insight is that the flow prediction module should be factored: predicting flow between two images using geometry latents from one and pose latents from the other. This factorization directly guides the learning of both scene geometry and camera motion, and naturally extends to dynamic scenes. In controlled experiments, we show that factored flow prediction outperforms alternative designs and that performance scales consistently with unlabeled data. Integrating factored flow into existing visual geometry architectures and training with ${\\sim}800$K unlabeled videos, Flow3r achieves state-of-the-art results across eight benchmarks spanning static and dynamic scenes, with its largest gains on in-the-wild dynamic videos where labeled data is most scarce.",
    "summary": "",
    "translation": "Flow3r：基于分解流预测的可扩展视觉几何学习",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及视觉几何学习，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然流预测技术可能在某些多模态系统中作为辅助模块，但论文标题明确聚焦于视觉几何，缺乏与文本、用户行为或推荐排序等关键要素的联系，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20150v1": {
    "title": "Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization",
    "url": "https://www.alphaxiv.org/abs/2602.20150v1",
    "arxiv_id": "2602.20150v1",
    "authors": "Wei-Cheng Huang, Jiaheng Han, Xiaohan Ye, Zherong Pan, Kris Hauser",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2026-02-23 18:58:24",
    "ori_summary": "Estimating simulation-ready scenes from real-world observations is crucial for downstream planning and policy learning tasks. Regretfully, existing methods struggle in cluttered environments, often exhibiting prohibitive computational cost, poor robustness, and restricted generality when scaling to multiple interacting objects. We propose a unified optimization-based formulation for real-to-sim scene estimation that jointly recovers the shapes and poses of multiple rigid objects under physical constraints. Our method is built on two key technical innovations. First, we leverage the recently introduced shape-differentiable contact model, whose global differentiability permits joint optimization over object geometry and pose while modeling inter-object contacts. Second, we exploit the structured sparsity of the augmented Lagrangian Hessian to derive an efficient linear system solver whose computational cost scales favorably with scene complexity. Building on this formulation, we develop an end-to-end real-to-sim scene estimation pipeline that integrates learning-based object initialization, physics-constrained joint shape-pose optimization, and differentiable texture refinement. Experiments on cluttered scenes with up to 5 objects and 22 convex hulls demonstrate that our approach robustly reconstructs physically valid, simulation-ready object shapes and poses.",
    "summary": "",
    "translation": "基于物理感知的联合形状与姿态优化的仿真就绪杂乱场景估计",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于计算机视觉中的场景重建与物理仿真，涉及形状优化、姿态估计和物理感知技术，属于纯粹的视觉或3D视觉领域。虽然物理感知和优化技术可能具有通用性，但标题未表明与推荐系统、搜索或广告的直接关联，也未提及LLM、Transformer架构或多模态建模等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20137v1": {
    "title": "Do Large Language Models Understand Data Visualization Rules?",
    "url": "https://www.alphaxiv.org/abs/2602.20137v1",
    "arxiv_id": "2602.20137v1",
    "authors": "Martin Sinnona, Valentin Bonas, Emmanuel Iarussi, Viviana Siless",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 18:47:51",
    "ori_summary": "Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. Constraint-based systems such as Draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivating the use of LLMs as flexible rule validators. In this paper, we present the first systematic evaluation of LLMs against visualization rules using hard-verification ground truth derived from Answer Set Programming (ASP). We translated a subset of Draco's constraints into natural-language statements and generated a controlled dataset of 2,000 Vega-Lite specifications annotated with explicit rule violations. LLMs were evaluated on both accuracy in detecting violations and prompt adherence, which measures whether outputs follow the required structured format. Results show that frontier models achieve high adherence (Gemma 3 4B / 27B: 100%, GPT-oss 20B: 98%) and reliably detect common violations (F1 up to 0.82),yet performance drops for subtler perceptual rules (F1 < 0.15 for some categories) and for outputs generated from technical ASP formulations.Translating constraints into natural language improved performance by up to 150% for smaller models. These findings demonstrate the potential of LLMs as flexible, language-driven validators while highlighting their current limitations compared to symbolic solvers.",
    "summary": "",
    "translation": "大型语言模型理解数据可视化规则吗？",
    "relevance_score": 2,
    "reasoning": "该论文主要探讨LLM在数据可视化规则理解方面的能力，属于LLM能力评估范畴，与您关注的RecSys/Search/Ads核心领域进展、LLM技术应用或Transformer架构改进等焦点关联较弱。虽然LLM理解能力可能间接影响某些应用场景，但论文本身更偏向NLP评估而非直接的技术应用或架构创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20119v1": {
    "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning",
    "url": "https://www.alphaxiv.org/abs/2602.20119v1",
    "arxiv_id": "2602.20119v1",
    "authors": "Jiahui Fu, Junyu Nan, Lingfeng Sun, Hongyu Li, Jianing Qian, Jennifer L. Barry, Kris Kitani, George Konidaris",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2026-02-23 18:35:18",
    "ori_summary": "Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/",
    "summary": "",
    "translation": "NovaPlan：通过闭环视频语言规划实现零样本长时程操作",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及机器人操作和视频语言规划，属于机器人学领域，与推荐系统、搜索或广告的核心关注点无直接关联。虽然提到了语言模型，但其应用场景（长时程操作）与RecSys/Search/Ads的典型问题（如排序、检索、用户建模）不匹配，且未明确指向这些领域的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20114v1": {
    "title": "Benchmarking Unlearning for Vision Transformers",
    "url": "https://www.alphaxiv.org/abs/2602.20114v1",
    "arxiv_id": "2602.20114v1",
    "authors": "Kairan Zhao, Iurie Luca, Peter Triantafillou",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-23 18:33:16",
    "ori_summary": "Research in machine unlearning (MU) has gained strong momentum: MU is now widely regarded as a critical capability for building safe and fair AI. In parallel, research into transformer architectures for computer vision tasks has been highly successful: Increasingly, Vision Transformers (VTs) emerge as strong alternatives to CNNs. Yet, MU research for vision tasks has largely centered on CNNs, not VTs. While benchmarking MU efforts have addressed LLMs, diffusion models, and CNNs, none exist for VTs. This work is the first to attempt this, benchmarking MU algorithm performance in different VT families (ViT and Swin-T) and at different capacities. The work employs (i) different datasets, selected to assess the impacts of dataset scale and complexity; (ii) different MU algorithms, selected to represent fundamentally different approaches for MU; and (iii) both single-shot and continual unlearning protocols. Additionally, it focuses on benchmarking MU algorithms that leverage training data memorization, since leveraging memorization has been recently discovered to significantly improve the performance of previously SOTA algorithms. En route, the work characterizes how VTs memorize training data relative to CNNs, and assesses the impact of different memorization proxies on performance. The benchmark uses unified evaluation metrics that capture two complementary notions of forget quality along with accuracy on unseen (test) data and on retained data. Overall, this work offers a benchmarking basis, enabling reproducible, fair, and comprehensive comparisons of existing (and future) MU algorithms on VTs. And, for the first time, it sheds light on how well existing algorithms work in VT settings, establishing a promising reference performance baseline.",
    "summary": "",
    "translation": "视觉Transformer遗忘能力的基准测试",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉Transformer的遗忘能力基准测试，属于纯粹的视觉领域研究，与推荐系统、搜索或广告的核心技术进展没有直接关联。虽然Transformer架构是相关领域的基础技术，但该论文未明确讨论其在推荐、搜索或广告中的潜在应用，也未涉及异构数据处理或多模态建模等当前关注方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20100v1": {
    "title": "Transcending the Annotation Bottleneck: AI-Powered Discovery in Biology and Medicine",
    "url": "https://www.alphaxiv.org/abs/2602.20100v1",
    "arxiv_id": "2602.20100v1",
    "authors": "Soumick Chatterjee",
    "categories": "cs.CV, cs.AI, eess.IV",
    "pub_date": "2026-02-23 18:15:30",
    "ori_summary": "The dependence on expert annotation has long constituted the primary rate-limiting step in the application of artificial intelligence to biomedicine. While supervised learning drove the initial wave of clinical algorithms, a paradigm shift towards unsupervised and self-supervised learning (SSL) is currently unlocking the latent potential of biobank-scale datasets. By learning directly from the intrinsic structure of data - whether pixels in a magnetic resonance image (MRI), voxels in a volumetric scan, or tokens in a genomic sequence - these methods facilitate the discovery of novel phenotypes, the linkage of morphology to genetics, and the detection of anomalies without human bias. This article synthesises seminal and recent advances in \"learning without labels,\" highlighting how unsupervised frameworks can derive heritable cardiac traits, predict spatial gene expression in histology, and detect pathologies with performance that rivals or exceeds supervised counterparts.",
    "summary": "",
    "translation": "超越标注瓶颈：人工智能驱动的生物学与医学发现",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于生物学和医学领域的AI应用，这属于明确的无关主题范畴。虽然提到了AI技术，但没有任何迹象表明该研究涉及推荐系统、搜索、广告或相关Transformer架构进展，也没有展示出对异构数据统一建模的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20089v1": {
    "title": "StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues",
    "url": "https://www.alphaxiv.org/abs/2602.20089v1",
    "arxiv_id": "2602.20089v1",
    "authors": "Zanxi Ruan, Qiuyu Kong, Songqun Gao, Yiming Wang, Marco Cristani",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-23 17:57:37",
    "ori_summary": "Edge-based representations are fundamental cues for visual understanding, a principle rooted in early vision research and still central today. We extend this principle to vision-language alignment, showing that isolating and aligning structural cues across modalities can greatly benefit fine-tuning on long, detail-rich captions, with a specific focus on improving cross-modal retrieval. We introduce StructXLIP, a fine-tuning alignment paradigm that extracts edge maps (e.g., Canny), treating them as proxies for the visual structure of an image, and filters the corresponding captions to emphasize structural cues, making them \"structure-centric\". Fine-tuning augments the standard alignment loss with three structure-centric losses: (i) aligning edge maps with structural text, (ii) matching local edge regions to textual chunks, and (iii) connecting edge maps to color images to prevent representation drift. From a theoretical standpoint, while standard CLIP maximizes the mutual information between visual and textual embeddings, StructXLIP additionally maximizes the mutual information between multimodal structural representations. This auxiliary optimization is intrinsically harder, guiding the model toward more robust and semantically stable minima, enhancing vision-language alignment. Beyond outperforming current competitors on cross-modal retrieval in both general and specialized domains, our method serves as a general boosting recipe that can be integrated into future approaches in a plug-and-play manner. Code and pretrained models are publicly available at: https://github.com/intelligolabs/StructXLIP.",
    "summary": "",
    "translation": "StructXLIP：利用多模态结构线索增强视觉语言模型",
    "relevance_score": 3,
    "reasoning": "该论文属于视觉语言模型（VLM）领域，与您关注的“VLM类比处理异构数据”有一定相关性，但主要聚焦于视觉-文本模态，而非您关注的推荐/搜索/广告中的异构数据（如上下文特征、用户序列）。虽然结构线索技术可能启发异构数据建模，但论文本身未明确指向这些应用场景，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20084v1": {
    "title": "Do Large Language Models Understand Data Visualization Principles?",
    "url": "https://www.alphaxiv.org/abs/2602.20084v1",
    "arxiv_id": "2602.20084v1",
    "authors": "Martin Sinnona, Valentin Bonas, Viviana Siless, Emmanuel Iarussi",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 17:51:06",
    "ori_summary": "Data visualization principles, derived from decades of research in design and perception, ensure proper visual communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they and their vision-language counterparts (VLMs) can reason about and enforce visualization principles directly. Constraint based systems encode these principles as logical rules for precise automated checks, but translating them into formal specifications demands expert knowledge. This motivates leveraging LLMs and VLMs as principle checkers that can reason about visual design directly, bypassing the need for symbolic rule specification. In this paper, we present the first systematic evaluation of both LLMs and VLMs on their ability to reason about visualization principles, using hard verification ground truth derived from Answer Set Programming (ASP). We compiled a set of visualization principles expressed as natural-language statements and generated a controlled dataset of approximately 2,000 Vega-Lite specifications annotated with explicit principle violations, complemented by over 300 real-world Vega-Lite charts. We evaluated both checking and fixing tasks, assessing how well models detect principle violations and correct flawed chart specifications. Our work highlights both the promise of large (vision-)language models as flexible validators and editors of visualization designs and the persistent gap with symbolic solvers on more nuanced aspects of visual perception. They also reveal an interesting asymmetry: frontier models tend to be more effective at correcting violations than at detecting them reliably.",
    "summary": "",
    "translation": "大型语言模型是否理解数据可视化原理？",
    "relevance_score": 2,
    "reasoning": "该论文主要探讨LLM在数据可视化理解方面的能力，属于纯粹的LLM能力评估范畴。虽然LLM理解能力可能间接影响某些应用，但论文标题没有明确指向推荐系统、搜索或广告领域的直接应用或技术赋能，更侧重于基础NLP评估。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20079v1": {
    "title": "SemanticNVS: Improving Semantic Scene Understanding in Generative Novel View Synthesis",
    "url": "https://www.alphaxiv.org/abs/2602.20079v1",
    "arxiv_id": "2602.20079v1",
    "authors": "Xinya Chen, Christopher Wewer, Jiahao Xie, Xinting Hu, Jan Eric Lenssen",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 17:45:21",
    "ori_summary": "We present SemanticNVS, a camera-conditioned multi-view diffusion model for novel view synthesis (NVS), which improves generation quality and consistency by integrating pre-trained semantic feature extractors. Existing NVS methods perform well for views near the input view, however, they tend to generate semantically implausible and distorted images under long-range camera motion, revealing severe degradation. We speculate that this degradation is due to current models failing to fully understand their conditioning or intermediate generated scene content. Here, we propose to integrate pre-trained semantic feature extractors to incorporate stronger scene semantics as conditioning to achieve high-quality generation even at distant viewpoints. We investigate two different strategies, (1) warped semantic features and (2) an alternating scheme of understanding and generation at each denoising step. Experimental results on multiple datasets demonstrate the clear qualitative and quantitative (4.69%-15.26% in FID) improvement over state-of-the-art alternatives.",
    "summary": "",
    "translation": "SemanticNVS：在生成式新视角合成中改进语义场景理解",
    "relevance_score": 2,
    "reasoning": "该论文主要关注生成式新视角合成中的语义场景理解，属于计算机视觉领域。虽然标题中提到了“语义理解”，但该技术主要针对3D场景重建和生成，与推荐系统、搜索或广告中的异构数据处理没有直接关联。没有明确的证据表明该技术可以应用于推荐系统、搜索或广告领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20068v1": {
    "title": "The Invisible Gorilla Effect in Out-of-distribution Detection",
    "url": "https://www.alphaxiv.org/abs/2602.20068v1",
    "arxiv_id": "2602.20068v1",
    "authors": "Harry Anthony, Ziyun Liang, Hermione Warr, Konstantinos Kamnitsas",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-02-23 17:24:18",
    "ori_summary": "Deep Neural Networks achieve high performance in vision tasks by learning features from regions of interest (ROI) within images, but their performance degrades when deployed on out-of-distribution (OOD) data that differs from training data. This challenge has led to OOD detection methods that aim to identify and reject unreliable predictions. Although prior work shows that OOD detection performance varies by artefact type, the underlying causes remain underexplored. To this end, we identify a previously unreported bias in OOD detection: for hard-to-detect artefacts (near-OOD), detection performance typically improves when the artefact shares visual similarity (e.g. colour) with the model's ROI and drops when it does not - a phenomenon we term the Invisible Gorilla Effect. For example, in a skin lesion classifier with red lesion ROI, we show the method Mahalanobis Score achieves a 31.5% higher AUROC when detecting OOD red ink (similar to ROI) compared to black ink (dissimilar) annotations. We annotated artefacts by colour in 11,355 images from three public datasets (e.g. ISIC) and generated colour-swapped counterfactuals to rule out dataset bias. We then evaluated 40 OOD methods across 7 benchmarks and found significant performance drops for most methods when artefacts differed from the ROI. Our findings highlight an overlooked failure mode in OOD detection and provide guidance for more robust detectors. Code and annotations are available at: https://github.com/HarryAnthony/Invisible_Gorilla_Effect.",
    "summary": "",
    "translation": "分布外检测中的隐形大猩猩效应",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及分布外检测中的认知偏差问题，属于模型评估和可靠性领域。虽然分布外检测在机器学习中很重要，但该标题没有明确指向推荐系统、搜索或广告领域的核心进展、LLM技术应用、Transformer架构改进，也没有涉及异构数据统一建模的VLM类比方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20066v1": {
    "title": "HeatPrompt: Zero-Shot Vision-Language Modeling of Urban Heat Demand from Satellite Images",
    "url": "https://www.alphaxiv.org/abs/2602.20066v1",
    "arxiv_id": "2602.20066v1",
    "authors": "Kundan Thota, Xuanhao Mu, Thorsten Schlachter, Veit Hagenmeyer",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-23 17:22:54",
    "ori_summary": "Accurate heat-demand maps play a crucial role in decarbonizing space heating, yet most municipalities lack detailed building-level data needed to calculate them. We introduce HeatPrompt, a zero-shot vision-language energy modeling framework that estimates annual heat demand using semantic features extracted from satellite images, basic Geographic Information System (GIS), and building-level features. We feed pretrained Large Vision Language Models (VLMs) with a domain-specific prompt to act as an energy planner and extract the visual attributes such as roof age, building density, etc, from the RGB satellite image that correspond to the thermal load. A Multi-Layer Perceptron (MLP) regressor trained on these captions shows an $R^2$ uplift of 93.7% and shrinks the mean absolute error (MAE) by 30% compared to the baseline model. Qualitative analysis shows that high-impact tokens align with high-demand zones, offering lightweight support for heat planning in data-scarce regions.",
    "summary": "",
    "translation": "HeatPrompt：基于卫星图像的零样本视觉语言建模用于城市热需求预测",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及卫星图像分析和城市热需求预测，属于特定领域应用（城市/环境科学），与推荐系统、搜索或广告的核心技术进展无关。虽然提到了视觉语言模型，但应用场景完全在无关领域，且没有表明对RecSys/Search/Ads的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20060v1": {
    "title": "MeanFuser: Fast One-Step Multi-Modal Trajectory Generation and Adaptive Reconstruction via MeanFlow for End-to-End Autonomous Driving",
    "url": "https://www.alphaxiv.org/abs/2602.20060v1",
    "arxiv_id": "2602.20060v1",
    "authors": "Junli Wang, Xueyi Liu, Yinan Zheng, Zebing Xing, Pengfei Li, Guang Li, Kun Ma, Guang Chen, Hangjun Ye, Zhongpu Xia, Long Chen, Qichao Zhang",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2026-02-23 17:17:26",
    "ori_summary": "Generative models have shown great potential in trajectory planning. Recent studies demonstrate that anchor-guided generative models are effective in modeling the uncertainty of driving behaviors and improving overall performance. However, these methods rely on discrete anchor vocabularies that must sufficiently cover the trajectory distribution during testing to ensure robustness, inducing an inherent trade-off between vocabulary size and model performance. To overcome this limitation, we propose MeanFuser, an end-to-end autonomous driving method that enhances both efficiency and robustness through three key designs. (1) We introduce Gaussian Mixture Noise (GMN) to guide generative sampling, enabling a continuous representation of the trajectory space and eliminating the dependency on discrete anchor vocabularies. (2) We adapt ``MeanFlow Identity\" to end-to-end planning, which models the mean velocity field between GMN and trajectory distribution instead of the instantaneous velocity field used in vanilla flow matching methods, effectively eliminating numerical errors from ODE solvers and significantly accelerating inference. (3) We design a lightweight Adaptive Reconstruction Module (ARM) that enables the model to implicitly select from all sampled proposals or reconstruct a new trajectory when none is satisfactory via attention weights. Experiments on the NAVSIM closed-loop benchmark demonstrate that MeanFuser achieves outstanding performance without the supervision of the PDM Score. and exceptional inference efficiency, offering a robust and efficient solution for end-to-end autonomous driving. Our code and model are available at https://github.com/wjl2244/MeanFuser.",
    "summary": "",
    "translation": "MeanFuser：通过MeanFlow实现端到端自动驾驶的快速一步多模态轨迹生成与自适应重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶领域，涉及轨迹生成和传感器融合技术，与推荐系统、搜索或广告的核心领域完全无关。虽然提到了多模态处理，但这是针对自动驾驶传感器数据（如摄像头、雷达），而非推荐/搜索/广告中的异构数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20055v1": {
    "title": "To Move or Not to Move: Constraint-based Planning Enables Zero-Shot Generalization for Interactive Navigation",
    "url": "https://www.alphaxiv.org/abs/2602.20055v1",
    "arxiv_id": "2602.20055v1",
    "authors": "Apoorva Vashisth, Manav Kulshrestha, Pranav Bakshi, Damon Conover, Guillaume Sartoretti, Aniket Bera",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2026-02-23 17:10:00",
    "ori_summary": "Visual navigation typically assumes the existence of at least one obstacle-free path between start and goal, which must be discovered/planned by the robot. However, in real-world scenarios, such as home environments and warehouses, clutter can block all routes. Targeted at such cases, we introduce the Lifelong Interactive Navigation problem, where a mobile robot with manipulation abilities can move clutter to forge its own path to complete sequential object- placement tasks - each involving placing an given object (eg. Alarm clock, Pillow) onto a target object (eg. Dining table, Desk, Bed). To address this lifelong setting - where effects of environment changes accumulate and have long-term effects - we propose an LLM-driven, constraint-based planning framework with active perception. Our framework allows the LLM to reason over a structured scene graph of discovered objects and obstacles, deciding which object to move, where to place it, and where to look next to discover task-relevant information. This coupling of reasoning and active perception allows the agent to explore the regions expected to contribute to task completion rather than exhaustively mapping the environment. A standard motion planner then executes the corresponding navigate-pick-place, or detour sequence, ensuring reliable low-level control. Evaluated in physics-enabled ProcTHOR-10k simulator, our approach outperforms non-learning and learning-based baselines. We further demonstrate our approach qualitatively on real-world hardware.",
    "summary": "",
    "translation": "移动与否：基于约束的规划实现交互式导航的零样本泛化",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及机器人导航和规划，属于强化学习在具体任务中的应用，与推荐系统、搜索或广告的核心技术领域无直接关联。虽然提到了零样本泛化，但应用场景是交互式导航，没有明确指向推荐、搜索或广告中的序列决策或用户交互建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20053v1": {
    "title": "Decoupling Defense Strategies for Robust Image Watermarking",
    "url": "https://www.alphaxiv.org/abs/2602.20053v1",
    "arxiv_id": "2602.20053v1",
    "authors": "Jiahui Chen, Zehang Deng, Zeyu Zhang, Chaoyang Li, Lianchen Jia, Lifeng Sun",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 17:02:55",
    "ori_summary": "Deep learning-based image watermarking, while robust against conventional distortions, remains vulnerable to advanced adversarial and regeneration attacks. Conventional countermeasures, which jointly optimize the encoder and decoder via a noise layer, face 2 inevitable challenges: (1) decrease of clean accuracy due to decoder adversarial training and (2) limited robustness due to simultaneous training of all three advanced attacks. To overcome these issues, we propose AdvMark, a novel two-stage fine-tuning framework that decouples the defense strategies. In stage 1, we address adversarial vulnerability via a tailored adversarial training paradigm that primarily fine-tunes the encoder while only conditionally updating the decoder. This approach learns to move the image into a non-attackable region, rather than modifying the decision boundary, thus preserving clean accuracy. In stage 2, we tackle distortion and regeneration attacks via direct image optimization. To preserve the adversarial robustness gained in stage 1, we formulate a principled, constrained image loss with theoretical guarantees, which balances the deviation from cover and previous encoded images. We also propose a quality-aware early-stop to further guarantee the lower bound of visual quality. Extensive experiments demonstrate AdvMark outperforms with the highest image quality and comprehensive robustness, i.e. up to 29\\%, 33\\% and 46\\% accuracy improvement for distortion, regeneration and adversarial attacks, respectively.",
    "summary": "",
    "translation": "鲁棒图像水印的防御策略解耦",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及图像水印技术，属于数字版权保护领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然水印技术可能在某些内容审核场景中有应用，但论文标题明确聚焦于图像水印的防御策略，这属于安全/隐私范畴，已被列为无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20051v1": {
    "title": "SEAL-pose: Enhancing 3D Human Pose Estimation via a Learned Loss for Structural Consistency",
    "url": "https://www.alphaxiv.org/abs/2602.20051v1",
    "arxiv_id": "2602.20051v1",
    "authors": "Yeonsung Kim, Junggeun Do, Seunguk Do, Sangmin Kim, Jaesik Park, Jay-Yoon Lee",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-23 17:00:35",
    "ori_summary": "3D human pose estimation (HPE) is characterized by intricate local and global dependencies among joints. Conventional supervised losses are limited in capturing these correlations because they treat each joint independently. Previous studies have attempted to promote structural consistency through manually designed priors or rule-based constraints; however, these approaches typically require manual specification and are often non-differentiable, limiting their use as end-to-end training objectives. We propose SEAL-pose, a data-driven framework in which a learnable loss-net trains a pose-net by evaluating structural plausibility. Rather than relying on hand-crafted priors, our joint-graph-based design enables the loss-net to learn complex structural dependencies directly from data. Extensive experiments on three 3D HPE benchmarks with eight backbones show that SEAL-pose reduces per-joint errors and improves pose plausibility compared with the corresponding backbones across all settings. Beyond improving each backbone, SEAL-pose also outperforms models with explicit structural constraints, despite not enforcing any such constraints. Finally, we analyze the relationship between the loss-net and structural consistency, and evaluate SEAL-pose in cross-dataset and in-the-wild settings.",
    "summary": "",
    "translation": "SEAL-pose：通过结构一致性学习损失增强3D人体姿态估计",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的3D人体姿态估计，属于纯粹的视觉任务，与推荐系统、搜索或广告领域没有直接关联。论文内容涉及姿态估计的损失函数改进，属于视觉技术范畴，不符合当前关注的任何技术方向（核心推荐系统进展、LLM技术、Transformer架构改进、LLM直接应用或异构数据统一建模）。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20046v1": {
    "title": "Closing the gap in multimodal medical representation alignment",
    "url": "https://www.alphaxiv.org/abs/2602.20046v1",
    "arxiv_id": "2602.20046v1",
    "authors": "Eleonora Grassucci, Giordano Cicchetti, Danilo Comminiello",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-02-23 16:57:39",
    "ori_summary": "In multimodal learning, CLIP has emerged as the de-facto approach for mapping different modalities into a shared latent space by bringing semantically similar representations closer while pushing apart dissimilar ones. However, CLIP-based contrastive losses exhibit unintended behaviors that negatively impact true semantic alignment, leading to sparse and fragmented latent spaces. This phenomenon, known as the modality gap, has been partially mitigated for standard text and image pairs but remains unknown and unresolved in more complex multimodal settings, such as the medical domain. In this work, we study this phenomenon in the latter case, revealing that the modality gap is present also in medical alignment, and we propose a modality-agnostic framework that closes this gap, ensuring that semantically related representations are more aligned, regardless of their source modality. Our method enhances alignment between radiology images and clinical text, improving cross-modal retrieval and image captioning.",
    "summary": "",
    "translation": "弥合多模态医学表征对齐的差距",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医学领域的多模态表征对齐，属于明确的医学领域应用。根据用户指定的无关主题，医学、生物学、化学、物理学或其他领域特定应用均被排除在外，因此该论文与用户当前关注的推荐系统、搜索、广告技术及相关的LLM/Transformer核心技术进展无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20041v1": {
    "title": "EEG-Driven Intention Decoding: Offline Deep Learning Benchmarking on a Robotic Rover",
    "url": "https://www.alphaxiv.org/abs/2602.20041v1",
    "arxiv_id": "2602.20041v1",
    "authors": "Ghadah Alosaimi, Maha Alsayyari, Yixin Sun, Stamos Katsigiannis, Amir Atapour-Abarghouei, Toby P. Breckon",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2026-02-23 16:50:21",
    "ori_summary": "Brain-computer interfaces (BCIs) provide a hands-free control modality for mobile robotics, yet decoding user intent during real-world navigation remains challenging. This work presents a brain-robot control framework for offline decoding of driving commands during robotic rover operation. A 4WD Rover Pro platform was remotely operated by 12 participants who navigated a predefined route using a joystick, executing the commands forward, reverse, left, right, and stop. Electroencephalogram (EEG) signals were recorded with a 16-channel OpenBCI cap and aligned with motor actions at Delta = 0 ms and future prediction horizons (Delta > 0 ms). After preprocessing, several deep learning models were benchmarked, including convolutional neural networks, recurrent neural networks, and Transformer architectures. ShallowConvNet achieved the highest performance for both action prediction and intent prediction. By combining real-world robotic control with multi-horizon EEG intention decoding, this study introduces a reproducible benchmark and reveals key design insights for predictive deep learning-based BCI systems.",
    "summary": "",
    "translation": "脑电图驱动的意图解码：基于机器人漫游车的离线深度学习基准测试",
    "relevance_score": 1,
    "reasoning": "该论文专注于脑电图（EEG）信号处理和机器人控制，属于生物医学工程和机器人学领域。虽然涉及深度学习技术，但其核心应用（脑机接口、机器人控制）与推荐系统、搜索或广告领域没有直接关联，也不符合任何指定的关注点（如核心推荐系统进展、Transformer架构改进、或LLM在推荐/搜索中的应用）。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.20008v1": {
    "title": "Token-UNet: A New Case for Transformers Integration in Efficient and Interpretable 3D UNets for Brain Imaging Segmentation",
    "url": "https://www.alphaxiv.org/abs/2602.20008v1",
    "arxiv_id": "2602.20008v1",
    "authors": "Louis Fabrice Tshimanga, Andrea Zanola, Federico Del Pup, Manfredo Atzori",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 16:15:38",
    "ori_summary": "We present Token-UNet, adopting the TokenLearner and TokenFuser modules to encase Transformers into UNets. While Transformers have enabled global interactions among input elements in medical imaging, current computational challenges hinder their deployment on common hardware. Models like (Swin)UNETR adapt the UNet architecture by incorporating (Swin)Transformer encoders, which process tokens that each represent small subvolumes ($8^3$ voxels) of the input. The Transformer attention mechanism scales quadratically with the number of tokens, which is tied to the cubic scaling of 3D input resolution. This work reconsiders the role of convolution and attention, introducing Token-UNets, a family of 3D segmentation models that can operate in constrained computational environments and time frames. To mitigate computational demands, our approach maintains the convolutional encoder of UNet-like models, and applies TokenLearner to 3D feature maps. This module pools a preset number of tokens from local and global structures. Our results show this tokenization effectively encodes task-relevant information, yielding naturally interpretable attention maps. The memory footprint, computation times at inference, and parameter counts of our heaviest model are reduced to 33\\%, 10\\%, and 35\\% of the SwinUNETR values, with better average performance (86.75\\% $\\pm 0.19\\%$ Dice score for SwinUNETR vs our 87.21\\% $\\pm 0.35\\%$). This work opens the way to more efficient trainings in contexts with limited computational resources, such as 3D medical imaging. Easing model optimization, fine-tuning, and transfer-learning in limited hardware settings can accelerate and diversify the development of approaches, for the benefit of the research community.",
    "summary": "",
    "translation": "Token-UNet：一种用于脑成像分割的高效可解释3D UNet中Transformer集成的新案例",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于3D脑成像分割的医学应用，这属于明确的无关主题（医学/生物学领域特定应用）。虽然提到了Transformer架构，但上下文完全局限于医学影像处理，没有提及任何与推荐系统、搜索或广告相关的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19994v1": {
    "title": "RADE-Net: Robust Attention Network for Radar-Only Object Detection in Adverse Weather",
    "url": "https://www.alphaxiv.org/abs/2602.19994v1",
    "arxiv_id": "2602.19994v1",
    "authors": "Christof Leitgeb, Thomas Puchleitner, Max Peter Ronecker, Daniel Watzenig",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 16:01:31",
    "ori_summary": "Automotive perception systems are obligated to meet high requirements. While optical sensors such as Camera and Lidar struggle in adverse weather conditions, Radar provides a more robust perception performance, effectively penetrating fog, rain, and snow. Since full Radar tensors have large data sizes and very few datasets provide them, most Radar-based approaches work with sparse point clouds or 2D projections, which can result in information loss. Additionally, deep learning methods show potential to extract richer and more dense features from low level Radar data and therefore significantly increase the perception performance. Therefore, we propose a 3D projection method for fast-Fourier-transformed 4D Range-Azimuth-Doppler-Elevation (RADE) tensors. Our method preserves rich Doppler and Elevation features while reducing the required data size for a single frame by 91.9% compared to a full tensor, thus achieving higher training and inference speed as well as lower model complexity. We introduce RADE-Net, a lightweight model tailored to 3D projections of the RADE tensor. The backbone enables exploitation of low-level and high-level cues of Radar tensors with spatial and channel-attention. The decoupled detection heads predict object center-points directly in the Range-Azimuth domain and regress rotated 3D bounding boxes from rich feature maps in the cartesian scene. We evaluate the model on scenes with multiple different road users and under various weather conditions on the large-scale K-Radar dataset and achieve a 16.7% improvement compared to their baseline, as well as 6.5% improvement over current Radar-only models. Additionally, we outperform several Lidar approaches in scenarios with adverse weather conditions. The code is available under https://github.com/chr-is-tof/RADE-Net.",
    "summary": "",
    "translation": "RADE-Net：恶劣天气下仅使用雷达的鲁棒注意力目标检测网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于雷达目标检测的计算机视觉任务，属于纯粹的视觉/传感器领域研究。虽然提到了注意力机制，但这是针对特定传感器（雷达）在特定场景（恶劣天气）的应用，与推荐系统、搜索或广告的核心技术或LLM应用没有直接关联。该研究没有展示出在异构数据处理、Transformer架构改进或LLM应用方面的潜在价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19974v1": {
    "title": "RL-RIG: A Generative Spatial Reasoner via Intrinsic Reflection",
    "url": "https://www.alphaxiv.org/abs/2602.19974v1",
    "arxiv_id": "2602.19974v1",
    "authors": "Tianyu Wang, Zhiyuan Ma, Qian Wang, Xinyi Zhang, Xinwei Long, Bowen Zhou",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 15:39:53",
    "ori_summary": "Recent advancements in image generation have achieved impressive results in producing high-quality images. However, existing image generation models still generally struggle with a spatial reasoning dilemma, lacking the ability to accurately capture fine-grained spatial relationships from the prompt and correctly generate scenes with structural integrity. To mitigate this dilemma, we propose RL-RIG, a Reinforcement Learning framework for Reflection-based Image Generation. Our architecture comprises four primary components: Diffuser, Checker, Actor, and Inverse Diffuser, following a Generate-Reflect-Edit paradigm to spark the Chain of Thought reasoning ability in image generation for addressing the dilemma. To equip the model with better intuition over generation trajectories, we further develop Reflection-GRPO to train the VLM Actor for edit prompts and the Image Editor for better image quality under a given prompt, respectively. Unlike traditional approaches that solely produce visually stunning yet structurally unreasonable content, our evaluation metrics prioritize spatial accuracy, utilizing Scene Graph IoU and employing a VLM-as-a-Judge strategy to assess the spatial consistency of generated images on LAION-SG dataset. Experimental results show that RL-RIG outperforms existing state-of-the-art open-source models by up to 11% in terms of controllable and precise spatial reasoning in image generation.",
    "summary": "",
    "translation": "RL-RIG：一种通过内在反思的生成式空间推理器",
    "relevance_score": 1,
    "reasoning": "该论文标题明确提及强化学习（RL），属于明确排除的无关主题。标题中的“空间推理器”暗示可能涉及视觉或空间理解任务，这些领域如果没有明确的推荐系统/搜索/广告应用，也属于排除范围。没有证据表明该技术有直接应用于推荐系统、搜索或广告的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19946v1": {
    "title": "When Pretty Isn't Useful: Investigating Why Modern Text-to-Image Models Fail as Reliable Training Data Generators",
    "url": "https://www.alphaxiv.org/abs/2602.19946v1",
    "arxiv_id": "2602.19946v1",
    "authors": "Krzysztof Adamkiewicz, Brian Moser, Stanislav Frolov, Tobias Christian Nauen, Federico Raue, Andreas Dengel",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-23 15:15:53",
    "ori_summary": "Recent text-to-image (T2I) diffusion models produce visually stunning images and demonstrate excellent prompt following. But do they perform well as synthetic vision data generators? In this work, we revisit the promise of synthetic data as a scalable substitute for real training sets and uncover a surprising performance regression. We generate large-scale synthetic datasets using state-of-the-art T2I models released between 2022 and 2025, train standard classifiers solely on this synthetic data, and evaluate them on real test data. Despite observable advances in visual fidelity and prompt adherence, classification accuracy on real test data consistently declines with newer T2I models as training data generators. Our analysis reveals a hidden trend: These models collapse to a narrow, aesthetic-centric distribution that undermines diversity and label-image alignment. Overall, our findings challenge a growing assumption in vision research, namely that progress in generative realism implies progress in data realism. We thus highlight an urgent need to rethink the capabilities of modern T2I models as reliable training data generators.",
    "summary": "",
    "translation": "当美观无用时：探究现代文生图模型为何无法作为可靠的训练数据生成器",
    "relevance_score": 2,
    "reasoning": "该论文主要研究文本到图像生成模型的可靠性问题，属于AIGC和内容生成领域。虽然可能涉及数据生成质量评估，但论文焦点是图像生成模型的局限性，与推荐系统、搜索或广告的核心技术进展没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19944v1": {
    "title": "Discover, Segment, and Select: A Progressive Mechanism for Zero-shot Camouflaged Object Segmentation",
    "url": "https://www.alphaxiv.org/abs/2602.19944v1",
    "arxiv_id": "2602.19944v1",
    "authors": "Yilong Yang, Jianxin Tian, Shengchuan Zhang, Liujuan Cao",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 15:15:37",
    "ori_summary": "Current zero-shot Camouflaged Object Segmentation methods typically employ a two-stage pipeline (discover-then-segment): using MLLMs to obtain visual prompts, followed by SAM segmentation. However, relying solely on MLLMs for camouflaged object discovery often leads to inaccurate localization, false positives, and missed detections. To address these issues, we propose the \\textbf{D}iscover-\\textbf{S}egment-\\textbf{S}elect (\\textbf{DSS}) mechanism, a progressive framework designed to refine segmentation step by step. The proposed method contains a Feature-coherent Object Discovery (FOD) module that leverages visual features to generate diverse object proposals, a segmentation module that refines these proposals through SAM segmentation, and a Semantic-driven Mask Selection (SMS) module that employs MLLMs to evaluate and select the optimal segmentation mask from multiple candidates. Without requiring any training or supervision, DSS achieves state-of-the-art performance on multiple COS benchmarks, especially in multiple-instance scenes.",
    "summary": "",
    "translation": "发现、分割与选择：一种用于零样本伪装目标分割的渐进机制",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及计算机视觉中的伪装目标分割，属于纯视觉任务，与推荐系统、搜索或广告的核心领域进展、LLM技术应用或Transformer架构改进没有直接关联。虽然分割技术可能在处理图像内容时有用，但标题未表明其在异构数据处理（如VLM类比）或RecSys/Search/Ads中的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19937v1": {
    "title": "Learning Positive-Incentive Point Sampling in Neural Implicit Fields for Object Pose Estimation",
    "url": "https://www.alphaxiv.org/abs/2602.19937v1",
    "arxiv_id": "2602.19937v1",
    "authors": "Yifei Shi, Boyan Wan, Xin Xu, Kai Xu",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 15:10:00",
    "ori_summary": "Learning neural implicit fields of 3D shapes is a rapidly emerging field that enables shape representation at arbitrary resolutions. Due to the flexibility, neural implicit fields have succeeded in many research areas, including shape reconstruction, novel view image synthesis, and more recently, object pose estimation. Neural implicit fields enable learning dense correspondences between the camera space and the object's canonical space-including unobserved regions in camera space-significantly boosting object pose estimation performance in challenging scenarios like highly occluded objects and novel shapes. Despite progress, predicting canonical coordinates for unobserved camera-space regions remains challenging due to the lack of direct observational signals. This necessitates heavy reliance on the model's generalization ability, resulting in high uncertainty. Consequently, densely sampling points across the entire camera space may yield inaccurate estimations that hinder the learning process and compromise performance. To alleviate this problem, we propose a method combining an SO(3)-equivariant convolutional implicit network and a positive-incentive point sampling (PIPS) strategy. The SO(3)-equivariant convolutional implicit network estimates point-level attributes with SO(3)-equivariance at arbitrary query locations, demonstrating superior performance compared to most existing baselines. The PIPS strategy dynamically determines sampling locations based on the input, thereby boosting the network's accuracy and training efficiency. Our method outperforms the state-of-the-art on three pose estimation datasets. Notably, it demonstrates significant improvements in challenging scenarios, such as objects captured with unseen pose, high occlusion, novel geometry, and severe noise.",
    "summary": "",
    "translation": "神经隐式场中学习正激励点采样用于物体姿态估计",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的物体姿态估计问题，属于3D视觉领域。虽然涉及神经隐式表示和采样方法，但这些技术主要针对视觉感知任务，没有明确展示在推荐系统、搜索或广告中的潜在应用。论文内容更接近纯粹的视觉研究，与您关注的LLM技术、推荐系统架构或异构数据统一建模等方向相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19931v1": {
    "title": "Expanding the Role of Diffusion Models for Robust Classifier Training",
    "url": "https://www.alphaxiv.org/abs/2602.19931v1",
    "arxiv_id": "2602.19931v1",
    "authors": "Pin-Han Huang, Shang-Tse Chen, Hsuan-Tien Lin",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2026-02-23 15:06:52",
    "ori_summary": "Incorporating diffusion-generated synthetic data into adversarial training (AT) has been shown to substantially improve the training of robust image classifiers. In this work, we extend the role of diffusion models beyond merely generating synthetic data, examining whether their internal representations, which encode meaningful features of the data, can provide additional benefits for robust classifier training. Through systematic experiments, we show that diffusion models offer representations that are both diverse and partially robust, and that explicitly incorporating diffusion representations as an auxiliary learning signal during AT consistently improves robustness across settings. Furthermore, our representation analysis indicates that incorporating diffusion models into AT encourages more disentangled features, while diffusion representations and diffusion-generated synthetic data play complementary roles in shaping representations. Experiments on CIFAR-10, CIFAR-100, and ImageNet validate these findings, demonstrating the effectiveness of jointly leveraging diffusion representations and synthetic data within AT.",
    "summary": "",
    "translation": "扩展扩散模型在鲁棒分类器训练中的作用",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于扩散模型在分类器训练中的应用，属于生成模型领域，与推荐系统、搜索或广告的核心技术无直接关联。扩散模型虽然属于生成式AI范畴，但论文讨论的是分类器训练而非推荐/搜索/广告的特定应用，且未提及任何潜在的跨领域应用可能性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19916v1": {
    "title": "Augmented Radiance Field: A General Framework for Enhanced Gaussian Splatting",
    "url": "https://www.alphaxiv.org/abs/2602.19916v1",
    "arxiv_id": "2602.19916v1",
    "authors": "Yixin Yang, Bojian Wu, Yang Zhou, Hui Huang",
    "categories": "cs.CV, cs.GR",
    "pub_date": "2026-02-23 14:55:31",
    "ori_summary": "Due to the real-time rendering performance, 3D Gaussian Splatting (3DGS) has emerged as the leading method for radiance field reconstruction. However, its reliance on spherical harmonics for color encoding inherently limits its ability to separate diffuse and specular components, making it challenging to accurately represent complex reflections. To address this, we propose a novel enhanced Gaussian kernel that explicitly models specular effects through view-dependent opacity. Meanwhile, we introduce an error-driven compensation strategy to improve rendering quality in existing 3DGS scenes. Our method begins with 2D Gaussian initialization and then adaptively inserts and optimizes enhanced Gaussian kernels, ultimately producing an augmented radiance field. Experiments demonstrate that our method not only surpasses state-of-the-art NeRF methods in rendering performance but also achieves greater parameter efficiency. Project page at: https://xiaoxinyyx.github.io/augs.",
    "summary": "",
    "translation": "增强辐射场：一种用于增强高斯泼溅的通用框架",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及计算机视觉中的3D重建和渲染技术（高斯泼溅和辐射场），属于纯粹的视觉领域研究。虽然标题提到“增强”和“通用框架”，但没有表明与推荐系统、搜索或广告有任何直接或间接的应用潜力。该技术主要面向3D视觉和图形学，属于明确排除的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19910v1": {
    "title": "Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery",
    "url": "https://www.alphaxiv.org/abs/2602.19910v1",
    "arxiv_id": "2602.19910v1",
    "authors": "Wei He, Xianghan Meng, Zhiyuan Huang, Xianbiao Qi, Rong Xiao, Chun-Guang Li",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 14:51:09",
    "ori_summary": "Generalized Category Discovery (GCD) aims to identify both known and unknown categories, with only partial labels given for the known categories, posing a challenging open-set recognition problem. State-of-the-art approaches for GCD task are usually built on multi-modality representation learning, which is heavily dependent upon inter-modality alignment. However, few of them cast a proper intra-modality alignment to generate a desired underlying structure of representation distributions. In this paper, we propose a novel and effective multi-modal representation learning framework for GCD via Semi-Supervised Rate Reduction, called SSR$^2$-GCD, to learn cross-modality representations with desired structural properties based on emphasizing to properly align intra-modality relationships. Moreover, to boost knowledge transfer, we integrate prompt candidates by leveraging the inter-modal alignment offered by Vision Language Models. We conduct extensive experiments on generic and fine-grained benchmark datasets demonstrating superior performance of our approach.",
    "summary": "",
    "translation": "基于半监督率降低的广义类别发现多模态表示学习",
    "relevance_score": 3,
    "reasoning": "该论文涉及多模态表示学习，可能类比于VLM处理异构数据作为不同模态的统一建模，这与'VLM Analogy for Heterogeneous Data'焦点有潜在关联。然而，标题明确强调'Generalized Category Discovery'，这更偏向于计算机视觉或通用机器学习任务，而非直接针对推荐系统、搜索或广告的特定应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19907v1": {
    "title": "Gradient based Severity Labeling for Biomarker Classification in OCT",
    "url": "https://www.alphaxiv.org/abs/2602.19907v1",
    "arxiv_id": "2602.19907v1",
    "authors": "Kiran Kokilepersaud, Mohit Prabhushankar, Ghassan AlRegib, Stephanie Trejo Corona, Charles Wykoff",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-02-23 14:46:08",
    "ori_summary": "In this paper, we propose a novel selection strategy for contrastive learning for medical images. On natural images, contrastive learning uses augmentations to select positive and negative pairs for the contrastive loss. However, in the medical domain, arbitrary augmentations have the potential to distort small localized regions that contain the biomarkers we are interested in detecting. A more intuitive approach is to select samples with similar disease severity characteristics, since these samples are more likely to have similar structures related to the progression of a disease. To enable this, we introduce a method that generates disease severity labels for unlabeled OCT scans on the basis of gradient responses from an anomaly detection algorithm. These labels are used to train a supervised contrastive learning setup to improve biomarker classification accuracy by as much as 6% above self-supervised baselines for key indicators of Diabetic Retinopathy.",
    "summary": "",
    "translation": "基于梯度的OCT生物标志物分类严重程度标注",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及医学领域（OCT-光学相干断层扫描）和生物标志物分类，这属于明确的无关主题。虽然提到了梯度方法，但核心应用完全在医疗诊断领域，与推荐系统、搜索或广告没有任何关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19900v1": {
    "title": "ExpPortrait: Expressive Portrait Generation via Personalized Representation",
    "url": "https://www.alphaxiv.org/abs/2602.19900v1",
    "arxiv_id": "2602.19900v1",
    "authors": "Junyi Wang, Yudong Guo, Boyang Guo, Shengming Yang, Juyong Zhang",
    "categories": "cs.CV, cs.GR",
    "pub_date": "2026-02-23 14:41:35",
    "ori_summary": "While diffusion models have shown great potential in portrait generation, generating expressive, coherent, and controllable cinematic portrait videos remains a significant challenge. Existing intermediate signals for portrait generation, such as 2D landmarks and parametric models, have limited disentanglement capabilities and cannot express personalized details due to their sparse or low-rank representation. Therefore, existing methods based on these models struggle to accurately preserve subject identity and expressions, hindering the generation of highly expressive portrait videos. To overcome these limitations, we propose a high-fidelity personalized head representation that more effectively disentangles expression and identity. This representation captures both static, subject-specific global geometry and dynamic, expression-related details. Furthermore, we introduce an expression transfer module to achieve personalized transfer of head pose and expression details between different identities. We use this sophisticated and highly expressive head model as a conditional signal to train a diffusion transformer (DiT)-based generator to synthesize richly detailed portrait videos. Extensive experiments on self- and cross-reenactment tasks demonstrate that our method outperforms previous models in terms of identity preservation, expression accuracy, and temporal stability, particularly in capturing fine-grained details of complex motion.",
    "summary": "",
    "translation": "ExpPortrait：通过个性化表征实现富有表现力的肖像生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于肖像生成，属于纯粹的视觉内容生成领域，与推荐系统、搜索或广告的排名核心任务无关。虽然标题中提及“个性化表征”，但这仅指肖像生成的个性化风格，而非推荐/搜索/广告领域所需的用户个性化建模。该论文不涉及任何推荐、搜索或广告的排名、检索或建模技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19896v1": {
    "title": "Monocular Mesh Recovery and Body Measurement of Female Saanen Goats",
    "url": "https://www.alphaxiv.org/abs/2602.19896v1",
    "arxiv_id": "2602.19896v1",
    "authors": "Bo Jin, Shichao Zhao, Jin Lyu, Bin Zhang, Tao Yu, Liang An, Yebin Liu, Meili Wang",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 14:37:07",
    "ori_summary": "The lactation performance of Saanen dairy goats, renowned for their high milk yield, is intrinsically linked to their body size, making accurate 3D body measurement essential for assessing milk production potential, yet existing reconstruction methods lack goat-specific authentic 3D data. To address this limitation, we establish the FemaleSaanenGoat dataset containing synchronized eight-view RGBD videos of 55 female Saanen goats (6-18 months). Using multi-view DynamicFusion, we fuse noisy, non-rigid point cloud sequences into high-fidelity 3D scans, overcoming challenges from irregular surfaces and rapid movement. Based on these scans, we develop SaanenGoat, a parametric 3D shape model specifically designed for female Saanen goats. This model features a refined template with 41 skeletal joints and enhanced udder representation, registered with our scan data. A comprehensive shape space constructed from 48 goats enables precise representation of diverse individual variations. With the help of SaanenGoat model, we get high-precision 3D reconstruction from single-view RGBD input, and achieve automated measurement of six critical body dimensions: body length, height, chest width, chest girth, hip width, and hip height. Experimental results demonstrate the superior accuracy of our method in both 3D reconstruction and body measurement, presenting a novel paradigm for large-scale 3D vision applications in precision livestock farming.",
    "summary": "",
    "translation": "单目相机下的雌性萨能山羊三维网格重建与体尺测量",
    "relevance_score": 1,
    "reasoning": "该论文专注于农业/动物科学领域的特定计算机视觉应用（山羊三维重建与测量），属于明确的领域特定应用。标题中没有任何内容表明与推荐系统、搜索、广告、LLM技术、Transformer架构或异构数据统一建模相关，完全符合列出的多个无关主题（如医学/生物学等特定领域应用、纯视觉论文）。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19891v1": {
    "title": "Using Unsupervised Domain Adaptation Semantic Segmentation for Pulmonary Embolism Detection in Computed Tomography Pulmonary Angiogram (CTPA) Images",
    "url": "https://www.alphaxiv.org/abs/2602.19891v1",
    "arxiv_id": "2602.19891v1",
    "authors": "Wen-Liang Lin, Yun-Chien Cheng",
    "categories": "eess.IV, cs.CV",
    "pub_date": "2026-02-23 14:33:24",
    "ori_summary": "While deep learning has demonstrated considerable promise in computer-aided diagnosis for pulmonary embolism (PE), practical deployment in Computed Tomography Pulmonary Angiography (CTPA) is often hindered by \"domain shift\" and the prohibitive cost of expert annotations. To address these challenges, an unsupervised domain adaptation (UDA) framework is proposed, utilizing a Transformer backbone and a Mean-Teacher architecture for cross-center semantic segmentation. The primary focus is placed on enhancing pseudo-label reliability by learning deep structural information within the feature space. Specifically, three modules are integrated and designed for this task: (1) a Prototype Alignment (PA) mechanism to reduce category-level distribution discrepancies; (2) Global and Local Contrastive Learning (GLCL) to capture both pixel-level topological relationships and global semantic representations; and (3) an Attention-based Auxiliary Local Prediction (AALP) module designed to reinforce sensitivity to small PE lesions by automatically extracting high-information slices from Transformer attention maps. Experimental validation conducted on cross-center datasets (FUMPE and CAD-PE) demonstrates significant performance gains. In the FUMPE -> CAD-PE task, the IoU increased from 0.1152 to 0.4153, while the CAD-PE -> FUMPE task saw an improvement from 0.1705 to 0.4302. Furthermore, the proposed method achieved a 69.9% Dice score in the CT -> MRI cross-modality task on the MMWHS dataset without utilizing any target-domain labels for model selection, confirming its robustness and generalizability for diverse clinical environments.",
    "summary": "",
    "translation": "基于无监督域自适应语义分割的计算机断层扫描肺血管造影图像肺栓塞检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像领域的肺栓塞检测，属于明确的医学/生物医学应用范畴，与搜索、推荐、广告等商业系统完全无关。论文涉及计算机视觉技术（语义分割），但应用场景严格限定在医疗诊断，没有任何潜在的应用于RecSys/Search/Ads的路径或相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19881v1": {
    "title": "Make Some Noise: Unsupervised Remote Sensing Change Detection Using Latent Space Perturbations",
    "url": "https://www.alphaxiv.org/abs/2602.19881v1",
    "arxiv_id": "2602.19881v1",
    "authors": "Blaž Rolih, Matic Fučka, Filip Wolf, Luka Čehovin Zajc",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-23 14:27:36",
    "ori_summary": "Unsupervised change detection (UCD) in remote sensing aims to localise semantic changes between two images of the same region without relying on labelled data during training. Most recent approaches rely either on frozen foundation models in a training-free manner or on training with synthetic changes generated in pixel space. Both strategies inherently rely on predefined assumptions about change types, typically introduced through handcrafted rules, external datasets, or auxiliary generative models. Due to these assumptions, such methods fail to generalise beyond a few change types, limiting their real-world usage, especially in rare or complex scenarios. To address this, we propose MaSoN (Make Some Noise), an end-to-end UCD framework that synthesises diverse changes directly in the latent feature space during training. It generates changes that are dynamically estimated using feature statistics of target data, enabling diverse yet data-driven variation aligned with the target domain. It also easily extends to new modalities, such as SAR. MaSoN generalises strongly across diverse change types and achieves state-of-the-art performance on five benchmarks, improving the average F1 score by 14.1 percentage points. Project page: https://blaz-r.github.io/mason_ucd",
    "summary": "",
    "translation": "制造一些噪声：基于潜在空间扰动的无监督遥感变化检测",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向遥感变化检测，属于计算机视觉在特定领域（遥感）的应用。虽然涉及无监督学习和潜在空间技术，但缺乏与推荐系统、搜索或广告领域的直接或间接联系，完全属于被排除的“纯粹视觉论文”类别。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19874v1": {
    "title": "BigMaQ: A Big Macaque Motion and Animation Dataset Bridging Image and 3D Pose Representations",
    "url": "https://www.alphaxiv.org/abs/2602.19874v1",
    "arxiv_id": "2602.19874v1",
    "authors": "Lucas Martini, Alexander Lappe, Anna Bognár, Rufin Vogels, Martin A. Giese",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 14:21:15",
    "ori_summary": "The recognition of dynamic and social behavior in animals is fundamental for advancing ethology, ecology, medicine and neuroscience. Recent progress in deep learning has enabled automated behavior recognition from video, yet an accurate reconstruction of the three-dimensional (3D) pose and shape has not been integrated into this process. Especially for non-human primates, mesh-based tracking efforts lag behind those for other species, leaving pose descriptions restricted to sparse keypoints that are unable to fully capture the richness of action dynamics. To address this gap, we introduce the $\\textbf{Big Ma}$ca$\\textbf{Q}$ue 3D Motion and Animation Dataset ($\\texttt{BigMaQ}$), a large-scale dataset comprising more than 750 scenes of interacting rhesus macaques with detailed 3D pose descriptions. Extending previous surface-based animal tracking methods, we construct subject-specific textured avatars by adapting a high-quality macaque template mesh to individual monkeys. This allows us to provide pose descriptions that are more accurate than previous state-of-the-art surface-based animal tracking methods. From the original dataset, we derive BigMaQ500, an action recognition benchmark that links surface-based pose vectors to single frames across multiple individual monkeys. By pairing features extracted from established image and video encoders with and without our pose descriptors, we demonstrate substantial improvements in mean average precision (mAP) when pose information is included. With these contributions, $\\texttt{BigMaQ}$ establishes the first dataset that both integrates dynamic 3D pose-shape representations into the learning task of animal action recognition and provides a rich resource to advance the study of visual appearance, posture, and social interaction in non-human primates. The code and data are publicly available at https://martinivis.github.io/BigMaQ/ .",
    "summary": "",
    "translation": "BigMaQ：一个连接图像与3D姿态表示的大型猕猴运动与动画数据集",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于计算机视觉领域（猕猴运动数据集、3D姿态表示），属于纯粹的视觉/3D视觉研究，与推荐系统、搜索或广告的核心技术栈无直接关联。虽然3D姿态估计在动作识别等视觉任务中有应用，但论文标题未表明其在RecSys/Search/Ads领域的潜在应用价值，因此属于明确排除的无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19872v1": {
    "title": "GOAL: Geometrically Optimal Alignment for Continual Generalized Category Discovery",
    "url": "https://www.alphaxiv.org/abs/2602.19872v1",
    "arxiv_id": "2602.19872v1",
    "authors": "Jizhou Han, Chenhao Ding, SongLin Dong, Yuhang He, Shaokun Wang, Qiang Wang, Yihong Gong",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-23 14:15:56",
    "ori_summary": "Continual Generalized Category Discovery (C-GCD) requires identifying novel classes from unlabeled data while retaining knowledge of known classes over time. Existing methods typically update classifier weights dynamically, resulting in forgetting and inconsistent feature alignment. We propose GOAL, a unified framework that introduces a fixed Equiangular Tight Frame (ETF) classifier to impose a consistent geometric structure throughout learning. GOAL conducts supervised alignment for labeled samples and confidence-guided alignment for novel samples, enabling stable integration of new classes without disrupting old ones. Experiments on four benchmarks show that GOAL outperforms the prior method Happy, reducing forgetting by 16.1% and boosting novel class discovery by 3.2%, establishing a strong solution for long-horizon continual discovery.",
    "summary": "",
    "translation": "GOAL：持续广义类别发现的几何最优对齐",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及持续学习和类别发现，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术焦点没有直接关联。虽然持续学习技术可能间接应用于用户兴趣演化建模，但论文标题明确指向视觉类别发现，缺乏明确的跨模态或序列建模应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19870v1": {
    "title": "ApET: Approximation-Error Guided Token Compression for Efficient VLMs",
    "url": "https://www.alphaxiv.org/abs/2602.19870v1",
    "arxiv_id": "2602.19870v1",
    "authors": "Qiankun Ma, Ziyao Zhang, Haofei Wang, Jie Chen, Zhen Song, Hairong Zheng",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 14:15:37",
    "ori_summary": "Recent Vision-Language Models (VLMs) have demonstrated remarkable multimodal understanding capabilities, yet the redundant visual tokens incur prohibitive computational overhead and degrade inference efficiency. Prior studies typically relies on [CLS] attention or text-vision cross-attention to identify and discard redundant visual tokens. Despite promising results, such solutions are prone to introduce positional bias and, more critically, are incompatible with efficient attention kernels such as FlashAttention, limiting their practical deployment for VLM acceleration. In this paper, we step away from attention dependencies and revisit visual token compression from an information-theoretic perspective, aiming to maximally preserve visual information without any attention involvement. We present ApET, an Approximation-Error guided Token compression framework. ApET first reconstructs the original visual tokens with a small set of basis tokens via linear approximation, then leverages the approximation error to identify and drop the least informative tokens. Extensive experiments across multiple VLMs and benchmarks demonstrate that ApET retains 95.2% of the original performance on image-understanding tasks and even attains 100.4% on video-understanding tasks, while compressing the token budgets by 88.9% and 87.5%, respectively. Thanks to its attention-free design, ApET seamlessly integrates with FlashAttention, enabling further inference acceleration and making VLM deployment more practical. Code is available at https://github.com/MaQianKun0/ApET.",
    "summary": "该论文研究视觉语言模型中冗余视觉令牌导致的计算效率低下问题。其核心方法是基于信息论视角，通过线性近似重构原始令牌并利用近似误差来识别和丢弃信息量最少的令牌，实现无需注意力参与的令牌压缩。",
    "translation": "ApET：基于近似误差引导的令牌压缩技术，用于高效视觉语言模型",
    "relevance_score": 6,
    "reasoning": "该论文提出了一种用于视觉语言模型（VLM）的令牌压缩技术，属于'使能Transformer技术'类别，专注于效率优化。虽然直接针对VLM，但类似的令牌压缩方法可以应用于处理用户行为序列或上下文特征的推荐/搜索系统中，以减少计算开销并提升推理速度。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出了一种不依赖注意力机制的视觉令牌压缩方法，直接解决了VLM效率瓶颈，其核心思想与异构数据统一建模高度相关，对推荐和搜索系统中的多模态数据处理具有重要启发价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.19863v1": {
    "title": "Brewing Stronger Features: Dual-Teacher Distillation for Multispectral Earth Observation",
    "url": "https://www.alphaxiv.org/abs/2602.19863v1",
    "arxiv_id": "2602.19863v1",
    "authors": "Filip Wolf, Blaž Rolih, Luka Čehovin Zajc",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 14:09:01",
    "ori_summary": "Foundation models are transforming Earth Observation (EO), yet the diversity of EO sensors and modalities makes a single universal model unrealistic. Multiple specialized EO foundation models (EOFMs) will likely coexist, making efficient knowledge transfer across modalities essential. Most existing EO pretraining relies on masked image modeling, which emphasizes local reconstruction but provides limited control over global semantic structure. To address this, we propose a dual-teacher contrastive distillation framework for multispectral imagery that aligns the student's pretraining objective with the contrastive self-distillation paradigm of modern optical vision foundation models (VFMs). Our approach combines a multispectral teacher with an optical VFM teacher, enabling coherent cross-modal representation learning. Experiments across diverse optical and multispectral benchmarks show that our model adapts to multispectral data without compromising performance on optical-only inputs, achieving state-of-the-art results in both settings, with an average improvement of 3.64 percentage points in semantic segmentation, 1.2 in change detection, and 1.31 in classification tasks. This demonstrates that contrastive distillation provides a principled and efficient approach to scalable representation learning across heterogeneous EO data sources. Code: Coming soon.",
    "summary": "",
    "translation": "酿造更强特征：用于多光谱地球观测的双教师蒸馏",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于多光谱地球观测这一特定领域应用，属于遥感或地理信息科学范畴。虽然提到了特征蒸馏技术，但该技术应用于地球观测这一与推荐系统、搜索或广告无关的领域，且未提及任何与Transformer架构、LLM技术或异构数据处理相关的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19857v1": {
    "title": "Contrastive meta-domain adaptation for robust skin lesion classification across clinical and acquisition conditions",
    "url": "https://www.alphaxiv.org/abs/2602.19857v1",
    "arxiv_id": "2602.19857v1",
    "authors": "Rodrigo Mota, Kelvin Cunha, Emanoel dos Santos, Fábio Papais, Francisco Filho, Thales Bezerra, Erico Medeiros, Paulo Borba, Tsang Ing Ren",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 13:56:49",
    "ori_summary": "Deep learning models for dermatological image analysis remain sensitive to acquisition variability and domain-specific visual characteristics, leading to performance degradation when deployed in clinical settings. We investigate how visual artifacts and domain shifts affect deep learning-based skin lesion classification. We propose an adaptation strategy, grounded in the idea of visual meta-domains, that transfers visual representations from larger dermoscopic datasets into clinical image domains, thereby improving generalization robustness. Experiments across multiple dermatology datasets show consistent gains in classification performance and reduced gaps between dermoscopic and clinical images. These results emphasize the importance of domain-aware training for deployable systems.",
    "summary": "",
    "translation": "跨临床和采集条件的鲁棒性皮肤病变分类的对比元域适应",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及医学领域的皮肤病变分类，属于\"Irrelevant Topics\"中明确排除的\"Medical, Biology, Chemistry, Physics or other domain-specific applications\"类别。虽然提到了对比学习和域适应技术，但这些技术本身不足以建立与推荐系统、搜索或广告领域的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19848v1": {
    "title": "DerMAE: Improving skin lesion classification through conditioned latent diffusion and MAE distillation",
    "url": "https://www.alphaxiv.org/abs/2602.19848v1",
    "arxiv_id": "2602.19848v1",
    "authors": "Francisco Filho, Kelvin Cunha, Fábio Papais, Emanoel dos Santos, Rodrigo Mota, Thales Bezerra, Erico Medeiros, Paulo Borba, Tsang Ing Ren",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 13:52:28",
    "ori_summary": "Skin lesion classification datasets often suffer from severe class imbalance, with malignant cases significantly underrepresented, leading to biased decision boundaries during deep learning training. We address this challenge using class-conditioned diffusion models to generate synthetic dermatological images, followed by self-supervised MAE pretraining to enable huge ViT models to learn robust, domain-relevant features. To support deployment in practical clinical settings, where lightweight models are required, we apply knowledge distillation to transfer these representations to a smaller ViT student suitable for mobile devices. Our results show that MAE pretraining on synthetic data, combined with distillation, improves classification performance while enabling efficient on-device inference for practical clinical use.",
    "summary": "",
    "translation": "DerMAE：通过条件化潜在扩散与MAE蒸馏改进皮肤病变分类",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分析（皮肤病变分类），属于明确的医学领域应用，与用户关注的推荐系统、搜索、广告等核心领域无关。标题中提到的条件化潜在扩散和MAE蒸馏技术虽然涉及生成模型和自监督学习，但论文的应用场景（医疗诊断）完全在用户指定的无关主题范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19832v1": {
    "title": "M3S-Net: Multimodal Feature Fusion Network Based on Multi-scale Data for Ultra-short-term PV Power Forecasting",
    "url": "https://www.alphaxiv.org/abs/2602.19832v1",
    "arxiv_id": "2602.19832v1",
    "authors": "Penghui Niu, Taotao Cai, Suqi Zhang, Junhua Gu, Ping Zhang, Qiqi Liu, Jianxin Li",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 13:30:59",
    "ori_summary": "The inherent intermittency and high-frequency variability of solar irradiance, particularly during rapid cloud advection, present significant stability challenges to high-penetration photovoltaic grids. Although multimodal forecasting has emerged as a viable mitigation strategy, existing architectures predominantly rely on shallow feature concatenation and binary cloud segmentation, thereby failing to capture the fine-grained optical features of clouds and the complex spatiotemporal coupling between visual and meteorological modalities. To bridge this gap, this paper proposes M3S-Net, a novel multimodal feature fusion network based on multi-scale data for ultra-short-term PV power forecasting. First, a multi-scale partial channel selection network leverages partial convolutions to explicitly isolate the boundary features of optically thin clouds, effectively transcending the precision limitations of coarse-grained binary masking. Second, a multi-scale sequence to image analysis network employs Fast Fourier Transform (FFT)-based time-frequency representation to disentangle the complex periodicity of meteorological data across varying time horizons. Crucially, the model incorporates a cross-modal Mamba interaction module featuring a novel dynamic C-matrix swapping mechanism. By exchanging state-space parameters between visual and temporal streams, this design conditions the state evolution of one modality on the context of the other, enabling deep structural coupling with linear computational complexity, thus overcoming the limitations of shallow concatenation. Experimental validation on the newly constructed fine-grained PV power dataset demonstrates that M3S-Net achieves a mean absolute error reduction of 6.2% in 10-minute forecasts compared to state-of-the-art baselines. The dataset and source code will be available at https://github.com/she1110/FGPD.",
    "summary": "",
    "translation": "M3S-Net：基于多尺度数据的超短期光伏功率预测多模态特征融合网络",
    "relevance_score": 1,
    "reasoning": "该论文专注于光伏功率预测这一特定能源领域应用，涉及多模态特征融合和多尺度数据处理，但属于物理/能源领域的专业应用。论文标题未显示与推荐系统、搜索、广告、LLM技术或Transformer架构的任何关联，也不涉及异构数据统一建模的VLM类比思想。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19828v1": {
    "title": "TextShield-R1: Reinforced Reasoning for Tampered Text Detection",
    "url": "https://www.alphaxiv.org/abs/2602.19828v1",
    "arxiv_id": "2602.19828v1",
    "authors": "Chenfan Qu, Yiwu Zhong, Jian Liu, Xuekang Zhu, Bohan Yu, Lianwen Jin",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 13:26:18",
    "ori_summary": "The growing prevalence of tampered images poses serious security threats, highlighting the urgent need for reliable detection methods. Multimodal large language models (MLLMs) demonstrate strong potential in analyzing tampered images and generating interpretations. However, they still struggle with identifying micro-level artifacts, exhibit low accuracy in localizing tampered text regions, and heavily rely on expensive annotations for forgery interpretation. To this end, we introduce TextShield-R1, the first reinforcement learning based MLLM solution for tampered text detection and reasoning. Specifically, our approach introduces Forensic Continual Pre-training, an easy-to-hard curriculum that well prepares the MLLM for tampered text detection by harnessing the large-scale cheap data from natural image forensic and OCR tasks. During fine-tuning, we perform Group Relative Policy Optimization with novel reward functions to reduce annotation dependency and improve reasoning capabilities. At inference time, we enhance localization accuracy via OCR Rectification, a method that leverages the MLLM's strong text recognition abilities to refine its predictions. Furthermore, to support rigorous evaluation, we introduce the Text Forensics Reasoning (TFR) benchmark, comprising over 45k real and tampered images across 16 languages, 10 tampering techniques, and diverse domains. Rich reasoning-style annotations are included, allowing for comprehensive assessment. Our TFR benchmark simultaneously addresses seven major limitations of existing benchmarks and enables robust evaluation under cross-style, cross-method, and cross-language conditions. Extensive experiments demonstrate that TextShield-R1 significantly advances the state of the art in interpretable tampered text detection.",
    "summary": "",
    "translation": "TextShield-R1：基于强化推理的篡改文本检测",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及篡改文本检测，属于内容安全/完整性验证领域，与推荐系统、搜索或广告的核心技术（如排序、召回、用户建模）没有直接关联。虽然强化学习被提及，但论文重点在于文本篡改检测这一特定安全任务，而非在推荐/搜索/广告场景中应用强化学习进行决策优化或策略学习。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19823v1": {
    "title": "Open-vocabulary 3D scene perception in industrial environments",
    "url": "https://www.alphaxiv.org/abs/2602.19823v1",
    "arxiv_id": "2602.19823v1",
    "authors": "Keno Moenck, Adrian Philip Florea, Julian Koch, Thorsten Schüppstuhl",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 13:22:51",
    "ori_summary": "Autonomous vision applications in production, intralogistics, or manufacturing environments require perception capabilities beyond a small, fixed set of classes. Recent open-vocabulary methods, leveraging 2D Vision-Language Foundation Models (VLFMs), target this task but often rely on class-agnostic segmentation models pre-trained on non-industrial datasets (e.g., household scenes). In this work, we first demonstrate that such models fail to generalize, performing poorly on common industrial objects. Therefore, we propose a training-free, open-vocabulary 3D perception pipeline that overcomes this limitation. Instead of using a pre-trained model to generate instance proposals, our method simply generates masks by merging pre-computed superpoints based on their semantic features. Following, we evaluate the domain-adapted VLFM \"IndustrialCLIP\" on a representative 3D industrial workshop scene for open-vocabulary querying. Our qualitative results demonstrate successful segmentation of industrial objects.",
    "summary": "",
    "translation": "工业环境中的开放词汇3D场景感知",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及3D视觉和工业环境感知，属于明确的无关主题（Purely Vision、3D Vision或Domain-specific applications）。虽然开放词汇概念在LLM中有应用，但论文聚焦于3D场景感知而非推荐/搜索/广告系统，且未表明与异构数据建模或Transformer架构的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19822v1": {
    "title": "Efficient endometrial carcinoma screening via cross-modal synthesis and gradient distillation",
    "url": "https://www.alphaxiv.org/abs/2602.19822v1",
    "arxiv_id": "2602.19822v1",
    "authors": "Dongjing Shan, Yamei Luo, Jiqing Xuan, Lu Huang, Jin Li, Mengchu Yang, Zeyu Chen, Fajin Lv, Yong Tang, Chunxiang Zhang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-23 13:22:25",
    "ori_summary": "Early detection of myometrial invasion is critical for the staging and life-saving management of endometrial carcinoma (EC), a prevalent global malignancy. Transvaginal ultrasound serves as the primary, accessible screening modality in resource-constrained primary care settings; however, its diagnostic reliability is severely hindered by low tissue contrast, high operator dependence, and a pronounced scarcity of positive pathological samples. Existing artificial intelligence solutions struggle to overcome this severe class imbalance and the subtle imaging features of invasion, particularly under the strict computational limits of primary care clinics. Here we present an automated, highly efficient two-stage deep learning framework that resolves both data and computational bottlenecks in EC screening. To mitigate pathological data scarcity, we develop a structure-guided cross-modal generation network that synthesizes diverse, high-fidelity ultrasound images from unpaired magnetic resonance imaging (MRI) data, strictly preserving clinically essential anatomical junctions. Furthermore, we introduce a lightweight screening network utilizing gradient distillation, which transfers discriminative knowledge from a high-capacity teacher model to dynamically guide sparse attention towards task-critical regions. Evaluated on a large, multicenter cohort of 7,951 participants, our model achieves a sensitivity of 99.5\\%, a specificity of 97.2\\%, and an area under the curve of 0.987 at a minimal computational cost (0.289 GFLOPs), substantially outperforming the average diagnostic accuracy of expert sonographers. Our approach demonstrates that combining cross-modal synthetic augmentation with knowledge-driven efficient modeling can democratize expert-level, real-time cancer screening for resource-constrained primary care settings.",
    "summary": "",
    "translation": "通过跨模态合成与梯度蒸馏实现高效的子宫内膜癌筛查",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学领域的癌症筛查应用，属于明确的医学/生物医学领域，这在'不相关主题'中被明确排除。虽然提到了'跨模态合成'和'梯度蒸馏'等技术术语，但这些技术被应用于特定的医疗诊断任务，与推荐系统、搜索或广告领域没有直接或潜在的应用关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19768v1": {
    "title": "TraceVision: Trajectory-Aware Vision-Language Model for Human-Like Spatial Understanding",
    "url": "https://www.alphaxiv.org/abs/2602.19768v1",
    "arxiv_id": "2602.19768v1",
    "authors": "Fan Yang, Shurong Zheng, Hongyin Zhao, Yufei Zhan, Xin Li, Yousong Zhu, Chaoyang Zhao Ming Tang, Jinqiao Wang",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 12:18:26",
    "ori_summary": "Recent Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in image understanding and natural language generation. However, current approaches focus predominantly on global image understanding, struggling to simulate human visual attention trajectories and explain associations between descriptions and specific regions. We propose TraceVision, a unified vision-language model integrating trajectory-aware spatial understanding in an end-to-end framework. TraceVision employs a Trajectory-aware Visual Perception (TVP) module for bidirectional fusion of visual features and trajectory information. We design geometric simplification to extract semantic keypoints from raw trajectories and propose a three-stage training pipeline where trajectories guide description generation and region localization. We extend TraceVision to trajectory-guided segmentation and video scene understanding, enabling cross-frame tracking and temporal attention analysis. We construct the Reasoning-based Interactive Localized Narratives (RILN) dataset to enhance logical reasoning and interpretability. Extensive experiments on trajectory-guided captioning, text-guided trajectory prediction, understanding, and segmentation demonstrate that TraceVision achieves state-of-the-art performance, establishing a foundation for intuitive spatial interaction and interpretable visual understanding.",
    "summary": "",
    "translation": "TraceVision：面向类人空间理解的轨迹感知视觉语言模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉语言模型在空间理解任务中的应用，属于纯粹的视觉-语言交叉领域研究。虽然标题中提到了“轨迹感知”，但这更可能指视觉场景中的运动轨迹分析，而非推荐系统或搜索广告中常见的用户行为序列建模。该工作缺乏与推荐、搜索或广告领域明确的关联性，属于被排除的“纯粹视觉”或“纯粹LLM中心”类别。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19766v1": {
    "title": "One2Scene: Geometric Consistent Explorable 3D Scene Generation from a Single Image",
    "url": "https://www.alphaxiv.org/abs/2602.19766v1",
    "arxiv_id": "2602.19766v1",
    "authors": "Pengfei Wang, Liyi Chen, Zhiyuan Ma, Yanjun Guo, Guowen Zhang, Lei Zhang",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 12:15:54",
    "ori_summary": "Generating explorable 3D scenes from a single image is a highly challenging problem in 3D vision. Existing methods struggle to support free exploration, often producing severe geometric distortions and noisy artifacts when the viewpoint moves far from the original perspective. We introduce \\textbf{One2Scene}, an effective framework that decomposes this ill-posed problem into three tractable sub-tasks to enable immersive explorable scene generation. We first use a panorama generator to produce anchor views from a single input image as initialization. Then, we lift these 2D anchors into an explicit 3D geometric scaffold via a generalizable, feed-forward Gaussian Splatting network. Instead of treating the panorama as a single image for reconstruction, we project it into multiple sparse anchor views and reformulate the reconstruction task as multi-view stereo matching, which allows us to leverage robust geometric priors learned from large-scale multi-view datasets. A bidirectional feature fusion module is used to enforce cross-view consistency, yielding an efficient and geometrically reliable scaffold. Finally, the scaffold serves as a strong prior for a novel view generator to produce photorealistic and geometrically accurate views at arbitrary cameras. By explicitly conditioning on a 3D-consistent scaffold to perform reconstruction, One2Scene works stably under large camera motions, supporting immersive scene exploration. Extensive experiments show that One2Scene substantially outperforms state-of-the-art methods in panorama depth estimation, feed-forward 360° reconstruction, and explorable 3D scene generation. Code and models will be released.",
    "summary": "",
    "translation": "One2Scene：从单张图像生成几何一致的可探索三维场景",
    "relevance_score": 1,
    "reasoning": "该论文专注于从单张图像生成3D场景，属于计算机视觉和3D图形领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然3D场景生成技术可能在某些特定应用（如虚拟试衣、AR购物）中作为辅助工具，但这不属于当前关注的推荐系统/搜索/广告的核心算法、架构或直接应用范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19763v1": {
    "title": "Training Deep Stereo Matching Networks on Tree Branch Imagery: A Benchmark Study for Real-Time UAV Forestry Applications",
    "url": "https://www.alphaxiv.org/abs/2602.19763v1",
    "arxiv_id": "2602.19763v1",
    "authors": "Yida Lin, Bing Xue, Mengjie Zhang, Sam Schofield, Richard Green",
    "categories": "cs.CV, eess.IV",
    "pub_date": "2026-02-23 12:12:43",
    "ori_summary": "Autonomous drone-based tree pruning needs accurate, real-time depth estimation from stereo cameras. Depth is computed from disparity maps using $Z = f B/d$, so even small disparity errors cause noticeable depth mistakes at working distances. Building on our earlier work that identified DEFOM-Stereo as the best reference disparity generator for vegetation scenes, we present the first study to train and test ten deep stereo matching networks on real tree branch images. We use the Canterbury Tree Branches dataset -- 5,313 stereo pairs from a ZED Mini camera at 1080P and 720P -- with DEFOM-generated disparity maps as training targets. The ten methods cover step-by-step refinement, 3D convolution, edge-aware attention, and lightweight designs. Using perceptual metrics (SSIM, LPIPS, ViTScore) and structural metrics (SIFT/ORB feature matching), we find that BANet-3D produces the best overall quality (SSIM = 0.883, LPIPS = 0.157), while RAFT-Stereo scores highest on scene-level understanding (ViTScore = 0.799). Testing on an NVIDIA Jetson Orin Super (16 GB, independently powered) mounted on our drone shows that AnyNet reaches 6.99 FPS at 1080P -- the only near-real-time option -- while BANet-2D gives the best quality-speed balance at 1.21 FPS. We also compare 720P and 1080P processing times to guide resolution choices for forestry drone systems.",
    "summary": "",
    "translation": "基于树冠图像训练深度立体匹配网络：面向实时无人机林业应用的基准研究",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的立体匹配技术，应用于无人机林业场景，属于纯粹的视觉应用领域。论文内容与推荐系统、搜索、广告或相关使能技术（如Transformer架构、LLM应用）无直接关联，也不涉及异构数据统一建模的概念。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19756v1": {
    "title": "Multimodal Dataset Distillation Made Simple by Prototype-Guided Data Synthesis",
    "url": "https://www.alphaxiv.org/abs/2602.19756v1",
    "arxiv_id": "2602.19756v1",
    "authors": "Junhyeok Choi, Sangwoo Mo, Minwoo Chae",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 12:08:28",
    "ori_summary": "Recent advances in multimodal learning have achieved remarkable success across diverse vision-language tasks. However, such progress heavily relies on large-scale image-text datasets, making training costly and inefficient. Prior efforts in dataset filtering and pruning attempt to mitigate this issue, but still require relatively large subsets to maintain performance and fail under very small subsets. Dataset distillation offers a promising alternative, yet existing multimodal dataset distillation methods require full-dataset training and joint optimization of image pixels and text features, making them architecture-dependent and limiting cross-architecture generalization. To overcome this, we propose a learning-free dataset distillation framework that eliminates the need for large-scale training and optimization while enhancing generalization across architectures. Our method uses CLIP to extract aligned image-text embeddings, obtains prototypes, and employs an unCLIP decoder to synthesize images, enabling efficient and scalable multimodal dataset distillation. Extensive experiments demonstrate that our approach consistently outperforms optimization-based dataset distillation and subset selection methods, achieving state-of-the-art cross-architecture generalization.",
    "summary": "",
    "translation": "通过原型引导数据合成实现简化的多模态数据集蒸馏",
    "relevance_score": 3,
    "reasoning": "该论文涉及多模态数据集蒸馏技术，属于数据效率领域，可能对需要处理异构数据的推荐/搜索系统有间接价值。然而，论文标题未明确说明与推荐系统、搜索或广告的直接应用，也未提及Transformer架构或LLM技术，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19753v1": {
    "title": "RAP: Fast Feedforward Rendering-Free Attribute-Guided Primitive Importance Score Prediction for Efficient 3D Gaussian Splatting Processing",
    "url": "https://www.alphaxiv.org/abs/2602.19753v1",
    "arxiv_id": "2602.19753v1",
    "authors": "Kaifa Yang, Qi Yang, Yiling Xu, Zhu Li",
    "categories": "cs.CV, cs.GR",
    "pub_date": "2026-02-23 12:02:03",
    "ori_summary": "3D Gaussian Splatting (3DGS) has emerged as a leading technology for high-quality 3D scene reconstruction. However, the iterative refinement and densification process leads to the generation of a large number of primitives, each contributing to the reconstruction to a substantially different extent. Estimating primitive importance is thus crucial, both for removing redundancy during reconstruction and for enabling efficient compression and transmission. Existing methods typically rely on rendering-based analyses, where each primitive is evaluated through its contribution across multiple camera viewpoints. However, such methods are sensitive to the number and selection of views, rely on specialized differentiable rasterizers, and have long calculation times that grow linearly with view count, making them difficult to integrate as plug-and-play modules and limiting scalability and generalization. To address these issues, we propose RAP, a fast feedforward rendering-free attribute-guided method for efficient importance score prediction in 3DGS. RAP infers primitive significance directly from intrinsic Gaussian attributes and local neighborhood statistics, avoiding rendering-based or visibility-dependent computations. A compact MLP predicts per-primitive importance scores using rendering loss, pruning-aware loss, and significance distribution regularization. After training on a small set of scenes, RAP generalizes effectively to unseen data and can be seamlessly integrated into reconstruction, compression, and transmission pipelines. Our code is publicly available at https://github.com/yyyykf/RAP.",
    "summary": "",
    "translation": "RAP：用于高效3D高斯泼溅处理的快速前馈无渲染属性引导基元重要性分数预测",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于3D高斯泼溅处理中的基元重要性分数预测，属于计算机图形学和3D视觉领域。虽然提到了高效处理，但未涉及推荐系统、搜索或广告的核心技术，也没有展示与LLM、Transformer架构或异构数据统一建模的明显关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19736v1": {
    "title": "InfScene-SR: Spatially Continuous Inference for Arbitrary-Size Image Super-Resolution",
    "url": "https://www.alphaxiv.org/abs/2602.19736v1",
    "arxiv_id": "2602.19736v1",
    "authors": "Shoukun Sun, Zhe Wang, Xiang Que, Jiyin Zhang, Xiaogang Ma",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 11:34:59",
    "ori_summary": "Image Super-Resolution (SR) aims to recover high-resolution (HR) details from low-resolution (LR) inputs, a task where Denoising Diffusion Probabilistic Models (DDPMs) have recently shown superior performance compared to Generative Adversarial Networks (GANs) based approaches. However, standard diffusion-based SR models, such as SR3, are typically trained on fixed-size patches and struggle to scale to arbitrary-sized images due to memory constraints. Applying these models via independent patch processing leads to visible seams and inconsistent textures across boundaries. In this paper, we propose InfScene-SR, a framework enabling spatially continuous super-resolution for large, arbitrary scenes. We adapt the iterative refinement process of diffusion models with a novel guided and variance-corrected fusion mechanism, allowing for the seamless generation of large-scale high-resolution imagery without retraining. We validate our approach on remote sensing datasets, demonstrating that InfScene-SR not only reconstructs fine details with high perceptual quality but also eliminates boundary artifacts, benefiting downstream tasks such as semantic segmentation.",
    "summary": "",
    "translation": "InfScene-SR：面向任意尺寸图像超分辨率的空间连续推理",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的图像超分辨率技术，属于纯粹的视觉处理任务。虽然标题中提到“推理”和“任意尺寸”，但这与推荐系统、搜索或广告中的序列建模、特征工程或排序优化没有直接关联。论文没有涉及LLM、Transformer架构改进，也没有展示如何将视觉技术应用于多模态推荐或广告场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19735v1": {
    "title": "VGGT-MPR: VGGT-Enhanced Multimodal Place Recognition in Autonomous Driving Environments",
    "url": "https://www.alphaxiv.org/abs/2602.19735v1",
    "arxiv_id": "2602.19735v1",
    "authors": "Jingyi Xu, Zhangshuo Qi, Zhongmiao Yan, Xuyu Gao, Qianyun Jiao, Songpengcheng Xia, Xieyuanli Chen, Ling Pei",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 11:33:56",
    "ori_summary": "In autonomous driving, robust place recognition is critical for global localization and loop closure detection. While inter-modality fusion of camera and LiDAR data in multimodal place recognition (MPR) has shown promise in overcoming the limitations of unimodal counterparts, existing MPR methods basically attend to hand-crafted fusion strategies and heavily parameterized backbones that require costly retraining. To address this, we propose VGGT-MPR, a multimodal place recognition framework that adopts the Visual Geometry Grounded Transformer (VGGT) as a unified geometric engine for both global retrieval and re-ranking. In the global retrieval stage, VGGT extracts geometrically-rich visual embeddings through prior depth-aware and point map supervision, and densifies sparse LiDAR point clouds with predicted depth maps to improve structural representation. This enhances the discriminative ability of fused multimodal features and produces global descriptors for fast retrieval. Beyond global retrieval, we design a training-free re-ranking mechanism that exploits VGGT's cross-view keypoint-tracking capability. By combining mask-guided keypoint extraction with confidence-aware correspondence scoring, our proposed re-ranking mechanism effectively refines retrieval results without additional parameter optimization. Extensive experiments on large-scale autonomous driving benchmarks and our self-collected data demonstrate that VGGT-MPR achieves state-of-the-art performance, exhibiting strong robustness to severe environmental changes, viewpoint shifts, and occlusions. Our code and data will be made publicly available.",
    "summary": "",
    "translation": "VGGT-MPR：自动驾驶环境中VGGT增强的多模态地点识别",
    "relevance_score": 1,
    "reasoning": "该论文专注于自动驾驶环境中的多模态地点识别，属于纯粹的计算机视觉应用领域，与推荐系统、搜索或广告的核心技术进展没有直接关联。论文标题中提到的VGGT增强和多模态处理虽然涉及技术方法，但应用场景（自动驾驶）和核心问题（地点识别）均不在您关注的RecSys/Search/Ads技术栈范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19723v1": {
    "title": "Towards Personalized Multi-Modal MRI Synthesis across Heterogeneous Datasets",
    "url": "https://www.alphaxiv.org/abs/2602.19723v1",
    "arxiv_id": "2602.19723v1",
    "authors": "Yue Zhang, Zhizheng Zhuo, Siyao Xu, Shan Lv, Zhaoxi Liu, Jun Qiu, Qiuli Wang, Yaou Liu, S. Kevin Zhou",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 11:20:27",
    "ori_summary": "Synthesizing missing modalities in multi-modal magnetic resonance imaging (MRI) is vital for ensuring diagnostic completeness, particularly when full acquisitions are infeasible due to time constraints, motion artifacts, and patient tolerance. Recent unified synthesis models have enabled flexible synthesis tasks by accommodating various input-output configurations. However, their training and evaluation are typically restricted to a single dataset, limiting their generalizability across diverse clinical datasets and impeding practical deployment. To address this limitation, we propose PMM-Synth, a personalized MRI synthesis framework that not only supports various synthesis tasks but also generalizes effectively across heterogeneous datasets. PMM-Synth is jointly trained on multiple multi-modal MRI datasets that differ in modality coverage, disease types, and intensity distributions. It achieves cross-dataset generalization through three core innovations: a Personalized Feature Modulation module that dynamically adapts feature representations based on dataset identifier to mitigate the impact of distributional shifts; a Modality-Consistent Batch Scheduler that facilitates stable and efficient batch training under inconsistent modality conditions; and a selective supervision loss to ensure effective learning when ground truth modalities are partially missing. Evaluated on four clinical multi-modal MRI datasets, PMM-Synth consistently outperforms state-of-the-art methods in both one-to-one and many-to-one synthesis tasks, achieving superior PSNR and SSIM scores. Qualitative results further demonstrate improved preservation of anatomical structures and pathological details. Additionally, downstream tumor segmentation and radiological reporting studies suggest that PMM-Synth holds potential for supporting reliable diagnosis under real-world modality-missing scenarios.",
    "summary": "",
    "translation": "面向跨异构数据集的个性化多模态MRI合成",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及医学影像（MRI）合成，属于医学/生物领域的特定应用，与您关注的推荐系统、搜索、广告、LLM技术或Transformer架构等核心领域完全无关。即使考虑多模态数据处理，其医学影像的特定领域性质使其无法应用于您的关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19719v1": {
    "title": "Generative 6D Pose Estimation via Conditional Flow Matching",
    "url": "https://www.alphaxiv.org/abs/2602.19719v1",
    "arxiv_id": "2602.19719v1",
    "authors": "Amir Hamza, Davide Boscaini, Weihang Li, Benjamin Busam, Fabio Poiesi",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 11:15:12",
    "ori_summary": "Existing methods for instance-level 6D pose estimation typically rely on neural networks that either directly regress the pose in $\\mathrm{SE}(3)$ or estimate it indirectly via local feature matching. The former struggle with object symmetries, while the latter fail in the absence of distinctive local features. To overcome these limitations, we propose a novel formulation of 6D pose estimation as a conditional flow matching problem in $\\mathbb{R}^3$. We introduce Flose, a generative method that infers object poses via a denoising process conditioned on local features. While prior approaches based on conditional flow matching perform denoising solely based on geometric guidance, Flose integrates appearance-based semantic features to mitigate ambiguities caused by object symmetries. We further incorporate RANSAC-based registration to handle outliers. We validate Flose on five datasets from the established BOP benchmark. Flose outperforms prior methods with an average improvement of +4.5 Average Recall. Project Website : https://tev-fbk.github.io/Flose/",
    "summary": "",
    "translation": "基于条件流匹配的生成式6D姿态估计",
    "relevance_score": 1,
    "reasoning": "该论文标题表明其专注于计算机视觉中的6D姿态估计问题，属于纯粹的视觉/3D视觉领域，与推荐系统、搜索或广告的核心技术栈没有直接关联。即使采用生成式方法，其应用场景（如机器人、增强现实）与RecSys/Search/Ads的异构数据处理、用户行为建模或排序优化等核心问题无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19715v1": {
    "title": "Pixels Don't Lie (But Your Detector Might): Bootstrapping MLLM-as-a-Judge for Trustworthy Deepfake Detection and Reasoning Supervision",
    "url": "https://www.alphaxiv.org/abs/2602.19715v1",
    "arxiv_id": "2602.19715v1",
    "authors": "Kartik Kuckreja, Parul Gupta, Muhammad Haris Khan, Abhinav Dhall",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 11:08:46",
    "ori_summary": "Deepfake detection models often generate natural-language explanations, yet their reasoning is frequently ungrounded in visual evidence, limiting reliability. Existing evaluations measure classification accuracy but overlook reasoning fidelity. We propose DeepfakeJudge, a framework for scalable reasoning supervision and evaluation, that integrates an out-of-distribution benchmark containing recent generative and editing forgeries, a human-annotated subset with visual reasoning labels, and a suite of evaluation models, that specialize in evaluating reasoning rationales without the need for explicit ground truth reasoning rationales. The Judge is optimized through a bootstrapped generator-evaluator process that scales human feedback into structured reasoning supervision and supports both pointwise and pairwise evaluation. On the proposed meta-evaluation benchmark, our reasoning-bootstrapped model achieves an accuracy of 96.2\\%, outperforming \\texttt{30x} larger baselines. The reasoning judge attains very high correlation with human ratings and 98.9\\% percent pairwise agreement on the human-annotated meta-evaluation subset. These results establish reasoning fidelity as a quantifiable dimension of deepfake detection and demonstrate scalable supervision for interpretable deepfake reasoning. Our user study shows that participants preferred the reasonings generated by our framework 70\\% of the time, in terms of faithfulness, groundedness, and usefulness, compared to those produced by other models and datasets. All of our datasets, models, and codebase are \\href{https://github.com/KjAeRsTuIsK/DeepfakeJudge}{open-sourced}.",
    "summary": "",
    "translation": "像素不会说谎（但你的检测器可能会）：基于MLLM作为评判者的自举方法，用于可信赖的深度伪造检测与推理监督",
    "relevance_score": 1,
    "reasoning": "该论文主要关注深度伪造检测和MLLM作为评判者的应用，属于计算机视觉和内容真实性验证领域。虽然涉及多模态大语言模型（MLLM），但其应用场景（深度伪造检测）与推荐系统、搜索或广告的核心排名、匹配、个性化任务没有直接关联，也不属于所关注的Transformer架构进展或LLM在推荐/搜索/广告中的直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19710v1": {
    "title": "Universal Pose Pretraining for Generalizable Vision-Language-Action Policies",
    "url": "https://www.alphaxiv.org/abs/2602.19710v1",
    "arxiv_id": "2602.19710v1",
    "authors": "Haitao Lin, Hanyang Yu, Jingshun Huang, He Zhang, Yonggen Ling, Ping Tan, Xiangyang Xue, Yanwei Fu",
    "categories": "cs.CV, cs.LG, cs.RO",
    "pub_date": "2026-02-23 11:00:08",
    "ori_summary": "Existing Vision-Language-Action (VLA) models often suffer from feature collapse and low training efficiency because they entangle high-level perception with sparse, embodiment-specific action supervision. Since these models typically rely on VLM backbones optimized for Visual Question Answering (VQA), they excel at semantic identification but often overlook subtle 3D state variations that dictate distinct action patterns. To resolve these misalignments, we propose Pose-VLA, a decoupled paradigm that separates VLA training into a pre-training phase for extracting universal 3D spatial priors in a unified camera-centric space, and a post-training phase for efficient embodiment alignment within robot-specific action space. By introducing discrete pose tokens as a universal representation, Pose-VLA seamlessly integrates spatial grounding from diverse 3D datasets with geometry-level trajectories from robotic demonstrations. Our framework follows a two-stage pre-training pipeline, establishing fundamental spatial grounding via poses followed by motion alignment through trajectory supervision. Extensive evaluations demonstrate that Pose-VLA achieves state-of-the-art results on RoboTwin 2.0 with a 79.5% average success rate and competitive performance on LIBERO at 96.0%. Real-world experiments further showcase robust generalization across diverse objects using only 100 demonstrations per task, validating the efficiency of our pre-training paradigm.",
    "summary": "",
    "translation": "通用姿态预训练：面向可泛化的视觉-语言-动作策略",
    "relevance_score": 2,
    "reasoning": "该论文标题表明其核心是视觉-语言-动作策略的预训练，这属于机器人或具身智能领域，与推荐系统、搜索或广告的直接关联性较弱。虽然涉及多模态（视觉-语言），但其重点在于动作策略和姿态控制，而非处理用户行为序列或上下文特征等推荐/搜索相关数据。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19708v1": {
    "title": "ChimeraLoRA: Multi-Head LoRA-Guided Synthetic Datasets",
    "url": "https://www.alphaxiv.org/abs/2602.19708v1",
    "arxiv_id": "2602.19708v1",
    "authors": "Hoyoung Kim, Minwoo Jang, Jabin Koo, Sangdoo Yun, Jungseul Ok",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 10:59:41",
    "ori_summary": "Beyond general recognition tasks, specialized domains including privacy-constrained medical applications and fine-grained settings often encounter data scarcity, especially for tail classes. To obtain less biased and more reliable models under such scarcity, practitioners leverage diffusion models to supplement underrepresented regions of real data. Specifically, recent studies fine-tune pretrained diffusion models with LoRA on few-shot real sets to synthesize additional images. While an image-wise LoRA trained on a single image captures fine-grained details yet offers limited diversity, a class-wise LoRA trained over all shots produces diverse images as it encodes class priors yet tends to overlook fine details. To combine both benefits, we separate the adapter into a class-shared LoRA~$A$ for class priors and per-image LoRAs~$\\mathcal{B}$ for image-specific characteristics. To expose coherent class semantics in the shared LoRA~$A$, we propose a semantic boosting by preserving class bounding boxes during training. For generation, we compose $A$ with a mixture of $\\mathcal{B}$ using coefficients drawn from a Dirichlet distribution. Across diverse datasets, our synthesized images are both diverse and detail-rich while closely aligning with the few-shot real distribution, yielding robust gains in downstream classification accuracy.",
    "summary": "",
    "translation": "ChimeraLoRA：多头LoRA引导的合成数据集",
    "relevance_score": 3,
    "reasoning": "该论文涉及LoRA（低秩适应）技术，属于LLM高效微调方法，可能对“使能LLM技术”有间接贡献。然而，标题聚焦于合成数据集生成，这更偏向数据工程而非核心架构或应用创新，且未明确指向推荐/搜索/广告领域的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19706v1": {
    "title": "HDR Reconstruction Boosting with Training-Free and Exposure-Consistent Diffusion",
    "url": "https://www.alphaxiv.org/abs/2602.19706v1",
    "arxiv_id": "2602.19706v1",
    "authors": "Yo-Tin Lin, Su-Kai Chen, Hou-Ning Hu, Yen-Yu Lin, Yu-Lun Liu",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 10:57:22",
    "ori_summary": "Single LDR to HDR reconstruction remains challenging for over-exposed regions where traditional methods often fail due to complete information loss. We present a training-free approach that enhances existing indirect and direct HDR reconstruction methods through diffusion-based inpainting. Our method combines text-guided diffusion models with SDEdit refinement to generate plausible content in over-exposed areas while maintaining consistency across multi-exposure LDR images. Unlike previous approaches requiring extensive training, our method seamlessly integrates with existing HDR reconstruction techniques through an iterative compensation mechanism that ensures luminance coherence across multiple exposures. We demonstrate significant improvements in both perceptual quality and quantitative metrics on standard HDR datasets and in-the-wild captures. Results show that our method effectively recovers natural details in challenging scenarios while preserving the advantages of existing HDR reconstruction pipelines. Project page: https://github.com/EusdenLin/HDR-Reconstruction-Boosting",
    "summary": "",
    "translation": "基于无训练和曝光一致性扩散的高动态范围重建增强",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉领域的高动态范围图像重建技术，使用扩散模型进行图像处理。虽然涉及生成模型，但内容纯粹针对视觉任务，没有展示任何在推荐系统、搜索或广告领域的潜在应用。标题明确指向图像处理，与所有关注领域均不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19697v1": {
    "title": "BayesFusion-SDF: Probabilistic Signed Distance Fusion with View Planning on CPU",
    "url": "https://www.alphaxiv.org/abs/2602.19697v1",
    "arxiv_id": "2602.19697v1",
    "authors": "Soumya Mazumdar, Vineet Kumar Rakesh, Tapas Samanta",
    "categories": "cs.CV, cs.GR, cs.RO",
    "pub_date": "2026-02-23 10:44:15",
    "ori_summary": "Key part of robotics, augmented reality, and digital inspection is dense 3D reconstruction from depth observations. Traditional volumetric fusion techniques, including truncated signed distance functions (TSDF), enable efficient and deterministic geometry reconstruction; however, they depend on heuristic weighting and fail to transparently convey uncertainty in a systematic way. Recent neural implicit methods, on the other hand, get very high fidelity but usually need a lot of GPU power for optimization and aren't very easy to understand for making decisions later on. This work presents BayesFusion-SDF, a CPU-centric probabilistic signed distance fusion framework that conceptualizes geometry as a sparse Gaussian random field with a defined posterior distribution over voxel distances. First, a rough TSDF reconstruction is used to create an adaptive narrow-band domain. Then, depth observations are combined using a heteroscedastic Bayesian formulation that is solved using sparse linear algebra and preconditioned conjugate gradients. Randomized diagonal estimators are a quick way to get an idea of posterior uncertainty. This makes it possible to extract surfaces and plan the next best view while taking into account uncertainty. Tests on a controlled ablation scene and a CO3D object sequence show that the new method is more accurate geometrically than TSDF baselines and gives useful estimates of uncertainty for active sensing. The proposed formulation provides a clear and easy-to-use alternative to GPU-heavy neural reconstruction methods while still being able to be understood in a probabilistic way and acting in a predictable way. GitHub: https://mazumdarsoumya.github.io/BayesFusionSDF",
    "summary": "",
    "translation": "BayesFusion-SDF：基于CPU的概率符号距离融合与视点规划",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及计算机视觉中的3D重建技术（符号距离函数融合和视点规划），属于纯粹的3D视觉领域。虽然提到了概率方法（BayesFusion），但没有表明与推荐系统、搜索或广告有任何直接关联，也不属于核心LLM技术、Transformer架构进展或异构数据统一建模的范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19679v1": {
    "title": "TeHOR: Text-Guided 3D Human and Object Reconstruction with Textures",
    "url": "https://www.alphaxiv.org/abs/2602.19679v1",
    "arxiv_id": "2602.19679v1",
    "authors": "Hyeongjin Nam, Daniel Sungho Jung, Kyoung Mu Lee",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-23 10:22:52",
    "ori_summary": "Joint reconstruction of 3D human and object from a single image is an active research area, with pivotal applications in robotics and digital content creation. Despite recent advances, existing approaches suffer from two fundamental limitations. First, their reconstructions rely heavily on physical contact information, which inherently cannot capture non-contact human-object interactions, such as gazing at or pointing toward an object. Second, the reconstruction process is primarily driven by local geometric proximity, neglecting the human and object appearances that provide global context crucial for understanding holistic interactions. To address these issues, we introduce TeHOR, a framework built upon two core designs. First, beyond contact information, our framework leverages text descriptions of human-object interactions to enforce semantic alignment between the 3D reconstruction and its textual cues, enabling reasoning over a wider spectrum of interactions, including non-contact cases. Second, we incorporate appearance cues of the 3D human and object into the alignment process to capture holistic contextual information, thereby ensuring visually plausible reconstructions. As a result, our framework produces accurate and semantically coherent reconstructions, achieving state-of-the-art performance.",
    "summary": "",
    "translation": "TeHOR：基于文本引导的带纹理三维人体与物体重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本引导的三维重建，属于计算机视觉和图形学领域，与推荐系统、搜索或广告的核心技术无直接关联。其内容涉及三维人体/物体重建和纹理生成，属于明确的无关主题（Purely Vision/3D Vision/Graphic papers without clear relevance to RecSys/Search/Ads），没有展示出在推荐、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19668v1": {
    "title": "Personalized Longitudinal Medical Report Generation via Temporally-Aware Federated Adaptation",
    "url": "https://www.alphaxiv.org/abs/2602.19668v1",
    "arxiv_id": "2602.19668v1",
    "authors": "He Zhu, Ren Togo, Takahiro Ogawa, Kenji Hirata, Minghui Tang, Takaaki Yoshimura, Hiroyuki Sugimori, Noriko Nishioka, Yukie Shimizu, Kohsuke Kudo, Miki Haseyama",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-02-23 10:14:36",
    "ori_summary": "Longitudinal medical report generation is clinically important yet remains challenging due to strict privacy constraints and the evolving nature of disease progression. Although federated learning (FL) enables collaborative training without data sharing, existing FL methods largely overlook longitudinal dynamics by assuming stationary client distributions, making them unable to model temporal shifts across visits or patient-specific heterogeneity-ultimately leading to unstable optimization and suboptimal report generation. We introduce Federated Temporal Adaptation (FTA), a federated setting that explicitly accounts for the temporal evolution of client data. Building upon this setting, we propose FedTAR, a framework that integrates demographic-driven personalization with time-aware global aggregation. FedTAR generates lightweight LoRA adapters from demographic embeddings and performs temporal residual aggregation, where updates from different visits are weighted by a meta-learned temporal policy optimized via first-order MAML. Experiments on J-MID (1M exams) and MIMIC-CXR demonstrate consistent improvements in linguistic accuracy, temporal coherence, and cross-site generalization, establishing FedTAR as a robust and privacy-preserving paradigm for federated longitudinal modeling.",
    "summary": "",
    "translation": "基于时序感知联邦适配的个性化纵向医疗报告生成",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及医疗领域（Medical Report Generation）和联邦学习（Federated Adaptation），这两者均属于明确的无关主题。虽然包含“个性化”和“时序感知”等可能相关的概念，但核心应用领域使其完全不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19631v1": {
    "title": "Localized Concept Erasure in Text-to-Image Diffusion Models via High-Level Representation Misdirection",
    "url": "https://www.alphaxiv.org/abs/2602.19631v1",
    "arxiv_id": "2602.19631v1",
    "authors": "Uichan Lee, Jeonghyeon Kim, Sangheum Hwang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-23 09:18:27",
    "ori_summary": "Recent advances in text-to-image (T2I) diffusion models have seen rapid and widespread adoption. However, their powerful generative capabilities raise concerns about potential misuse for synthesizing harmful, private, or copyrighted content. To mitigate such risks, concept erasure techniques have emerged as a promising solution. Prior works have primarily focused on fine-tuning the denoising component (e.g., the U-Net backbone). However, recent causal tracing studies suggest that visual attribute information is localized in the early self-attention layers of the text encoder, indicating a potential alternative for concept erasing. Building on this insight, we conduct preliminary experiments and find that directly fine-tuning early layers can suppress target concepts but often degrades the generation quality of non-target concepts. To overcome this limitation, we propose High-Level Representation Misdirection (HiRM), which misdirects high-level semantic representations of target concepts in the text encoder toward designated vectors such as random directions or semantically defined directions (e.g., supercategories), while updating only early layers that contain causal states of visual attributes. Our decoupling strategy enables precise concept removal with minimal impact on unrelated concepts, as demonstrated by strong results on UnlearnCanvas and NSFW benchmarks across diverse targets (e.g., objects, styles, nudity). HiRM also preserves generative utility at low training cost, transfers to state-of-the-art architectures such as Flux without additional training, and shows synergistic effects with denoiser-based concept erasing methods.",
    "summary": "",
    "translation": "通过高层表征误导实现文本到图像扩散模型中的局部概念擦除",
    "relevance_score": 2,
    "reasoning": "该论文主要关注文本到图像扩散模型的特定编辑技术（概念擦除），属于纯粹的视觉生成领域，与推荐系统、搜索或广告的核心技术无关。虽然涉及扩散模型，但论文重点在于图像生成中的概念控制，而非推荐/搜索/广告中所需的文本理解、序列建模或异构数据处理能力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19624v1": {
    "title": "Accurate Planar Tracking With Robust Re-Detection",
    "url": "https://www.alphaxiv.org/abs/2602.19624v1",
    "arxiv_id": "2602.19624v1",
    "authors": "Jonas Serych, Jiri Matas",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 09:13:55",
    "ori_summary": "We present SAM-H and WOFTSAM, novel planar trackers that combine robust long-term segmentation tracking provided by SAM 2 with 8 degrees-of-freedom homography pose estimation. SAM-H estimates homographies from segmentation mask contours and is thus highly robust to target appearance changes. WOFTSAM significantly improves the current state-of-the-art planar tracker WOFT by exploiting lost target re-detection provided by SAM-H. The proposed methods are evaluated on POT-210 and PlanarTrack tracking benchmarks, setting the new state-of-the-art performance on both. On the latter, they outperform the second best by a large margin, +12.4 and +15.2pp on the p@15 metric. We also present improved ground-truth annotations of initial PlanarTrack poses, enabling more accurate benchmarking in the high-precision p@5 metric. The code and the re-annotations are available at https://github.com/serycjon/WOFTSAM",
    "summary": "",
    "translation": "基于鲁棒重检测的精确平面跟踪",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及计算机视觉中的平面跟踪技术，属于纯视觉领域的研究。虽然提到了“鲁棒重检测”，但这主要针对视觉跟踪中的特定技术问题，与推荐系统、搜索或广告的核心技术、LLM应用、Transformer架构进展或异构数据统一建模等当前关注领域没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19623v1": {
    "title": "PedaCo-Gen: Scaffolding Pedagogical Agency in Human-AI Collaborative Video Authoring",
    "url": "https://www.alphaxiv.org/abs/2602.19623v1",
    "arxiv_id": "2602.19623v1",
    "authors": "Injun Baek, Yearim Kim, Nojun Kwak",
    "categories": "cs.CV, cs.AI, cs.HC",
    "pub_date": "2026-02-23 09:12:13",
    "ori_summary": "While advancements in Text-to-Video (T2V) generative AI offer a promising path toward democratizing content creation, current models are often optimized for visual fidelity rather than instructional efficacy. This study introduces PedaCo-Gen, a pedagogically-informed human-AI collaborative video generating system for authoring instructional videos based on Mayer's Cognitive Theory of Multimedia Learning (CTML). Moving away from traditional \"one-shot\" generation, PedaCo-Gen introduces an Intermediate Representation (IR) phase, enabling educators to interactively review and refine video blueprints-comprising scripts and visual descriptions-with an AI reviewer. Our study with 23 education experts demonstrates that PedaCo-Gen significantly enhances video quality across various topics and CTML principles compared to baselines. Participants perceived the AI-driven guidance not merely as a set of instructions but as a metacognitive scaffold that augmented their instructional design expertise, reporting high production efficiency (M=4.26) and guide validity (M=4.04). These findings highlight the importance of reclaiming pedagogical agency through principled co-creation, providing a foundation for future AI authoring tools that harmonize generative power with human professional expertise.",
    "summary": "",
    "translation": "PedaCo-Gen：人机协作视频创作中教学能动性的支架式构建",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于人机协作视频创作中的教学能动性，属于内容生成（AIGC）和教育技术领域。虽然涉及AI协作，但核心是特定领域（教育视频创作）的应用，与推荐系统、搜索、广告的核心技术进展、Transformer架构改进或LLM直接应用无关，也不涉及异构数据的统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19615v1": {
    "title": "Seeing Clearly, Reasoning Confidently: Plug-and-Play Remedies for Vision Language Model Blindness",
    "url": "https://www.alphaxiv.org/abs/2602.19615v1",
    "arxiv_id": "2602.19615v1",
    "authors": "Xin Hu, Haomiao Ni, Yunbei Zhang, Jihun Hamm, Zechen Li, Zhengming Ding",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 09:02:40",
    "ori_summary": "Vision language models (VLMs) have achieved remarkable success in broad visual understanding, yet they remain challenged by object-centric reasoning on rare objects due to the scarcity of such instances in pretraining data. While prior efforts alleviate this issue by retrieving additional data or introducing stronger vision encoders, these methods are still computationally intensive during finetuning VLMs and don't fully exploit the original training data. In this paper, we introduce an efficient plug-and-play module that substantially improves VLMs' reasoning over rare objects by refining visual tokens and enriching input text prompts, without VLMs finetuning. Specifically, we propose to learn multi-modal class embeddings for rare objects by leveraging prior knowledge from vision foundation models and synonym-augmented text descriptions, compensating for limited training examples. These embeddings refine the visual tokens in VLMs through a lightweight attention-based enhancement module that improves fine-grained object details. In addition, we use the learned embeddings as object-aware detectors to generate informative hints, which are injected into the text prompts to help guide the VLM's attention toward relevant image regions. Experiments on two benchmarks show consistent and substantial gains for pretrained VLMs in rare object recognition and reasoning. Further analysis reveals how our method strengthens the VLM's ability to focus on and reason about rare objects.",
    "summary": "",
    "translation": "清晰观察，自信推理：视觉语言模型盲点的即插即用解决方案",
    "relevance_score": 2,
    "reasoning": "该论文主要针对视觉语言模型（VLM）的盲点问题提出解决方案，属于纯粹的视觉-语言交叉领域研究。虽然标题提到“Reasoning”，但核心是解决VLM的视觉理解缺陷，没有明确涉及推荐系统、搜索或广告领域的应用场景。该研究可能对多模态理解有贡献，但缺乏与RecSys/Search/Ads的直接关联或明确的应用潜力说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19611v1": {
    "title": "RAID: Retrieval-Augmented Anomaly Detection",
    "url": "https://www.alphaxiv.org/abs/2602.19611v1",
    "arxiv_id": "2602.19611v1",
    "authors": "Mingxiu Cai, Zhe Zhang, Gaochang Wu, Tianyou Chai, Xiatian Zhu",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 08:54:27",
    "ori_summary": "Unsupervised Anomaly Detection (UAD) aims to identify abnormal regions by establishing correspondences between test images and normal templates. Existing methods primarily rely on image reconstruction or template retrieval but face a fundamental challenge: matching between test images and normal templates inevitably introduces noise due to intra-class variations, imperfect correspondences, and limited templates. Observing that Retrieval-Augmented Generation (RAG) leverages retrieved samples directly in the generation process, we reinterpret UAD through this lens and introduce \\textbf{RAID}, a retrieval-augmented UAD framework designed for noise-resilient anomaly detection and localization. Unlike standard RAG that enriches context or knowledge, we focus on using retrieved normal samples to guide noise suppression in anomaly map generation. RAID retrieves class-, semantic-, and instance-level representations from a hierarchical vector database, forming a coarse-to-fine pipeline. A matching cost volume correlates the input with retrieved exemplars, followed by a guided Mixture-of-Experts (MoE) network that leverages the retrieved samples to adaptively suppress matching noise and produce fine-grained anomaly maps. RAID achieves state-of-the-art performance across full-shot, few-shot, and multi-dataset settings on MVTec, VisA, MPDD, and BTAD benchmarks. \\href{https://github.com/Mingxiu-Cai/RAID}{https://github.com/Mingxiu-Cai/RAID}.",
    "summary": "",
    "translation": "RAID：检索增强的异常检测",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及检索增强技术，这属于LLM技术范畴，但异常检测主要应用于网络安全、系统监控等领域，与推荐系统、搜索或广告的排名、匹配等核心任务关联较弱。虽然检索增强技术本身有潜力应用于RecSys/Search/Ads（例如增强检索或上下文理解），但论文标题明确指向异常检测这一特定应用，因此整体相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19608v1": {
    "title": "Satellite-Based Detection of Looted Archaeological Sites Using Machine Learning",
    "url": "https://www.alphaxiv.org/abs/2602.19608v1",
    "arxiv_id": "2602.19608v1",
    "authors": "Girmaw Abebe Tadesse, Titien Bartette, Andrew Hassanali, Allen Kim, Jonathan Chemla, Andrew Zolli, Yves Ubelmann, Caleb Robinson, Inbal Becker-Reshef, Juan Lavista Ferres",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-23 08:50:07",
    "ori_summary": "Looting at archaeological sites poses a severe risk to cultural heritage, yet monitoring thousands of remote locations remains operationally difficult. We present a scalable and satellite-based pipeline to detect looted archaeological sites, using PlanetScope monthly mosaics (4.7m/pixel) and a curated dataset of 1,943 archaeological sites in Afghanistan (898 looted, 1,045 preserved) with multi-year imagery (2016--2023) and site-footprint masks. We compare (i) end-to-end CNN classifiers trained on raw RGB patches and (ii) traditional machine learning (ML) trained on handcrafted spectral/texture features and embeddings from recent remote-sensing foundation models. Results indicate that ImageNet-pretrained CNNs combined with spatial masking reach an F1 score of 0.926, clearly surpassing the strongest traditional ML setup, which attains an F1 score of 0.710 using SatCLIP-V+RF+Mean, i.e., location and vision embeddings fed into a Random Forest with mean-based temporal aggregation. Ablation studies demonstrate that ImageNet pretraining (even in the presence of domain shift) and spatial masking enhance performance. In contrast, geospatial foundation model embeddings perform competitively with handcrafted features, suggesting that looting signatures are extremely localized. The repository is available at https://github.com/microsoft/looted_site_detection.",
    "summary": "",
    "translation": "基于卫星图像与机器学习的考古遗址盗掘检测",
    "relevance_score": 1,
    "reasoning": "该论文涉及卫星图像分析和特定领域（考古学）的机器学习应用，与推荐系统、搜索或广告的核心技术进展、LLM技术、Transformer架构或异构数据统一建模均无直接关联。它属于明确的无关主题（特定领域应用），没有任何技术内容可应用于当前关注的领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19605v1": {
    "title": "CLCR: Cross-Level Semantic Collaborative Representation for Multimodal Learning",
    "url": "https://www.alphaxiv.org/abs/2602.19605v1",
    "arxiv_id": "2602.19605v1",
    "authors": "Chunlei Meng, Guanhong Huang, Rong Fu, Runmin Jian, Zhongxue Gan, Chun Ouyang",
    "categories": "cs.CV, cs.AI, cs.MM",
    "pub_date": "2026-02-23 08:47:19",
    "ori_summary": "Multimodal learning aims to capture both shared and private information from multiple modalities. However, existing methods that project all modalities into a single latent space for fusion often overlook the asynchronous, multi-level semantic structure of multimodal data. This oversight induces semantic misalignment and error propagation, thereby degrading representation quality. To address this issue, we propose Cross-Level Co-Representation (CLCR), which explicitly organizes each modality's features into a three-level semantic hierarchy and specifies level-wise constraints for cross-modal interactions. First, a semantic hierarchy encoder aligns shallow, mid, and deep features across modalities, establishing a common basis for interaction. And then, at each level, an Intra-Level Co-Exchange Domain (IntraCED) factorizes features into shared and private subspaces and restricts cross-modal attention to the shared subspace via a learnable token budget. This design ensures that only shared semantics are exchanged and prevents leakage from private channels. To integrate information across levels, the Inter-Level Co-Aggregation Domain (InterCAD) synchronizes semantic scales using learned anchors, selectively fuses the shared representations, and gates private cues to form a compact task representation. We further introduce regularization terms to enforce separation of shared and private features and to minimize cross-level interference. Experiments on six benchmarks spanning emotion recognition, event localization, sentiment analysis, and action recognition show that CLCR achieves strong performance and generalizes well across tasks.",
    "summary": "该论文研究多模态学习中因忽略数据异步多层级语义结构导致的语义错位问题。其核心方法是构建三层语义层次结构，通过层级内共享子空间约束和层级间语义尺度同步，实现跨模态的精准语义对齐与融合。",
    "translation": "CLCR：跨层级语义协同表示用于多模态学习",
    "relevance_score": 8,
    "reasoning": "该论文涉及多模态学习中的协同表示方法，与'VLM类比用于异构数据'焦点高度相关，可将用户序列和上下文特征视为不同模态进行统一建模。跨层级语义协同表示技术可直接应用于推荐系统中处理用户行为序列、商品属性和上下文信息等多源异构数据，提升推荐效果。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出的跨层级语义协同表示方法，通过分层约束和多模态交互机制，直接解决了异构数据融合中的语义对齐问题，与VLM类比和统一建模高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.19596v1": {
    "title": "Learning Mutual View Information Graph for Adaptive Adversarial Collaborative Perception",
    "url": "https://www.alphaxiv.org/abs/2602.19596v1",
    "arxiv_id": "2602.19596v1",
    "authors": "Yihang Tao, Senkang Hu, Haonan An, Zhengru Fang, Hangcheng Cao, Yuguang Fang",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 08:38:27",
    "ori_summary": "Collaborative perception (CP) enables data sharing among connected and autonomous vehicles (CAVs) to enhance driving safety. However, CP systems are vulnerable to adversarial attacks where malicious agents forge false objects via feature-level perturbations. Current defensive systems use threshold-based consensus verification by comparing collaborative and ego detection results. Yet, these defenses remain vulnerable to more sophisticated attack strategies that could exploit two critical weaknesses: (i) lack of robustness against attacks with systematic timing and target region optimization, and (ii) inadvertent disclosure of vulnerability knowledge through implicit confidence information in shared collaboration data. In this paper, we propose MVIG attack, a novel adaptive adversarial CP framework learning to capture vulnerability knowledge disclosed by different defensive CP systems from a unified mutual view information graph (MVIG) representation. Our approach combines MVIG representation with temporal graph learning to generate evolving fabrication risk maps and employs entropy-aware vulnerability search to optimize attack location, timing and persistence, enabling adaptive attacks with generalizability across various defensive configurations. Extensive evaluations on OPV2V and Adv-OPV2V datasets demonstrate that MVIG attack reduces defense success rates by up to 62\\% against state-of-the-art defenses while achieving 47\\% lower detection for persistent attacks at 29.9 FPS, exposing critical security gaps in CP systems. Code will be released at https://github.com/yihangtao/MVIG.git",
    "summary": "",
    "translation": "学习互视图信息图以实现自适应对抗协同感知",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及协同感知和对抗学习，可能属于多智能体系统或机器人领域。虽然提到了'图'和'自适应'概念，但没有明确指向推荐系统、搜索或广告的核心问题。标题中的'协同感知'更可能指传感器融合或环境感知，而非用户-物品交互建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19575v1": {
    "title": "ConceptPrism: Concept Disentanglement in Personalized Diffusion Models via Residual Token Optimization",
    "url": "https://www.alphaxiv.org/abs/2602.19575v1",
    "arxiv_id": "2602.19575v1",
    "authors": "Minseo Kim, Minchan Kwon, Dongyeun Lee, Yunho Jeon, Junmo Kim",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 07:46:19",
    "ori_summary": "Personalized text-to-image generation suffers from concept entanglement, where irrelevant residual information from reference images is captured, leading to a trade-off between concept fidelity and text alignment. Recent disentanglement approaches attempt to solve this utilizing manual guidance, such as linguistic cues or segmentation masks, which limits their applicability and fails to fully articulate the target concept. In this paper, we propose ConceptPrism, a novel framework that automatically disentangles the shared visual concept from image-specific residuals by comparing images within a set. Our method jointly optimizes a target token and image-wise residual tokens using two complementary objectives: a reconstruction loss to ensure fidelity, and a novel exclusion loss that compels residual tokens to discard the shared concept. This process allows the target token to capture the pure concept without direct supervision. Extensive experiments demonstrate that ConceptPrism effectively resolves concept entanglement, achieving a significantly improved trade-off between fidelity and alignment.",
    "summary": "",
    "translation": "ConceptPrism：通过残差令牌优化在个性化扩散模型中实现概念解耦",
    "relevance_score": 2,
    "reasoning": "该论文主要关注个性化扩散模型中的概念解耦技术，属于AIGC/内容生成领域。虽然扩散模型是生成式AI的重要技术，但论文标题未表明与推荐系统、搜索或广告的直接应用关联，也未涉及Transformer架构改进或异构数据处理等关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19571v1": {
    "title": "HOCA-Bench: Beyond Semantic Perception to Predictive World Modeling via Hegelian Ontological-Causal Anomalies",
    "url": "https://www.alphaxiv.org/abs/2602.19571v1",
    "arxiv_id": "2602.19571v1",
    "authors": "Chang Liu, Yunfan Ye, Qingyang Zhou, Xichen Tan, Mengxuan Luo, Zhenyu Qiu, Wei Peng, Zhiping Cai",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 07:40:32",
    "ori_summary": "Video-LLMs have improved steadily on semantic perception, but they still fall short on predictive world modeling, which is central to physically grounded intelligence. We introduce HOCA-Bench, a benchmark that frames physical anomalies through a Hegelian lens. HOCA-Bench separates anomalies into two types: ontological anomalies, where an entity violates its own definition or persistence, and causal anomalies, where interactions violate physical relations. Using state-of-the-art generative video models as adversarial simulators, we build a testbed of 1,439 videos (3,470 QA pairs). Evaluations on 17 Video-LLMs show a clear cognitive lag: models often identify static ontological violations (e.g., shape mutations) but struggle with causal mechanisms (e.g., gravity or friction), with performance dropping by more than 20% on causal tasks. System-2 \"Thinking\" modes improve reasoning, but they do not close the gap, suggesting that current architectures recognize visual patterns more readily than they apply basic physical laws.",
    "summary": "",
    "translation": "HOCA-Bench：超越语义感知，通过黑格尔本体论因果异常实现预测性世界建模",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及哲学概念（黑格尔本体论）和异常检测，与推荐系统、搜索或广告的核心技术进展、LLM应用或Transformer架构改进均无直接关联。标题中的“预测性世界建模”可能暗示通用AI能力，但未明确指向RecSys/Search/Ads领域的实际应用场景，属于理论性较强且领域不匹配的内容。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19570v1": {
    "title": "VALD: Multi-Stage Vision Attack Detection for Efficient LVLM Defense",
    "url": "https://www.alphaxiv.org/abs/2602.19570v1",
    "arxiv_id": "2602.19570v1",
    "authors": "Nadav Kadvil, Ayellet Tal",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 07:39:43",
    "ori_summary": "Large Vision-Language Models (LVLMs) can be vulnerable to adversarial images that subtly bias their outputs toward plausible yet incorrect responses. We introduce a general, efficient, and training-free defense that combines image transformations with agentic data consolidation to recover correct model behavior. A key component of our approach is a two-stage detection mechanism that quickly filters out the majority of clean inputs. We first assess image consistency under content-preserving transformations at negligible computational cost. For more challenging cases, we examine discrepancies in a text-embedding space. Only when necessary do we invoke a powerful LLM to resolve attack-induced divergences. A key idea is to consolidate multiple responses, leveraging both their similarities and their differences. We show that our method achieves state-of-the-art accuracy while maintaining notable efficiency: most clean images skip costly processing, and even in the presence of numerous adversarial examples, the overhead remains minimal.",
    "summary": "",
    "translation": "VALD：面向高效大型视觉语言模型防御的多阶段视觉攻击检测",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及视觉攻击检测和大型视觉语言模型（LVLM）防御，属于计算机视觉和安全领域。虽然提到了LVLM，但核心关注点是安全防御而非推荐系统、搜索或广告的应用。根据您的关注点排除标准，这属于“纯粹视觉论文”且涉及“安全”等非技术性话题，与您的技术焦点完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19565v1": {
    "title": "DICArt: Advancing Category-level Articulated Object Pose Estimation in Discrete State-Spaces",
    "url": "https://www.alphaxiv.org/abs/2602.19565v1",
    "arxiv_id": "2602.19565v1",
    "authors": "Li Zhang, Mingyu Mei, Ailing Wang, Xianhui Meng, Yan Zhong, Xinyuan Song, Liu Liu, Rujing Wang, Zaixing He, Cewu Lu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-23 07:30:47",
    "ori_summary": "Articulated object pose estimation is a core task in embodied AI. Existing methods typically regress poses in a continuous space, but often struggle with 1) navigating a large, complex search space and 2) failing to incorporate intrinsic kinematic constraints. In this work, we introduce DICArt (DIsCrete Diffusion for Articulation Pose Estimation), a novel framework that formulates pose estimation as a conditional discrete diffusion process. Instead of operating in a continuous domain, DICArt progressively denoises a noisy pose representation through a learned reverse diffusion procedure to recover the GT pose. To improve modeling fidelity, we propose a flexible flow decider that dynamically determines whether each token should be denoised or reset, effectively balancing the real and noise distributions during diffusion. Additionally, we incorporate a hierarchical kinematic coupling strategy, estimating the pose of each rigid part hierarchically to respect the object's kinematic structure. We validate DICArt on both synthetic and real-world datasets. Experimental results demonstrate its superior performance and robustness. By integrating discrete generative modeling with structural priors, DICArt offers a new paradigm for reliable category-level 6D pose estimation in complex environments.",
    "summary": "",
    "translation": "DICArt：在离散状态空间中推进类别级关节物体姿态估计",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的3D姿态估计问题，属于纯粹的视觉/3D视觉领域研究。标题中提到的“关节物体姿态估计”和“离散状态空间”表明这是针对特定视觉任务的算法改进，没有显示出与推荐系统、搜索或广告领域的直接或间接关联。即使考虑VLM类比，该研究处理的是物理对象的几何姿态，而非推荐/搜索中常见的异构数据模态（如用户序列、上下文特征）。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19562v1": {
    "title": "A Multimodal Framework for Aligning Human Linguistic Descriptions with Visual Perceptual Data",
    "url": "https://www.alphaxiv.org/abs/2602.19562v1",
    "arxiv_id": "2602.19562v1",
    "authors": "Joseph Bingham",
    "categories": "cs.AI, cs.CV",
    "pub_date": "2026-02-23 07:20:11",
    "ori_summary": "Establishing stable mappings between natural language expressions and visual percepts is a foundational problem for both cognitive science and artificial intelligence. Humans routinely ground linguistic reference in noisy, ambiguous perceptual contexts, yet the mechanisms supporting such cross-modal alignment remain poorly understood. In this work, we introduce a computational framework designed to model core aspects of human referential interpretation by integrating linguistic utterances with perceptual representations derived from large-scale, crowd-sourced imagery. The system approximates human perceptual categorization by combining scale-invariant feature transform (SIFT) alignment with the Universal Quality Index (UQI) to quantify similarity in a cognitively plausible feature space, while a set of linguistic preprocessing and query-transformation operations captures pragmatic variability in referring expressions. We evaluate the model on the Stanford Repeated Reference Game corpus (15,000 utterances paired with tangram stimuli), a paradigm explicitly developed to probe human-level perceptual ambiguity and coordination. Our framework achieves robust referential grounding. It requires 65\\% fewer utterances than human interlocutors to reach stable mappings and can correctly identify target objects from single referring expressions 41.66\\% of the time (versus 20\\% for humans).These results suggest that relatively simple perceptual-linguistic alignment mechanisms can yield human-competitive behavior on a classic cognitive benchmark, and offers insights into models of grounded communication, perceptual inference, and cross-modal concept formation. Code is available at https://anonymous.4open.science/r/metasequoia-9D13/README.md .",
    "summary": "",
    "translation": "一种将人类语言描述与视觉感知数据对齐的多模态框架",
    "relevance_score": 2,
    "reasoning": "该论文虽然涉及多模态对齐，但其核心关注视觉感知数据与语言描述的匹配，这主要属于视觉-语言交叉领域。对于您关注的推荐/搜索/广告领域，该技术可能通过类比应用于异构数据（如用户行为序列与上下文特征）的统一建模，但标题未明确表明这种应用方向，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19542v1": {
    "title": "Vinedresser3D: Agentic Text-guided 3D Editing",
    "url": "https://www.alphaxiv.org/abs/2602.19542v1",
    "arxiv_id": "2602.19542v1",
    "authors": "Yankuan Chi, Xiang Li, Zixuan Huang, James M. Rehg",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 06:30:36",
    "ori_summary": "Text-guided 3D editing aims to modify existing 3D assets using natural-language instructions. Current methods struggle to jointly understand complex prompts, automatically localize edits in 3D, and preserve unedited content. We introduce Vinedresser3D, an agentic framework for high-quality text-guided 3D editing that operates directly in the latent space of a native 3D generative model. Given a 3D asset and an editing prompt, Vinedresser3D uses a multimodal large language model to infer rich descriptions of the original asset, identify the edit region and edit type (addition, modification, deletion), and generate decomposed structural and appearance-level text guidance. The agent then selects an informative view and applies an image editing model to obtain visual guidance. Finally, an inversion-based rectified-flow inpainting pipeline with an interleaved sampling module performs editing in the 3D latent space, enforcing prompt alignment while maintaining 3D coherence and unedited regions. Experiments on diverse 3D edits demonstrate that Vinedresser3D outperforms prior baselines in both automatic metrics and human preference studies, while enabling precise, coherent, and mask-free 3D editing.",
    "summary": "",
    "translation": "Vinedresser3D：基于智能体文本引导的三维编辑",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及文本引导的3D编辑和智能体技术，属于计算机视觉和图形学领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然提到了文本引导，但主要聚焦于3D内容生成而非推荐、搜索或广告中的文本理解或序列建模应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19540v1": {
    "title": "A Green Learning Approach to LDCT Image Restoration",
    "url": "https://www.alphaxiv.org/abs/2602.19540v1",
    "arxiv_id": "2602.19540v1",
    "authors": "Wei Wang, Yixing Wu, C. -C. Jay Kuo",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-23 06:21:56",
    "ori_summary": "This work proposes a green learning (GL) approach to restore medical images. Without loss of generality, we use low-dose computed tomography (LDCT) images as examples. LDCT images are susceptible to noise and artifacts, where the imaging process introduces distortion. LDCT image restoration is an important preprocessing step for further medical analysis. Deep learning (DL) methods have been developed to solve this problem. We examine an alternative solution using the Green Learning (GL) methodology. The new restoration method is characterized by mathematical transparency, computational and memory efficiency, and high performance. Experiments show that our GL method offers state-of-the-art restoration performance at a smaller model size and with lower inference complexity.",
    "summary": "",
    "translation": "一种用于低剂量CT图像恢复的绿色学习方法",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学影像处理（LDCT指低剂量计算机断层扫描），属于医疗领域的特定应用。虽然提到了学习方法，但没有任何内容表明与推荐系统、搜索、广告或相关使能技术有关联。该主题完全属于被排除的医学/生物学领域特定应用范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19539v1": {
    "title": "Can a Teenager Fool an AI? Evaluating Low-Cost Cosmetic Attacks on Age Estimation Systems",
    "url": "https://www.alphaxiv.org/abs/2602.19539v1",
    "arxiv_id": "2602.19539v1",
    "authors": "Xingyu Shen, Tommy Duong, Xiaodong An, Zengqi Zhao, Zebang Hu, Haoyu Hu, Ziyou Wang, Finn Guo, Simiao Ren",
    "categories": "cs.CV, cs.CR, cs.LG",
    "pub_date": "2026-02-23 06:13:52",
    "ori_summary": "Age estimation systems are increasingly deployed as gatekeepers for age-restricted online content, yet their robustness to cosmetic modifications has not been systematically evaluated. We investigate whether simple, household-accessible cosmetic changes, including beards, grey hair, makeup, and simulated wrinkles, can cause AI age estimators to classify minors as adults. To study this threat at scale without ethical concerns, we simulate these physical attacks on 329 facial images of individuals aged 10 to 21 using a VLM image editor (Gemini 2.5 Flash Image). We then evaluate eight models from our prior benchmark: five specialized architectures (MiVOLO, Custom-Best, Herosan, MiViaLab, DEX) and three vision-language models (Gemini 3 Flash, Gemini 2.5 Flash, GPT-5-Nano). We introduce the Attack Conversion Rate (ACR), defined as the fraction of images predicted as minor at baseline that flip to adult after attack, a population-agnostic metric that does not depend on the ratio of minors to adults in the test set. Our results reveal that a synthetic beard alone achieves 28 to 69 percent ACR across all eight models; combining all four attacks shifts predicted age by +7.7 years on average across all 329 subjects and reaches up to 83 percent ACR; and vision-language models exhibit lower ACR (59 to 71 percent) than specialized models (63 to 83 percent) under the full attack, although the ACR ranges overlap and the difference is not statistically tested. These findings highlight a critical vulnerability in deployed age-verification pipelines and call for adversarial robustness evaluation as a mandatory criterion for model selection.",
    "summary": "",
    "translation": "青少年能否愚弄AI？评估低成本化妆攻击对年龄估计系统的影响",
    "relevance_score": 1,
    "reasoning": "该论文主要关注计算机视觉中的年龄估计系统及其对抗性攻击，属于纯粹的视觉领域研究。虽然年龄估计在广告定向中可能有潜在应用，但论文焦点是安全性和对抗攻击，这属于被排除的“非技术性话题”（如安全、隐私），且没有明确涉及推荐系统、搜索或广告的核心技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19536v1": {
    "title": "Fore-Mamba3D: Mamba-based Foreground-Enhanced Encoding for 3D Object Detection",
    "url": "https://www.alphaxiv.org/abs/2602.19536v1",
    "arxiv_id": "2602.19536v1",
    "authors": "Zhiwei Ning, Xuanang Gao, Jiaxi Cao, Runze Yang, Huiying Xu, Xinzhong Zhu, Jie Yang, Wei Liu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-23 06:03:07",
    "ori_summary": "Linear modeling methods like Mamba have been merged as the effective backbone for the 3D object detection task. However, previous Mamba-based methods utilize the bidirectional encoding for the whole non-empty voxel sequence, which contains abundant useless background information in the scenes. Though directly encoding foreground voxels appears to be a plausible solution, it tends to degrade detection performance. We attribute this to the response attenuation and restricted context representation in the linear modeling for fore-only sequences. To address this problem, we propose a novel backbone, termed Fore-Mamba3D, to focus on the foreground enhancement by modifying Mamba-based encoder. The foreground voxels are first sampled according to the predicted scores. Considering the response attenuation existing in the interaction of foreground voxels across different instances, we design a regional-to-global slide window (RGSW) to propagate the information from regional split to the entire sequence. Furthermore, a semantic-assisted and state spatial fusion module (SASFMamba) is proposed to enrich contextual representation by enhancing semantic and geometric awareness within the Mamba model. Our method emphasizes foreground-only encoding and alleviates the distance-based and causal dependencies in the linear autoregression model. The superior performance across various benchmarks demonstrates the effectiveness of Fore-Mamba3D in the 3D object detection task.",
    "summary": "",
    "translation": "Fore-Mamba3D：基于Mamba架构的前景增强编码用于三维物体检测",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及3D物体检测和Mamba架构，属于计算机视觉领域。虽然Mamba架构（作为状态空间模型）是Transformer的一种替代方案，属于“使能Transformer技术”范畴，但论文明确聚焦于3D视觉应用，且未提及与推荐系统、搜索或广告的任何潜在关联。根据“无关主题”中“无明确相关性的纯视觉、3D视觉论文”的排除标准，其相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19530v1": {
    "title": "ORION: ORthonormal Text Encoding for Universal VLM AdaptatION",
    "url": "https://www.alphaxiv.org/abs/2602.19530v1",
    "arxiv_id": "2602.19530v1",
    "authors": "Omprakash Chakraborty, Jose Dolz, Ismail Ben Ayed",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 05:47:28",
    "ori_summary": "Vision language models (VLMs) have demonstrated remarkable generalization across diverse tasks, yet their performance remains constrained by the quality and geometry of the textual prototypes used to represent classes. Standard zero shot classifiers, derived from frozen text encoders and handcrafted prompts, may yield correlated or weakly separated embeddings that limit task specific discriminability. We introduce ORION, a text encoder fine tuning framework that improves pretrained VLMs using only class names. Our method optimizes, via low rank adaptation, a novel loss integrating two terms, one promoting pairwise orthogonality between the textual representations of the classes of a given task and the other penalizing deviations from the initial class prototypes. Furthermore, we provide a probabilistic interpretation of our orthogonality penalty, connecting it to the general maximum likelihood estimation (MLE) principle via Huygens theorem. We report extensive experiments on 11 benchmarks and three large VLM backbones, showing that the refined textual embeddings yield powerful replacements for the standard CLIP prototypes. Added as plug and play module on top of various state of the art methods, and across different prediction settings (zero shot, few shot and test time adaptation), ORION improves the performance consistently and significantly.",
    "summary": "该论文研究视觉语言模型中文本原型表示的质量和几何结构限制跨任务泛化能力的问题，其核心方法是提出一个通过低秩适配优化的文本编码微调框架，引入促进类间表示正交性和保持初始原型一致性的损失函数来改进预训练VLM。",
    "translation": "ORION：面向通用视觉语言模型适配的正交文本编码",
    "relevance_score": 8,
    "reasoning": "该论文提出正交文本编码方法用于视觉语言模型（VLM）的通用适配，属于“VLM类比用于异构数据”的焦点领域。正交编码技术可处理推荐/搜索中的异构特征（如用户序列、上下文特征），实现跨模态统一建模，在个性化推荐和多模态搜索中有直接应用潜力。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出的正交文本编码方法通过优化类原型表示直接提升VLM判别能力，其核心思想与处理异构数据模态的统一建模理念高度相关，但主要针对视觉语言任务而非推荐/搜索/广告的直接应用。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.19523v1": {
    "title": "OSInsert: Towards High-authenticity and High-fidelity Image Composition",
    "url": "https://www.alphaxiv.org/abs/2602.19523v1",
    "arxiv_id": "2602.19523v1",
    "authors": "Jingyuan Wang, Li Niu",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 05:25:05",
    "ori_summary": "Generative image composition aims to regenerate the given foreground object in the background image to produce a realistic composite image. Some high-authenticity methods can adjust foreground pose/view to be compatible with background, while some high-fidelity methods can preserve the foreground details accurately. However, existing methods can hardly achieve both goals at the same time. In this work, we propose a two-stage strategy to achieve both goals. In the first stage, we use high-authenticity method to generate reasonable foreground shape, serving as the condition of high-fidelity method in the second stage. The experiments on MureCOM dataset verify the effectiveness of our two-stage strategy. The code and model have been released at https://github.com/bcmi/OSInsert-Image-Composition.",
    "summary": "",
    "translation": "OSInsert：迈向高真实性与高保真度的图像合成",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于图像合成技术，属于计算机视觉领域。虽然提到了“高真实性”和“高保真度”，但未表明与推荐系统、搜索或广告有任何直接或间接的联系。根据您的关注点，这属于“不相关主题”中的“纯视觉论文，与RecSys/搜索/广告无明确相关性”。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19512v1": {
    "title": "Variational Trajectory Optimization of Anisotropic Diffusion Schedules",
    "url": "https://www.alphaxiv.org/abs/2602.19512v1",
    "arxiv_id": "2602.19512v1",
    "authors": "Pengxi Liu, Zeyu Michael Li, Xiang Cheng",
    "categories": "cs.LG, cs.CV",
    "pub_date": "2026-02-23 04:56:41",
    "ori_summary": "We introduce a variational framework for diffusion models with anisotropic noise schedules parameterized by a matrix-valued path $M_t(θ)$ that allocates noise across subspaces. Central to our framework is a trajectory-level objective that jointly trains the score network and learns $M_t(θ)$, which encompasses general parameterization classes of matrix-valued noise schedules. We further derive an estimator for the derivative with respect to $θ$ of the score that enables efficient optimization of the $M_t(θ)$ schedule. For inference, we develop an efficiently-implementable reverse-ODE solver that is an anisotropic generalization of the second-order Heun discretization algorithm. Across CIFAR-10, AFHQv2, FFHQ, and ImageNet-64, our method consistently improves upon the baseline EDM model in all NFE regimes. Code is available at https://github.com/lizeyu090312/anisotropic-diffusion-paper.",
    "summary": "",
    "translation": "各向异性扩散调度的变分轨迹优化",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及扩散模型和变分优化，属于生成模型领域。虽然扩散模型在推荐系统中可用于数据增强或生成用户序列，但标题未明确指向推荐/搜索/广告应用，且未涉及LLM、Transformer架构或异构数据统一建模等当前关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19506v1": {
    "title": "Relational Feature Caching for Accelerating Diffusion Transformers",
    "url": "https://www.alphaxiv.org/abs/2602.19506v1",
    "arxiv_id": "2602.19506v1",
    "authors": "Byunggwan Son, Jeimin Jeon, Jeongwoo Choi, Bumsub Ham",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-02-23 04:45:38",
    "ori_summary": "Feature caching approaches accelerate diffusion transformers (DiTs) by storing the output features of computationally expensive modules at certain timesteps, and exploiting them for subsequent steps to reduce redundant computations. Recent forecasting-based caching approaches employ temporal extrapolation techniques to approximate the output features with cached ones. Although effective, relying exclusively on temporal extrapolation still suffers from significant prediction errors, leading to performance degradation. Through a detailed analysis, we find that 1) these errors stem from the irregular magnitude of changes in the output features, and 2) an input feature of a module is strongly correlated with the corresponding output. Based on this, we propose relational feature caching (RFC), a novel framework that leverages the input-output relationship to enhance the accuracy of the feature prediction. Specifically, we introduce relational feature estimation (RFE) to estimate the magnitude of changes in the output features from the inputs, enabling more accurate feature predictions. We also present relational cache scheduling (RCS), which estimates the prediction errors using the input features and performs full computations only when the errors are expected to be substantial. Extensive experiments across various DiT models demonstrate that RFC consistently outperforms prior approaches significantly. Project page is available at https://cvlab.yonsei.ac.kr/projects/RFC",
    "summary": "该论文研究扩散Transformer中特征缓存方法的预测误差问题，核心思想是利用模块输入与输出之间的强相关性，通过关系特征估计来预测输出特征的变化幅度，从而实现更准确的特征预测和缓存调度。",
    "translation": "用于加速扩散变换器的关系特征缓存",
    "relevance_score": 4,
    "reasoning": "该论文涉及Transformer架构的效率优化（缓存机制），这属于'Enabling Transformer Tech'范畴。虽然扩散模型主要用于生成任务，但高效的Transformer缓存技术可潜在应用于推荐/搜索系统中的序列建模，以加速推理过程。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出了一种加速扩散Transformer的新框架，通过利用输入-输出关系来提升特征预测精度，这直接属于Transformer架构效率提升的核心技术领域。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.19505v1": {
    "title": "Test-Time Computing for Referring Multimodal Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.19505v1",
    "arxiv_id": "2602.19505v1",
    "authors": "Mingrui Wu, Hao Chen, Jiayi Ji, Xiaoshuai Sun, Zhiyuan Liu, Liujuan Cao, Ming-Ming Cheng, Rongrong Ji",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 04:42:10",
    "ori_summary": "We propose ControlMLLM++, a novel test-time adaptation framework that injects learnable visual prompts into frozen multimodal large language models (MLLMs) to enable fine-grained region-based visual reasoning without any model retraining or fine-tuning. Leveraging the insight that cross-modal attention maps intrinsically encode semantic correspondences between textual tokens and visual regions, ControlMLLM++ optimizes a latent visual token modifier during inference via a task-specific energy function to steer model attention towards user-specified areas. To enhance optimization stability and mitigate language prompt biases, ControlMLLM++ incorporates an improved optimization strategy (Optim++) and a prompt debiasing mechanism (PromptDebias). Supporting diverse visual prompt types including bounding boxes, masks, scribbles, and points, our method demonstrates strong out-of-domain generalization and interpretability. The code is available at https://github.com/mrwu-mac/ControlMLLM.",
    "summary": "",
    "translation": "用于指代多模态大语言模型的测试时计算",
    "relevance_score": 3,
    "reasoning": "该论文涉及多模态大语言模型（MLLM）的测试时计算优化，属于核心LLM技术进展。虽然测试时计算技术可能提升模型效率，但论文标题未明确其与推荐系统、搜索或广告的具体应用关联，且多模态特性主要针对视觉-语言任务，而非您关注的异构数据统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19503v1": {
    "title": "A Text-Guided Vision Model for Enhanced Recognition of Small Instances",
    "url": "https://www.alphaxiv.org/abs/2602.19503v1",
    "arxiv_id": "2602.19503v1",
    "authors": "Hyun-Ki Jung",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 04:40:14",
    "ori_summary": "As drone-based object detection technology continues to evolve, the demand is shifting from merely detecting objects to enabling users to accurately identify specific targets. For example, users can input particular targets as prompts to precisely detect desired objects. To address this need, an efficient text-guided object detection model has been developed to enhance the detection of small objects. Specifically, an improved version of the existing YOLO-World model is introduced. The proposed method replaces the C2f layer in the YOLOv8 backbone with a C3k2 layer, enabling more precise representation of local features, particularly for small objects or those with clearly defined boundaries. Additionally, the proposed architecture improves processing speed and efficiency through parallel processing optimization, while also contributing to a more lightweight model design. Comparative experiments on the VisDrone dataset show that the proposed model outperforms the original YOLO-World model, with precision increasing from 40.6% to 41.6%, recall from 30.8% to 31%, F1 score from 35% to 35.5%, and mAP@0.5 from 30.4% to 30.7%, confirming its enhanced accuracy. Furthermore, the model demonstrates superior lightweight performance, with the parameter count reduced from 4 million to 3.8 million and FLOPs decreasing from 15.7 billion to 15.2 billion. These results indicate that the proposed approach provides a practical and effective solution for precise object detection in drone-based applications.",
    "summary": "",
    "translation": "一种用于增强小实例识别的文本引导视觉模型",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的小实例识别问题，虽然涉及多模态（文本引导视觉），但未明确展示与推荐系统、搜索或广告的相关性。标题未提及序列建模、用户行为、上下文特征或排名等核心概念，因此潜在应用不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19497v1": {
    "title": "MICON-Bench: Benchmarking and Enhancing Multi-Image Context Image Generation in Unified Multimodal Models",
    "url": "https://www.alphaxiv.org/abs/2602.19497v1",
    "arxiv_id": "2602.19497v1",
    "authors": "Mingrui Wu, Hang Liu, Jiayi Ji, Xiaoshuai Sun, Rongrong Ji",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 04:32:52",
    "ori_summary": "Recent advancements in Unified Multimodal Models (UMMs) have enabled remarkable image understanding and generation capabilities. However, while models like Gemini-2.5-Flash-Image show emerging abilities to reason over multiple related images, existing benchmarks rarely address the challenges of multi-image context generation, focusing mainly on text-to-image or single-image editing tasks. In this work, we introduce \\textbf{MICON-Bench}, a comprehensive benchmark covering six tasks that evaluate cross-image composition, contextual reasoning, and identity preservation. We further propose an MLLM-driven Evaluation-by-Checkpoint framework for automatic verification of semantic and visual consistency, where multimodal large language model (MLLM) serves as a verifier. Additionally, we present \\textbf{Dynamic Attention Rebalancing (DAR)}, a training-free, plug-and-play mechanism that dynamically adjusts attention during inference to enhance coherence and reduce hallucinations. Extensive experiments on various state-of-the-art open-source models demonstrate both the rigor of MICON-Bench in exposing multi-image reasoning challenges and the efficacy of DAR in improving generation quality and cross-image coherence. Github: https://github.com/Angusliuuu/MICON-Bench.",
    "summary": "",
    "translation": "MICON-Bench：统一多模态模型中多图像上下文图像生成的基准测试与增强",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多图像上下文图像生成的基准测试，属于视觉内容生成领域，与AIGC/内容生成高度相关。虽然涉及多模态模型，但其核心是图像生成评估，没有明确展示在推荐系统、搜索或广告中的潜在应用，因此与当前关注点相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19487v1": {
    "title": "Exploiting Label-Independent Regularization from Spatial Dependencies for Whole Slide Image Analysis",
    "url": "https://www.alphaxiv.org/abs/2602.19487v1",
    "arxiv_id": "2602.19487v1",
    "authors": "Weiyi Wu, Xinwen Xu, Chongyang Gao, Xingjian Diao, Siting Li, Jiang Gui",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 04:07:08",
    "ori_summary": "Whole slide images, with their gigapixel-scale panoramas of tissue samples, are pivotal for precise disease diagnosis. However, their analysis is hindered by immense data size and scarce annotations. Existing MIL methods face challenges due to the fundamental imbalance where a single bag-level label must guide the learning of numerous patch-level features. This sparse supervision makes it difficult to reliably identify discriminative patches during training, leading to unstable optimization and suboptimal solutions. We propose a spatially regularized MIL framework that leverages inherent spatial relationships among patch features as label-independent regularization signals. Our approach learns a shared representation space by jointly optimizing feature-induced spatial reconstruction and label-guided classification objectives, enforcing consistency between intrinsic structural patterns and supervisory signals. Experimental results on multiple public datasets demonstrate significant improvements over state-of-the-art methods, offering a promising direction.",
    "summary": "",
    "translation": "利用空间依赖性中的标签无关正则化进行全切片图像分析",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学图像分析（全切片图像），属于医学/生物学领域，与推荐系统、搜索或广告无关。虽然提到了正则化技术，但应用场景完全在医疗领域，没有展示与推荐系统、搜索或广告的潜在联系。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19474v1": {
    "title": "Structured Bitmap-to-Mesh Triangulation for Geometry-Aware Discretization of Image-Derived Domains",
    "url": "https://www.alphaxiv.org/abs/2602.19474v1",
    "arxiv_id": "2602.19474v1",
    "authors": "Wei Feng, Haiyong Zheng",
    "categories": "cs.CG, cs.CV, cs.GR",
    "pub_date": "2026-02-23 03:36:55",
    "ori_summary": "We propose a template-driven triangulation framework that embeds raster- or segmentation-derived boundaries into a regular triangular grid for stable PDE discretization on image-derived domains. Unlike constrained Delaunay triangulation (CDT), which may trigger global connectivity updates, our method retriangulates only triangles intersected by the boundary, preserves the base mesh, and supports synchronization-free parallel execution. To ensure determinism and scalability, we classify all local boundary-intersection configurations up to discrete equivalence and triangle symmetries, yielding a finite symbolic lookup table that maps each case to a conflict-free retriangulation template. We prove that the resulting mesh is closed, has bounded angles, and is compatible with cotangent-based discretizations and standard finite element methods. Experiments on elliptic and parabolic PDEs, signal interpolation, and structural metrics show fewer sliver elements, more regular triangles, and improved geometric fidelity near complex boundaries. The framework is well suited for real-time geometric analysis and physically based simulation over image-derived domains.",
    "summary": "",
    "translation": "面向图像衍生域几何感知离散化的结构化位图到网格三角剖分",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及计算机图形学中的网格生成和图像处理技术，专注于几何离散化方法。虽然提到了图像衍生域，但其核心是图形学中的三角剖分算法，与推荐系统、搜索或广告的当前关注点（如LLM技术、Transformer架构、异构数据建模等）没有直接关联。该技术主要应用于计算机视觉和图形渲染领域，不具备在推荐/搜索/广告系统中的明显应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19471v1": {
    "title": "Forgetting-Resistant and Lesion-Aware Source-Free Domain Adaptive Fundus Image Analysis with Vision-Language Model",
    "url": "https://www.alphaxiv.org/abs/2602.19471v1",
    "arxiv_id": "2602.19471v1",
    "authors": "Zheang Huai, Hui Tang, Hualiang Wang, Xiaomeng Li",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 03:29:54",
    "ori_summary": "Source-free domain adaptation (SFDA) aims to adapt a model trained in the source domain to perform well in the target domain, with only unlabeled target domain data and the source model. Taking into account that conventional SFDA methods are inevitably error-prone under domain shift, recently greater attention has been directed to SFDA assisted with off-the-shelf foundation models, e.g., vision-language (ViL) models. However, existing works of leveraging ViL models for SFDA confront two issues: (i) Although mutual information is exploited to consider the joint distribution between the predictions of ViL model and the target model, we argue that the forgetting of some superior predictions of the target model still occurs, as indicated by the decline of the accuracies of certain classes during adaptation; (ii) Prior research disregards the rich, fine-grained knowledge embedded in the ViL model, which offers detailed grounding for fundus image diagnosis. In this paper, we introduce a novel forgetting-resistant and lesion-aware (FRLA) method for SFDA of fundus image diagnosis with ViL model. Specifically, a forgetting-resistant adaptation module explicitly preserves the confident predictions of the target model, and a lesion-aware adaptation module yields patch-wise predictions from ViL model and employs them to help the target model be aware of the lesion areas and leverage the ViL model's fine-grained knowledge. Extensive experiments show that our method not only significantly outperforms the vision-language model, but also achieves consistent improvements over the state-of-the-art methods. Our code will be released.",
    "summary": "",
    "translation": "基于视觉语言模型的抗遗忘与病灶感知的无源域自适应眼底图像分析",
    "relevance_score": 2,
    "reasoning": "该论文主要涉及医学图像分析（眼底图像）和域自适应技术，属于明确的医学领域应用。虽然提到了视觉语言模型，但这是针对特定医疗场景的，没有展示在推荐系统、搜索或广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19470v1": {
    "title": "Physics-informed Active Polarimetric 3D Imaging for Specular Surfaces",
    "url": "https://www.alphaxiv.org/abs/2602.19470v1",
    "arxiv_id": "2602.19470v1",
    "authors": "Jiazhang Wang, Hyelim Yang, Tianyi Wang, Florian Willomitzer",
    "categories": "cs.CV, physics.optics",
    "pub_date": "2026-02-23 03:28:41",
    "ori_summary": "3D imaging of specular surfaces remains challenging in real-world scenarios, such as in-line inspection or hand-held scanning, requiring fast and accurate measurement of complex geometries. Optical metrology techniques such as deflectometry achieve high accuracy but typically rely on multi-shot acquisition, making them unsuitable for dynamic environments. Fourier-based single-shot approaches alleviate this constraint, yet their performance deteriorates when measuring surfaces with high spatial frequency structure or large curvature. Alternatively, polarimetric 3D imaging in computer vision operates in a single-shot fashion and exhibits robustness to geometric complexity. However, its accuracy is fundamentally limited by the orthographic imaging assumption. In this paper, we propose a physics-informed deep learning framework for single-shot 3D imaging of complex specular surfaces. Polarization cues provide orientation priors that assist in interpreting geometric information encoded by structured illumination. These complementary cues are processed through a dual-encoder architecture with mutual feature modulation, allowing the network to resolve their nonlinear coupling and directly infer surface normals. The proposed method achieves accurate and robust normal estimation in single-shot with fast inference, enabling practical 3D imaging of complex specular surfaces.",
    "summary": "",
    "translation": "基于物理信息的主动偏振三维成像技术用于镜面表面",
    "relevance_score": 1,
    "reasoning": "该论文涉及物理信息、偏振成像和镜面表面处理，属于计算机视觉和图形学领域。虽然标题中提到'3D成像'，但核心内容与推荐系统、搜索或广告的异构数据建模、Transformer架构或LLM应用无关。该技术主要针对特定视觉问题（镜面表面成像），没有明确的RecSys/Search/Ads应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19461v1": {
    "title": "Laplacian Multi-scale Flow Matching for Generative Modeling",
    "url": "https://www.alphaxiv.org/abs/2602.19461v1",
    "arxiv_id": "2602.19461v1",
    "authors": "Zelin Zhao, Petr Molodyk, Haotian Xue, Yongxin Chen",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-02-23 03:09:56",
    "ori_summary": "In this paper, we present Laplacian multiscale flow matching (LapFlow), a novel framework that enhances flow matching by leveraging multi-scale representations for image generative modeling. Our approach decomposes images into Laplacian pyramid residuals and processes different scales in parallel through a mixture-of-transformers (MoT) architecture with causal attention mechanisms. Unlike previous cascaded approaches that require explicit renoising between scales, our model generates multi-scale representations in parallel, eliminating the need for bridging processes. The proposed multi-scale architecture not only improves generation quality but also accelerates the sampling process and promotes scaling flow matching methods. Through extensive experimentation on CelebA-HQ and ImageNet, we demonstrate that our method achieves superior sample quality with fewer GFLOPs and faster inference compared to single-scale and multi-scale flow matching baselines. The proposed model scales effectively to high-resolution generation (up to 1024$\\times$1024) while maintaining lower computational overhead.",
    "summary": "",
    "translation": "用于生成建模的拉普拉斯多尺度流匹配",
    "relevance_score": 3,
    "reasoning": "该论文提出了一种新的生成建模方法，属于核心LLM技术中的生成模型进展，可能对推荐系统或广告中的内容生成有潜在应用。然而，标题未明确提及推荐系统、搜索或广告的具体应用，且生成建模本身可能偏向AIGC等非核心领域，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19454v1": {
    "title": "HD-TTA: Hypothesis-Driven Test-Time Adaptation for Safer Brain Tumor Segmentation",
    "url": "https://www.alphaxiv.org/abs/2602.19454v1",
    "arxiv_id": "2602.19454v1",
    "authors": "Kartik Jhawar, Lipo Wang",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 02:53:05",
    "ori_summary": "Standard Test-Time Adaptation (TTA) methods typically treat inference as a blind optimization task, applying generic objectives to all or filtered test samples. In safety-critical medical segmentation, this lack of selectivity often causes the tumor mask to spill into healthy brain tissue or degrades predictions that were already correct. We propose Hypothesis-Driven TTA, a novel framework that reformulates adaptation as a dynamic decision process. Rather than forcing a single optimization trajectory, our method generates intuitive competing geometric hypotheses: compaction (is the prediction noisy? trim artifacts) versus inflation (is the valid tumor under-segmented? safely inflate to recover). It then employs a representation-guided selector to autonomously identify the safest outcome based on intrinsic texture consistency. Additionally, a pre-screening Gatekeeper prevents negative transfer by skipping adaptation on confident cases. We validate this proof-of-concept on a cross-domain binary brain tumor segmentation task, applying a source model trained on adult BraTS gliomas to unseen pediatric and more challenging meningioma target domains. HD-TTA improves safety-oriented outcomes (Hausdorff Distance (HD95) and Precision) over several state-of-the-art representative baselines in the challenging safety regime, reducing the HD95 by approximately 6.4 mm and improving Precision by over 4%, while maintaining comparable Dice scores. These results demonstrate that resolving the safety-adaptation trade-off via explicit hypothesis selection is a viable, robust path for safe clinical model deployment. Code will be made publicly available upon acceptance.",
    "summary": "",
    "translation": "HD-TTA：基于假设的测试时自适应用于更安全的脑肿瘤分割",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及医学领域（脑肿瘤分割）和测试时自适应技术，这属于明确的无关主题（医学/生物学领域特定应用）。标题中没有任何元素表明与推荐系统、搜索、广告、LLM技术或Transformer架构相关，也没有展示出将这些技术应用于RecSys/Search/Ads领域的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19449v1": {
    "title": "Decoupling Vision and Language: Codebook Anchored Visual Adaptation",
    "url": "https://www.alphaxiv.org/abs/2602.19449v1",
    "arxiv_id": "2602.19449v1",
    "authors": "Jason Wu, Tianchen Zhao, Chang Liu, Jiarui Cai, Zheng Zhang, Zhuowei Li, Aaditya Singh, Xiang Xu, Mani Srivastava, Jonathan Wu",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 02:39:26",
    "ori_summary": "Large Vision-Language Models (LVLMs) use their vision encoders to translate images into representations for downstream reasoning, but the encoders often underperform in domain-specific visual tasks such as medical image diagnosis or fine-grained classification, where representation errors can cascade through the language model, leading to incorrect responses. Existing adaptation methods modify the continuous feature interface between encoder and language model through projector tuning or other parameter-efficient updates, which still couples the two components and requires re-alignment whenever the encoder changes. We introduce CRAFT (Codebook RegulAted Fine-Tuning), a lightweight method that fine-tunes the encoder using a discrete codebook that anchors visual representations to a stable token space, achieving domain adaptation without modifying other parts of the model. This decoupled design allows the adapted encoder to seamlessly boost the performance of LVLMs with different language architectures, as long as they share the same codebook. Empirically, CRAFT achieves an average gain of 13.51% across 10 domain-specific benchmarks such as VQARAD and PlantVillage, while preserving the LLM's linguistic capabilities and outperforming peer methods that operate on continuous tokens.",
    "summary": "",
    "translation": "解耦视觉与语言：基于码本锚定的视觉适配",
    "relevance_score": 2,
    "reasoning": "该论文标题表明其核心是视觉-语言模型中的视觉组件适配技术，属于VLM领域。虽然您关注VLM类比处理异构数据的思路，但该论文聚焦于视觉模态的专门适配，缺乏明确的推荐/搜索/广告应用潜力说明，且可能更偏向纯视觉技术而非跨模态统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19442v1": {
    "title": "UrbanAlign: Post-hoc Semantic Calibration for VLM-Human Preference Alignment",
    "url": "https://www.alphaxiv.org/abs/2602.19442v1",
    "arxiv_id": "2602.19442v1",
    "authors": "Yecheng Zhang, Rong Zhao, Zhizhou Sha, Yong Li, Lei Wang, Ce Hou, Wen Ji, Hao Huang, Yunshan Wan, Jian Yu, Junhao Xia, Yuru Zhang, Chunlei Shi",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 02:24:55",
    "ori_summary": "Aligning vision-language model (VLM) outputs with human preferences in domain-specific tasks typically requires fine-tuning or reinforcement learning, both of which demand labelled data and GPU compute. We show that for subjective perception tasks, this alignment can be achieved without any model training: VLMs are already strong concept extractors but poor decision calibrators, and the gap can be closed externally. We propose a training-free post-hoc concept-bottleneck pipeline consisting of three tightly coupled stages: concept mining, multi-agent structured scoring, and geometric calibration, unified by an end-to-end dimension optimization loop. Interpretable evaluation dimensions are mined from a handful of human annotations; an Observer-Debater-Judge chain extracts robust continuous concept scores from a frozen VLM; and locally-weighted ridge regression on a hybrid visual-semantic manifold calibrates these scores against human ratings. Applied to urban perception as UrbanAlign, the framework achieves 72.2% accuracy ($κ=0.45$) on Place Pulse 2.0 across six categories, outperforming the best supervised baseline by +15.1 pp and uncalibrated VLM scoring by +16.3 pp, with full dimension-level interpretability and zero model-weight modification.",
    "summary": "",
    "translation": "UrbanAlign：面向视觉语言模型与人类偏好对齐的事后语义校准",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及视觉语言模型（VLM）与人类偏好的对齐校准，属于VLM技术范畴。虽然提到了“对齐”概念，但未明确展示与推荐系统、搜索或广告领域的直接关联或潜在应用，且主要聚焦于VLM而非异构数据建模的通用方法，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19437v1": {
    "title": "FinSight-Net:A Physics-Aware Decoupled Network with Frequency-Domain Compensation for Underwater Fish Detection in Smart Aquaculture",
    "url": "https://www.alphaxiv.org/abs/2602.19437v1",
    "arxiv_id": "2602.19437v1",
    "authors": "Jinsong Yang, Zeyuan Hu, Yichen Li, Hong Yu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-23 02:12:47",
    "ori_summary": "Underwater fish detection (UFD) is a core capability for smart aquaculture and marine ecological monitoring. While recent detectors improve accuracy by stacking feature extractors or introducing heavy attention modules, they often incur substantial computational overhead and, more importantly, neglect the physics that fundamentally limits UFD: wavelength-dependent absorption and turbidity-induced scattering significantly degrade contrast, blur fine structures, and introduce backscattering noise, leading to unreliable localization and recognition. To address these challenges, we propose FinSight-Net, an efficient and physics-aware detection framework tailored for complex aquaculture environments. FinSight-Net introduces a Multi-Scale Decoupled Dual-Stream Processing (MS-DDSP) bottleneck that explicitly targets frequency-specific information loss via heterogeneous convolutional branches, suppressing backscattering artifacts while compensating distorted biological cues through scale-aware and channel-weighted pathways. We further design an Efficient Path Aggregation FPN (EPA-FPN) as a detail-filling mechanism: it restores high-frequency spatial information typically attenuated in deep layers by establishing long-range skip connections and pruning redundant fusion routes, enabling robust detection of non-rigid fish targets under severe blur and turbidity. Extensive experiments on DeepFish, AquaFishSet, and our challenging UW-BlurredFish benchmark demonstrate that FinSight-Net achieves state-of-the-art performance. In particular, on UW-BlurredFish, FinSight-Net reaches 92.8% mAP, outperforming YOLOv11s by 4.8% while reducing parameters by 29.0%, providing a strong and lightweight solution for real-time automated monitoring in smart aquaculture.",
    "summary": "",
    "translation": "FinSight-Net：一种用于智能水产养殖中水下鱼类检测的物理感知解耦网络，具有频域补偿功能",
    "relevance_score": 1,
    "reasoning": "该论文专注于水下鱼类检测的计算机视觉应用，属于纯粹的视觉领域，与推荐系统、搜索或广告的核心技术无直接关联。标题中提到的物理感知网络和频域补偿技术是针对特定视觉任务的优化，没有显示出在异构数据处理、Transformer架构或LLM应用方面的潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19432v1": {
    "title": "CountEx: Fine-Grained Counting via Exemplars and Exclusion",
    "url": "https://www.alphaxiv.org/abs/2602.19432v1",
    "arxiv_id": "2602.19432v1",
    "authors": "Yifeng Huang, Gia Khanh Nguyen, Minh Hoai",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 02:01:44",
    "ori_summary": "This paper presents CountEx, a discriminative visual counting framework designed to address a key limitation of existing prompt-based methods: the inability to explicitly exclude visually similar distractors. While current approaches allow users to specify what to count via inclusion prompts, they often struggle in cluttered scenes with confusable object categories, leading to ambiguity and overcounting. CountEx enables users to express both inclusion and exclusion intent, specifying what to count and what to ignore, through multimodal prompts including natural language descriptions and optional visual exemplars. At the core of CountEx is a novel Discriminative Query Refinement module, which jointly reasons over inclusion and exclusion cues by first identifying shared visual features, then isolating exclusion-specific patterns, and finally applying selective suppression to refine the counting query. To support systematic evaluation of fine-grained counting methods, we introduce CoCount, a benchmark comprising 1,780 videos and 10,086 annotated frames across 97 category pairs. Experiments show that CountEx achieves substantial improvements over state-of-the-art methods for counting objects from both known and novel categories. The data and code are available at https://github.com/bbvisual/CountEx.",
    "summary": "",
    "translation": "CountEx：通过范例与排除实现细粒度计数",
    "relevance_score": 1,
    "reasoning": "该论文标题表明其专注于计算机视觉中的细粒度计数任务，这属于纯粹的视觉领域研究。虽然计数技术理论上可能应用于某些推荐或广告场景（如库存管理），但标题未显示与推荐系统、搜索或广告的直接关联，也未涉及LLM、Transformer架构或多模态建模等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19430v1": {
    "title": "TherA: Thermal-Aware Visual-Language Prompting for Controllable RGB-to-Thermal Infrared Translation",
    "url": "https://www.alphaxiv.org/abs/2602.19430v1",
    "arxiv_id": "2602.19430v1",
    "authors": "Dong-Guw Lee, Tai Hyoung Rhee, Hyunsoo Jang, Young-Sik Shin, Ukcheol Shin, Ayoung Kim",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 01:56:29",
    "ori_summary": "Despite the inherent advantages of thermal infrared(TIR) imaging, large-scale data collection and annotation remain a major bottleneck for TIR-based perception. A practical alternative is to synthesize pseudo TIR data via image translation; however, most RGB-to-TIR approaches heavily rely on RGB-centric priors that overlook thermal physics, yielding implausible heat distributions. In this paper, we introduce TherA, a controllable RGB-to-TIR translation framework that produces diverse and thermally plausible images at both scene and object level. TherA couples TherA-VLM with a latent-diffusion-based translator. Given a single RGB image and a user-prompted condition pair, TherA-VLM yields a thermal-aware embedding that encodes scene, object, material, and heat-emission context reflecting the input scene-condition pair. Conditioning the diffusion model on this embedding enables realistic TIR synthesis and fine-grained control across time of day, weather, and object state. Compared to other baselines, TherA achieves state-of-the-art translation performance, demonstrating improved zero-shot translation performance up to 33% increase averaged across all metrics.",
    "summary": "",
    "translation": "TherA：用于可控RGB到热红外图像转换的热感知视觉-语言提示方法",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域中的RGB到热红外图像转换任务，属于纯粹的视觉模态处理。虽然标题中提到“视觉-语言提示”，但其核心是热红外图像生成而非推荐/搜索/广告应用。该技术没有明显的推荐系统、搜索或广告应用潜力，与用户行为建模、内容排序或广告投放等核心任务无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19424v1": {
    "title": "Hepato-LLaVA: An Expert MLLM with Sparse Topo-Pack Attention for Hepatocellular Pathology Analysis on Whole Slide Images",
    "url": "https://www.alphaxiv.org/abs/2602.19424v1",
    "arxiv_id": "2602.19424v1",
    "authors": "Yuxuan Yang, Zhonghao Yan, Yi Zhang, Bo Yun, Muxi Diao, Guowei Zhao, Kongming Liang, Wenbin Li, Zhanyu Ma",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 01:43:32",
    "ori_summary": "Hepatocellular Carcinoma diagnosis relies heavily on the interpretation of gigapixel Whole Slide Images. However, current computational approaches are constrained by fixed-resolution processing mechanisms and inefficient feature aggregation, which inevitably lead to either severe information loss or high feature redundancy. To address these challenges, we propose Hepato-LLaVA, a specialized Multi-modal Large Language Model designed for fine-grained hepatocellular pathology analysis. We introduce a novel Sparse Topo-Pack Attention mechanism that explicitly models 2D tissue topology. This mechanism effectively aggregates local diagnostic evidence into semantic summary tokens while preserving global context. Furthermore, to overcome the lack of multi-scale data, we present HepatoPathoVQA, a clinically grounded dataset comprising 33K hierarchically structured question-answer pairs validated by expert pathologists. Our experiments demonstrate that Hepato-LLaVA achieves state-of-the-art performance on HCC diagnosis and captioning tasks, significantly outperforming existing methods. Our code and implementation details are available at https://pris-cv.github.io/Hepto-LLaVA/.",
    "summary": "",
    "translation": "Hepato-LLaVA：一种采用稀疏拓扑包注意力的专家多模态大语言模型，用于全切片图像上的肝细胞癌病理分析",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医学领域（肝细胞癌病理分析）和视觉模态（全切片图像），属于明确的医学应用。虽然涉及多模态大语言模型（MLLM）技术，但缺乏与推荐系统、搜索或广告领域的潜在应用关联，完全属于“Irrelevant Topics”中排除的医学领域应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.19423v1": {
    "title": "Prefer-DAS: Learning from Local Preferences and Sparse Prompts for Domain Adaptive Segmentation of Electron Microscopy",
    "url": "https://www.alphaxiv.org/abs/2602.19423v1",
    "arxiv_id": "2602.19423v1",
    "authors": "Jiabao Chen, Shan Xiong, Jialin Peng",
    "categories": "cs.CV",
    "pub_date": "2026-02-23 01:39:03",
    "ori_summary": "Domain adaptive segmentation (DAS) is a promising paradigm for delineating intracellular structures from various large-scale electron microscopy (EM) without incurring extensive annotated data in each domain. However, the prevalent unsupervised domain adaptation (UDA) strategies often demonstrate limited and biased performance, which hinders their practical applications. In this study, we explore sparse points and local human preferences as weak labels in the target domain, thereby presenting a more realistic yet annotation-efficient setting. Specifically, we develop Prefer-DAS, which pioneers sparse promptable learning and local preference alignment. The Prefer-DAS is a promptable multitask model that integrates self-training and prompt-guided contrastive learning. Unlike SAM-like methods, the Prefer-DAS allows for the use of full, partial, and even no point prompts during both training and inference stages and thus enables interactive segmentation. Instead of using image-level human preference alignment for segmentation, we introduce Local direct Preference Optimization (LPO) and sparse LPO (SLPO), plug-and-play solutions for alignment with spatially varying human feedback or sparse feedback. To address potential missing feedback, we also introduce Unsupervised Preference Optimization (UPO), which leverages self-learned preferences. As a result, the Prefer-DAS model can effectively perform both weakly-supervised and unsupervised DAS, depending on the availability of points and human preferences. Comprehensive experiments on four challenging DAS tasks demonstrate that our model outperforms SAM-like methods as well as unsupervised and weakly-supervised DAS methods in both automatic and interactive segmentation modes, highlighting strong generalizability and flexibility. Additionally, the performance of our model is very close to or even exceeds that of supervised models.",
    "summary": "",
    "translation": "Prefer-DAS：基于局部偏好和稀疏提示学习用于电子显微镜图像的自适应分割",
    "relevance_score": 1,
    "reasoning": "该论文专注于电子显微镜图像的领域自适应分割，属于医学/生物学领域的计算机视觉应用。虽然涉及迁移学习和领域适应技术，但这些技术主要针对特定医学图像模态，与推荐系统、搜索或广告中的异构数据处理没有直接关联。论文标题明确指向电子显微镜这一特定领域，属于明确的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  }
}