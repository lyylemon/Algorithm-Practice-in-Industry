{
  "2602.08917v1": {
    "title": "Automatic In-Domain Exemplar Construction and LLM-Based Refinement of Multi-LLM Expansions for Query Expansion",
    "url": "https://www.alphaxiv.org/abs/2602.08917v1",
    "arxiv_id": "2602.08917v1",
    "authors": "Minghan Li, Ercong Nie, Siqi Zhao, Tongna Chen, Huiping Huang, Guodong Zhou",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2026-02-09 17:16:39",
    "ori_summary": "Query expansion with large language models is promising but often relies on hand-crafted prompts, manually chosen exemplars, or a single LLM, making it non-scalable and sensitive to domain shift. We present an automated, domain-adaptive QE framework that builds in-domain exemplar pools by harvesting pseudo-relevant passages using a BM25-MonoT5 pipeline. A training-free cluster-based strategy selects diverse demonstrations, yielding strong and stable in-context QE without supervision. To further exploit model complementarity, we introduce a two-LLM ensemble in which two heterogeneous LLMs independently generate expansions and a refinement LLM consolidates them into one coherent expansion. Across TREC DL20, DBPedia, and SciFact, the refined ensemble delivers consistent and statistically significant gains over BM25, Rocchio, zero-shot, and fixed few-shot baselines. The framework offers a reproducible testbed for exemplar selection and multi-LLM generation, and a practical, label-free solution for real-world QE.",
    "summary": "该论文研究如何实现无需人工干预、可适应不同领域的自动化查询扩展。其核心方法是：首先通过BM25-MonoT5流水线自动构建领域内示例池，并采用无监督聚类策略选择多样化演示；然后设计了一个两阶段多LLM集成框架，让异构LLM独立生成扩展，再由精炼LLM将其整合为连贯的扩展查询。",
    "translation": "基于多LLM扩展的查询扩展自动领域内范例构建与LLM精炼",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及查询扩展（Search领域核心任务），结合了多LLM扩展和LLM精炼技术，属于'直接LLM应用'和'核心领域进展'范畴。LLM在查询扩展中的自动范例构建和精炼可直接提升搜索相关性，具有明确的Search应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对搜索领域核心问题，提出了自动化领域自适应查询扩展框架，结合了多LLM集成和精炼机制，与用户关注的LLM应用、Transformer技术及搜索领域进展高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08896v1": {
    "title": "OmniReview: A Large-scale Benchmark and LLM-enhanced Framework for Realistic Reviewer Recommendation",
    "url": "https://www.alphaxiv.org/abs/2602.08896v1",
    "arxiv_id": "2602.08896v1",
    "authors": "Yehua Huang, Penglei Sun, Zebin Chen, Zhenheng Tang, Xiaowen Chu",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2026-02-09 16:57:35",
    "ori_summary": "Academic peer review remains the cornerstone of scholarly validation, yet the field faces some challenges in data and methods. From the data perspective, existing research is hindered by the scarcity of large-scale, verified benchmarks and oversimplified evaluation metrics that fail to reflect real-world editorial workflows. To bridge this gap, we present OmniReview, a comprehensive dataset constructed by integrating multi-source academic platforms encompassing comprehensive scholarly profiles through the disambiguation pipeline, yielding 202, 756 verified review records. Based on this data, we introduce a three-tier hierarchical evaluaion framework to assess recommendations from recall to precise expert identification. From the method perspective, existing embedding-based approaches suffer from the information bottleneck of semantic compression and limited interpretability. To resolve these method limitations, we propose Profiling Scholars with Multi-gate Mixture-of-Experts (Pro-MMoE), a novel framework that synergizes Large Language Models (LLMs) with Multi-task Learning. Specifically, it utilizes LLM-generated semantic profiles to preserve fine-grained expertise nuances and interpretability, while employing a Task-Adaptive MMoE architecture to dynamically balance conflicting evaluation goals. Comprehensive experiments demonstrate that Pro-MMoE achieves state-of-the-art performance across six of seven metrics, establishing a new benchmark for realistic reviewer recommendation.",
    "summary": "该论文研究学术同行评审中的审稿人推荐问题，核心思想是利用LLM生成细粒度语义档案来保留专家细微差异和可解释性，同时采用任务自适应多门混合专家架构动态平衡冲突的评估目标。",
    "translation": "OmniReview：一个面向现实评论者推荐的大规模基准与LLM增强框架",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及推荐系统（RecSys）中的具体应用场景——评论者推荐，属于核心领域进展范畴。标题明确提及LLM增强框架，这属于直接LLM应用方向，通过大语言模型技术改进推荐系统的实际功能，具有明确的实践价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接应用LLM技术解决推荐系统问题，提出结合LLM生成语义档案与多门混合专家架构的创新框架，完全符合直接LLM应用和推荐系统核心进展的关注点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08886v1": {
    "title": "Contrastive Learning for Diversity-Aware Product Recommendations in Retail",
    "url": "https://www.alphaxiv.org/abs/2602.08886v1",
    "arxiv_id": "2602.08886v1",
    "authors": "Vasileios Karlis, Ezgi Yıldırım, David Vos, Maarten de Rijke",
    "categories": "cs.IR, cs.LG",
    "pub_date": "2026-02-09 16:48:16",
    "ori_summary": "Recommender systems often struggle with long-tail distributions and limited item catalog exposure, where a small subset of popular items dominates recommendations. This challenge is especially critical in large-scale online retail settings with extensive and diverse product assortments. This paper introduces an approach to enhance catalog coverage without compromising recommendation quality in the existing digital recommendation pipeline at IKEA Retail. Drawing inspiration from recent advances in negative sampling to address popularity bias, we integrate contrastive learning with carefully selected negative samples. Through offline and online evaluations, we demonstrate that our method improves catalog coverage, ensuring a more diverse set of recommendations yet preserving strong recommendation performance.",
    "summary": "该论文研究大规模零售推荐系统中长尾商品曝光不足、推荐多样性受限的问题。其核心方法是借鉴负采样技术解决流行度偏差，通过对比学习结合精心设计的负样本来提升商品目录覆盖度，从而获得更均衡的推荐结果。",
    "translation": "零售场景中基于对比学习的多样性感知商品推荐",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及推荐系统（RecSys）中的多样性推荐这一核心领域进展，属于Core Domain Advances范畴。对比学习作为有效的表示学习方法，在提升推荐多样性方面具有明确的技术路径和应用价值，与用户兴趣建模和商品排序直接相关。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统的核心挑战——长尾分布和多样性问题，提出结合对比学习与负采样来提升商品覆盖度，属于推荐系统领域的重要进展。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08873v1": {
    "title": "Whose Name Comes Up? Benchmarking and Intervention-Based Auditing of LLM-Based Scholar Recommendation",
    "url": "https://www.alphaxiv.org/abs/2602.08873v1",
    "arxiv_id": "2602.08873v1",
    "authors": "Lisette Espin-Noboa, Gonzalo Gabriel Mendez",
    "categories": "cs.IR, cs.AI, cs.CY, cs.SI, physics.soc-ph",
    "pub_date": "2026-02-09 16:34:57",
    "ori_summary": "Large language models (LLMs) are increasingly used for academic expert recommendation. Existing audits typically evaluate model outputs in isolation, largely ignoring end-user inference-time interventions. As a result, it remains unclear whether failures such as refusals, hallucinations, and uneven coverage stem from model choice or deployment decisions. We introduce LLMScholarBench, a benchmark for auditing LLM-based scholar recommendation that jointly evaluates model infrastructure and end-user interventions across multiple tasks. LLMScholarBench measures both technical quality and social representation using nine metrics. We instantiate the benchmark in physics expert recommendation and audit 22 LLMs under temperature variation, representation-constrained prompting, and retrieval-augmented generation (RAG) via web search. Our results show that end-user interventions do not yield uniform improvements but instead redistribute error across dimensions. Higher temperature degrades validity, consistency, and factuality. Representation-constrained prompting improves diversity at the expense of factuality, while RAG primarily improves technical quality while reducing diversity and parity. Overall, end-user interventions reshape trade-offs rather than providing a general fix. We release code and data that can be adapted to other disciplines by replacing domain-specific ground truth and metrics.",
    "summary": "",
    "translation": "谁的名字被提及？基于LLM的学者推荐系统的基准测试与干预式审计",
    "relevance_score": 3,
    "reasoning": "该论文标题涉及LLM在推荐系统中的应用（学者推荐），这属于'直接LLM应用'的范畴。然而，标题中的'基准测试与干预式审计'暗示了评估、审计等非技术性主题，这些属于'无关主题'中的公平性、伦理等范畴，因此整体相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08872v1": {
    "title": "Large Language Models for Geolocation Extraction in Humanitarian Crisis Response",
    "url": "https://www.alphaxiv.org/abs/2602.08872v1",
    "arxiv_id": "2602.08872v1",
    "authors": "G. Cafferata, T. Demarco, K. Kalimeri, Y. Mejova, M. G. Beiró",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2026-02-09 16:34:25",
    "ori_summary": "Humanitarian crises demand timely and accurate geographic information to inform effective response efforts. Yet, automated systems that extract locations from text often reproduce existing geographic and socioeconomic biases, leading to uneven visibility of crisis-affected regions. This paper investigates whether Large Language Models (LLMs) can address these geographic disparities in extracting location information from humanitarian documents. We introduce a two-step framework that combines few-shot LLM-based named entity recognition with an agent-based geocoding module that leverages context to resolve ambiguous toponyms. We benchmark our approach against state-of-the-art pretrained and rule-based systems using both accuracy and fairness metrics across geographic and socioeconomic dimensions. Our evaluation uses an extended version of the HumSet dataset with refined literal toponym annotations. Results show that LLM-based methods substantially improve both the precision and fairness of geolocation extraction from humanitarian texts, particularly for underrepresented regions. By bridging advances in LLM reasoning with principles of responsible and inclusive AI, this work contributes to more equitable geospatial data systems for humanitarian response, advancing the goal of leaving no place behind in crisis analytics.",
    "summary": "",
    "translation": "大型语言模型在危机人道主义响应中的地理位置提取应用",
    "relevance_score": 2,
    "reasoning": "该论文虽然涉及LLM技术，但其应用领域是人道主义危机响应中的地理位置提取，这属于特定领域应用而非RecSys/Search/Ads核心领域。论文标题未表明该技术有明确的推荐系统、搜索或广告应用潜力，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08837v1": {
    "title": "AMEM4Rec: Leveraging Cross-User Similarity for Memory Evolution in Agentic LLM Recommenders",
    "url": "https://www.alphaxiv.org/abs/2602.08837v1",
    "arxiv_id": "2602.08837v1",
    "authors": "Minh-Duc Nguyen, Hai-Dang Kieu, Dung D. Le",
    "categories": "cs.IR, cs.LG",
    "pub_date": "2026-02-09 16:06:55",
    "ori_summary": "Agentic systems powered by Large Language Models (LLMs) have shown strong potential in recommender systems but remain hindered by several challenges. Fine-tuning LLMs is parameter-inefficient, and prompt-based agentic reasoning is limited by context length and hallucination risk. Moreover, existing agentic recommendation systems predominantly leverages semantic knowledge while neglecting the collaborative filtering (CF) signals essential for implicit preference modeling. To address these limitations, we propose AMEM4Rec, an agentic LLM-based recommender that learns collaborative signals in an end-to-end manner through cross-user memory evolution. AMEM4Rec stores abstract user behavior patterns from user histories in a global memory pool. Within this pool, memories are linked to similar existing ones and iteratively evolved to reinforce shared cross-user patterns, enabling the system to become aware of CF signals without relying on a pre-trained CF model. Extensive experiments on Amazon and MIND datasets show that AMEM4Rec consistently outperforms state-of-the-art LLM-based recommenders, demonstrating the effectiveness of evolving memory-guided collaborative filtering.",
    "summary": "该论文研究如何让基于LLM的智能体推荐系统有效建模用户隐式偏好。其核心思想是设计一个全局记忆池来存储抽象用户行为模式，并通过跨用户相似性链接和迭代演化这些记忆，从而端到端地学习协同过滤信号，无需依赖预训练的协同过滤模型。",
    "translation": "AMEM4Rec：基于跨用户相似性的智能LLM推荐系统中记忆演化机制",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及LLM在推荐系统中的应用（Direct LLM Applications），提出了一种基于跨用户相似性的记忆演化机制，这属于推荐系统核心领域的创新。同时，论文探讨了智能LLM推荐系统（Agentic LLM Recommenders），这属于LLM技术在推荐领域的直接应用，具有明确的实践意义。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对LLM推荐系统的核心挑战，提出通过跨用户记忆演化学习协同过滤信号，完美契合对直接LLM应用和核心领域进展的关注。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08742v1": {
    "title": "Welfarist Formulations for Diverse Similarity Search",
    "url": "https://www.alphaxiv.org/abs/2602.08742v1",
    "arxiv_id": "2602.08742v1",
    "authors": "Siddharth Barman, Nirjhar Das, Shivam Gupta, Kirankumar Shiragur",
    "categories": "cs.DS, cs.CG, cs.GT, cs.IR, cs.LG",
    "pub_date": "2026-02-09 14:42:28",
    "ori_summary": "Nearest Neighbor Search (NNS) is a fundamental problem in data structures with wide-ranging applications, such as web search, recommendation systems, and, more recently, retrieval-augmented generations (RAG). In such recent applications, in addition to the relevance (similarity) of the returned neighbors, diversity among the neighbors is a central requirement. In this paper, we develop principled welfare-based formulations in NNS for realizing diversity across attributes. Our formulations are based on welfare functions -- from mathematical economics -- that satisfy central diversity (fairness) and relevance (economic efficiency) axioms. With a particular focus on Nash social welfare, we note that our welfare-based formulations provide objective functions that adaptively balance relevance and diversity in a query-dependent manner. Notably, such a balance was not present in the prior constraint-based approach, which forced a fixed level of diversity and optimized for relevance. In addition, our formulation provides a parametric way to control the trade-off between relevance and diversity, providing practitioners with flexibility to tailor search results to task-specific requirements. We develop efficient nearest neighbor algorithms with provable guarantees for the welfare-based objectives. Notably, our algorithm can be applied on top of any standard ANN method (i.e., use standard ANN method as a subroutine) to efficiently find neighbors that approximately maximize our welfare-based objectives. Experimental results demonstrate that our approach is practical and substantially improves diversity while maintaining high relevance of the retrieved neighbors.",
    "summary": "该论文研究在最近邻搜索中如何同时满足相关性和多样性的核心问题，其核心思想是引入福利函数（特别是纳什社会福利）作为目标函数，以查询自适应的方式平衡相关性和多样性，并提供参数化方法控制两者权衡。",
    "translation": "福利主义视角下的多样化相似性搜索公式化方法",
    "relevance_score": 8,
    "reasoning": "该论文直接涉及搜索领域的核心算法问题（多样化相似性搜索），这是搜索系统的基本功能之一。福利主义视角可能为推荐和广告系统中的多样性优化提供新的数学框架，有助于平衡相关性和多样性目标。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文针对搜索和推荐中的多样性相似性搜索提出福利主义公式，通过纳什社会福利平衡相关性和多样性，直接适用于搜索和推荐系统的核心算法改进。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08700v1": {
    "title": "Do Images Clarify? A Study on the Effect of Images on Clarifying Questions in Conversational Search",
    "url": "https://www.alphaxiv.org/abs/2602.08700v1",
    "arxiv_id": "2602.08700v1",
    "authors": "Clemencia Siro, Zahra Abbasiantaeb, Yifei Yuan, Mohammad Aliannejadi, Maarten de Rijke",
    "categories": "cs.CL, cs.HC, cs.IR",
    "pub_date": "2026-02-09 14:16:11",
    "ori_summary": "Conversational search systems increasingly employ clarifying questions to refine user queries and improve the search experience. Previous studies have demonstrated the usefulness of text-based clarifying questions in enhancing both retrieval performance and user experience. While images have been shown to improve retrieval performance in various contexts, their impact on user performance when incorporated into clarifying questions remains largely unexplored. We conduct a user study with 73 participants to investigate the role of images in conversational search, specifically examining their effects on two search-related tasks: (i) answering clarifying questions and (ii) query reformulation. We compare the effect of multimodal and text-only clarifying questions in both tasks within a conversational search context from various perspectives. Our findings reveal that while participants showed a strong preference for multimodal questions when answering clarifying questions, preferences were more balanced in the query reformulation task. The impact of images varied with both task type and user expertise. In answering clarifying questions, images helped maintain engagement across different expertise levels, while in query reformulation they led to more precise queries and improved retrieval performance. Interestingly, for clarifying question answering, text-only setups demonstrated better user performance as they provided more comprehensive textual information in the absence of images. These results provide valuable insights for designing effective multimodal conversational search systems, highlighting that the benefits of visual augmentation are task-dependent and should be strategically implemented based on the specific search context and user characteristics.",
    "summary": "",
    "translation": "图像能澄清吗？关于图像在对话式搜索澄清问题中作用的研究",
    "relevance_score": 3,
    "reasoning": "该论文研究对话式搜索中的澄清问题，这属于搜索领域的核心进展，符合'Core Domain Advances'中的搜索范畴。然而，论文明确关注图像的作用，这更偏向视觉-语言交互的特定方面，而非您关注的异构数据统一建模的VLM类比方向，且未涉及LLM或Transformer架构的进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08678v1": {
    "title": "SA-CAISR: Stage-Adaptive and Conflict-Aware Incremental Sequential Recommendation",
    "url": "https://www.alphaxiv.org/abs/2602.08678v1",
    "arxiv_id": "2602.08678v1",
    "authors": "Xiaomeng Song, Xinru Wang, Hanbing Wang, Hongyu Lu, Yu Chen, Zhaochun Ren, Zhumin Chen",
    "categories": "cs.IR",
    "pub_date": "2026-02-09 14:00:52",
    "ori_summary": "Sequential recommendation (SR) aims to predict a user's next action by learning from their historical interaction sequences. In real-world applications, these models require periodic updates to adapt to new interactions and evolving user preferences. While incremental learning methods facilitate these updates, they face significant challenges. Replay-based approaches incur high memory and computational costs, and regularization-based methods often struggle to discard outdated or conflicting knowledge. To overcome these challenges, we propose SA-CAISR, a Stage-Adaptive and Conflict-Aware Incremental Sequential Recommendation framework. As a buffer-free framework, SA-CAISR operates using only the old model and new data, directly addressing the high costs of replay-based techniques. SA-CAISR introduces a novel Fisher-weighted knowledge-screening mechanism that dynamically identifies outdated knowledge by estimating parameter-level conflicts between the old model and new data, allowing our approach to selectively remove obsolete knowledge while preserving compatible historical patterns. This dynamic balance between stability and adaptability allows our method to achieve a new state-of-the-art performance in incremental SR. Specifically, SA-CAISR improves Recall@20 by 2.0%, MRR@20 by 1.2%, and NDCG@20 by 1.4% on average across datasets, while reducing memory usage by 97.5% and training time by 46.9% compared to the best baselines. This efficiency allows real-world systems to rapidly update user profiles with minimal computational overhead, ensuring more timely and accurate recommendations.",
    "summary": "研究增量序列推荐中模型更新时新旧知识冲突与过时知识遗忘的问题。提出无缓冲框架SA-CAISR，通过Fisher加权知识筛选机制动态识别参数级冲突，选择性移除过时知识同时保留兼容历史模式。",
    "translation": "SA-CAISR：阶段自适应与冲突感知的增量序列推荐",
    "relevance_score": 9,
    "reasoning": "该论文直接聚焦于序列推荐这一推荐系统核心领域，属于'Core Domain Advances'范畴。阶段自适应和冲突感知机制是针对用户行为序列建模的重要技术改进，对提升推荐系统的实时性和准确性具有明确价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文针对推荐系统核心挑战——增量学习，提出无缓冲框架解决冲突知识问题，直接关联推荐系统领域的技术进步。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08668v1": {
    "title": "Retrieval Pivot Attacks in Hybrid RAG: Measuring and Mitigating Amplified Leakage from Vector Seeds to Graph Expansion",
    "url": "https://www.alphaxiv.org/abs/2602.08668v1",
    "arxiv_id": "2602.08668v1",
    "authors": "Scott Thornton",
    "categories": "cs.CR, cs.IR, cs.LG",
    "pub_date": "2026-02-09 13:55:04",
    "ori_summary": "Hybrid Retrieval-Augmented Generation (RAG) pipelines combine vector similarity search with knowledge graph expansion for multi-hop reasoning. We show that this composition introduces a distinct security failure mode: a vector-retrieved \"seed\" chunk can pivot via entity links into sensitive graph neighborhoods, causing cross-tenant data leakage that does not occur in vector-only retrieval. We formalize this risk as Retrieval Pivot Risk (RPR) and introduce companion metrics Leakage@k, Amplification Factor, and Pivot Depth (PD) to quantify leakage magnitude and traversal structure. We present seven Retrieval Pivot Attacks that exploit the vector-to-graph boundary and show that adversarial injection is not required: naturally shared entities create cross-tenant pivot paths organically. Across a synthetic multi-tenant enterprise corpus and the Enron email corpus, the undefended hybrid pipeline exhibits high pivot risk (RPR up to 0.95) with multiple unauthorized items returned per query. Leakage consistently appears at PD=2, which we attribute to the bipartite chunk-entity topology and formalize as a proposition. We then show that enforcing authorization at a single location, the graph expansion boundary, eliminates measured leakage (RPR near 0) across both corpora, all attack variants, and label forgery rates up to 10 percent, with minimal overhead. Our results indicate the root cause is boundary enforcement, not inherently complex defenses: two individually secure retrieval components can compose into an insecure system unless authorization is re-checked at the transition point.",
    "summary": "",
    "translation": "混合检索增强生成中的检索枢纽攻击：从向量种子到图扩展的放大泄露度量与缓解",
    "relevance_score": 2,
    "reasoning": "该论文主要关注检索增强生成（RAG）系统的安全漏洞和攻击缓解，属于安全/隐私领域，这在您的无关主题列表中明确排除。虽然RAG技术本身与搜索/推荐相关，但论文焦点是攻击检测和防御，而非核心检索算法、LLM技术或直接应用创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08667v1": {
    "title": "SRSUPM: Sequential Recommender System Based on User Psychological Motivation",
    "url": "https://www.alphaxiv.org/abs/2602.08667v1",
    "arxiv_id": "2602.08667v1",
    "authors": "Yicheng Di, Yuan Liu, Zhi Chen, Jingcai Guo",
    "categories": "cs.IR",
    "pub_date": "2026-02-09 13:54:34",
    "ori_summary": "Sequential recommender infers users' evolving psychological motivations from historical interactions to recommend the next preferred items. Most existing methods compress recent behaviors into a single vector and optimize it toward a single observed target item, but lack explicit modeling of psychological motivation shift. As a result, they struggle to uncover the distributional patterns across different shift degrees and to capture collaborative knowledge that is sensitive to psychological motivation shift. We propose a general framework, the Sequential Recommender System Based on User Psychological Motivation, to enhance sequential recommenders with psychological motivation shift-aware user modeling. Specifically, the Psychological Motivation Shift Assessment quantitatively measures psychological motivation shift; guided by PMSA, the Shift Information Construction models dynamically evolving multi-level shift states, and the Psychological Motivation Shift-driven Information Decomposition decomposes and regularizes representations across shift levels. Moreover, the Psychological Motivation Shift Information Matching strengthens collaborative patterns related to psychological motivation shift to learn more discriminative user representations. Extensive experiments on three public benchmarks show that SRSUPM consistently outperforms representative baselines on diverse sequential recommender tasks.",
    "summary": "该论文研究序列推荐中用户心理动机转移的建模问题，核心思想是通过量化评估心理动机转移程度，构建动态多级转移状态，并分解和正则化不同转移级别的表示，以增强用户建模。",
    "translation": "SRSUPM：基于用户心理动机的序列推荐系统",
    "relevance_score": 8,
    "reasoning": "该论文直接属于'核心领域进展'类别，专注于序列推荐系统，这是推荐系统领域的核心研究方向。虽然标题未明确提及LLM，但用户心理动机建模可以受益于LLM技术进行更深入的用户意图理解，在推荐系统中具有直接应用价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对序列推荐系统的核心建模问题，提出显式建模用户心理动机转移的通用框架，属于推荐系统领域的核心进展，与用户序列建模高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08612v1": {
    "title": "OneLive: Dynamically Unified Generative Framework for Live-Streaming Recommendation",
    "url": "https://www.alphaxiv.org/abs/2602.08612v1",
    "arxiv_id": "2602.08612v1",
    "authors": "Shen Wang, Yusheng Huang, Ruochen Yang, Shuang Wen, Pengbo Xu, Jiangxia Cao, Yueyang Liu, Kuo Cai, Chengcheng Guo, Shiyao Wang, Xinchen Luo, Qiang Luo, Ruiming Tang, Shuang Yang, Zhaojie Liu, Guorui Zhou, Han Li, Kun Gai",
    "categories": "cs.IR",
    "pub_date": "2026-02-09 12:56:39",
    "ori_summary": "Live-streaming recommender system serves as critical infrastructure that bridges the patterns of real-time interactions between users and authors. Similar to traditional industrial recommender systems, live-streaming recommendation also relies on cascade architectures to support large-scale concurrency. Recent advances in generative recommendation unify the multi-stage recommendation process with Transformer-based architectures, offering improved scalability and higher computational efficiency. However, the inherent complexity of live-streaming prevents the direct transfer of these methods to live-streaming scenario, where continuously evolving content, limited lifecycles, strict real-time constraints, and heterogeneous multi-objectives introduce unique challenges that invalidate static tokenization and conventional model framework. To address these issues, we propose OneLive, a dynamically unified generative recommendation framework tailored for live-streaming scenario. OneLive integrates four key components: (i) A Dynamic Tokenizer that continuously encodes evolving real-time live content fused with behavior signal through residual quantization; (ii) A Time-Aware Gated Attention mechanism that explicitly models temporal dynamics for timely decision making; (iii) An efficient decoder-only generative architecture enhanced with Sequential MTP and QK Norm for stable training and accelerated inference; (iv) A Unified Multi-Objective Alignment Framework reinforces policy optimization for personalized preferences.",
    "summary": "该论文研究直播推荐系统中动态内容、实时约束和多目标优化的核心挑战。其核心方法是提出OneLive框架，通过动态分词器、时序感知门控注意力、高效解码器架构和统一多目标对齐框架，实现直播场景下生成式推荐的动态统一建模。",
    "translation": "OneLive：面向直播推荐的动态统一生成式框架",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及推荐系统（RecSys）领域，专注于直播推荐这一具体应用场景。标题中提到的'动态统一生成式框架'暗示了创新的架构方法，可能涉及序列建模或生成式技术，这与'直接LLM应用'和'核心领域进展'高度相关。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文针对直播推荐场景的动态性挑战，提出了动态统一生成框架，直接应用Transformer架构解决推荐系统核心问题，并引入时序感知和多目标对齐等创新机制。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08575v1": {
    "title": "RankGR: Rank-Enhanced Generative Retrieval with Listwise Direct Preference Optimization in Recommendation",
    "url": "https://www.alphaxiv.org/abs/2602.08575v1",
    "arxiv_id": "2602.08575v1",
    "authors": "Kairui Fu, Changfa Wu, Kun Yuan, Binbin Cao, Dunxian Huang, Yuliang Yan, Junjun Zheng, Jianning Zhang, Silu Zhou, Jian Wu, Kun Kuang",
    "categories": "cs.IR",
    "pub_date": "2026-02-09 12:13:43",
    "ori_summary": "Generative retrieval (GR) has emerged as a promising paradigm in recommendation systems by autoregressively decoding identifiers of target items. Despite its potential, current approaches typically rely on the next-token prediction schema, which treats each token of the next interacted items as the sole target. This narrow focus 1) limits their ability to capture the nuanced structure of user preferences, and 2) overlooks the deep interaction between decoded identifiers and user behavior sequences. In response to these challenges, we propose RankGR, a Rank-enhanced Generative Retrieval method that incorporates listwise direct preference optimization for recommendation. RankGR decomposes the retrieval process into two complementary stages: the Initial Assessment Phase (IAP) and the Refined Scoring Phase (RSP). In IAP, we incorporate a novel listwise direct preference optimization strategy into GR, thus facilitating a more comprehensive understanding of the hierarchical user preferences and more effective partial-order modeling. The RSP then refines the top-λ candidates generated by IAP with interactions towards input sequences using a lightweight scoring module, leading to more precise candidate evaluation. Both phases are jointly optimized under a unified GR model, ensuring consistency and efficiency. Additionally, we implement several practical improvements in training and deployment, ultimately achieving a real-time system capable of handling nearly ten thousand requests per second. Extensive offline performance on both research and industrial datasets, as well as the online gains on the \"Guess You Like\" section of Taobao, validate the effectiveness and scalability of RankGR.",
    "summary": "该论文研究推荐系统中生成式检索范式因单步预测而难以捕捉用户偏好层次和序列交互的问题。其核心方法是提出RankGR，将检索过程分解为初始评估和精化评分两阶段，并在第一阶段引入列表直接偏好优化来建模用户偏好的层次结构和部分顺序关系。",
    "translation": "RankGR：基于列表式直接偏好优化的排序增强生成式检索在推荐系统中的应用",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及推荐系统中的生成式检索（Generative Retrieval），这是LLM在推荐系统中的直接应用。论文标题明确提到了排序增强（Rank-Enhanced）和列表式直接偏好优化（Listwise Direct Preference Optimization），这些都属于推荐系统核心领域的技术进展，特别是排序和检索优化方面。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统中的生成式检索范式进行改进，通过两阶段分解和列表偏好优化，核心解决了用户偏好建模和序列交互的关键问题，与用户当前关注的生成式检索应用、Transformer架构优化和推荐系统核心进展高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08569v1": {
    "title": "Towards Reliable Social A/B Testing: Spillover-Contained Clustering with Robust Post-Experiment Analysis",
    "url": "https://www.alphaxiv.org/abs/2602.08569v1",
    "arxiv_id": "2602.08569v1",
    "authors": "Xu Min, Zhaoxu Yang, Kaixuan Tan, Juan Yan, Xunbin Xiong, Zihao Zhu, Kaiyu Zhu, Fenglin Cui, Yang Yang, Sihua Yang, Jianhui Bu",
    "categories": "cs.SI, cs.IR",
    "pub_date": "2026-02-09 12:08:29",
    "ori_summary": "A/B testing is the foundation of decision-making in online platforms, yet social products often suffer from network interference: user interactions cause treatment effects to spill over into the control group. Such spillovers bias causal estimates and undermine experimental conclusions. Existing approaches face key limitations: user-level randomization ignores network structure, while cluster-based methods often rely on general-purpose clustering that is not tailored for spillover containment and has difficulty balancing unbiasedness and statistical power at scale. We propose a spillover-contained experimentation framework with two stages. In the pre-experiment stage, we build social interaction graphs and introduce a Balanced Louvain algorithm that produces stable, size-balanced clusters while minimizing cross-cluster edges, enabling reliable cluster-based randomization. In the post-experiment stage, we develop a tailored CUPAC estimator that leverages pre-experiment behavioral covariates to reduce the variance induced by cluster-level assignment, thereby improving statistical power. Together, these components provide both structural spillover containment and robust statistical inference. We validate our approach through large-scale social sharing experiments on Kuaishou, a platform serving hundreds of millions of users. Results show that our method substantially reduces spillover and yields more accurate assessments of social strategies than traditional user-level designs, establishing a reliable and scalable framework for networked A/B testing.",
    "summary": "",
    "translation": "迈向可靠的社会化A/B测试：具有稳健后实验分析的溢出效应约束聚类",
    "relevance_score": 2,
    "reasoning": "该论文主要关注A/B测试中的社会溢出效应和稳健统计方法，属于实验设计领域。虽然A/B测试在推荐系统、搜索和广告中有应用，但该论文的核心内容（社会网络效应、统计稳健性）与您关注的核心领域进展、LLM技术、Transformer架构或直接LLM应用没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08559v1": {
    "title": "QARM V2: Quantitative Alignment Multi-Modal Recommendation for Reasoning User Sequence Modeling",
    "url": "https://www.alphaxiv.org/abs/2602.08559v1",
    "arxiv_id": "2602.08559v1",
    "authors": "Tian Xia, Jiaqi Zhang, Yueyang Liu, Hongjian Dou, Tingya Yin, Jiangxia Cao, Xulei Liang, Tianlu Xie, Lihao Liu, Xiang Chen, Shen Wang, Changxin Lao, Haixiang Gan, Jinkai Yu, Keting Cen, Lu Hao, Xu Zhang, Qiqiang Zhong, Zhongbo Sun, Yiyu Wang, Shuang Yang, Mingxin Wen, Xiangyu Wu, Shaoguo Liu, Tingting Gao, Zhaojie Liu, Han Li, Kun Gai",
    "categories": "cs.IR",
    "pub_date": "2026-02-09 11:57:28",
    "ori_summary": "With the evolution of large language models (LLMs), there is growing interest in leveraging their rich semantic understanding to enhance industrial recommendation systems (RecSys). Traditional RecSys relies on ID-based embeddings for user sequence modeling in the General Search Unit (GSU) and Exact Search Unit (ESU) paradigm, which suffers from low information density, knowledge isolation, and weak generalization ability. While LLMs offer complementary strengths with dense semantic representations and strong generalization, directly applying LLM embeddings to RecSys faces critical challenges: representation unmatch with business objectives and representation unlearning end-to-end with downstream tasks. In this paper, we present QARM V2, a unified framework that bridges LLM semantic understanding with RecSys business requirements for user sequence modeling.",
    "summary": "主题：解决传统推荐系统基于ID嵌入的用户序列建模存在信息密度低、知识隔离和泛化能力弱的问题，以及LLM嵌入直接应用时与业务目标不匹配、无法端到端学习的挑战。核心思想：提出QARM V2统一框架，通过量化对齐多模态方法，将LLM的语义理解能力与推荐系统的业务需求进行桥接，实现用户序列的联合建模。",
    "translation": "QARM V2：面向推理式用户序列建模的定量对齐多模态推荐",
    "relevance_score": 8,
    "reasoning": "该论文标题直接涉及多模态推荐和用户序列建模，这属于推荐系统的核心领域进展。标题中的'定量对齐'和'推理'暗示了可能结合LLM技术进行序列理解和推理，符合直接LLM应用和异构数据统一建模的关注点。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对推荐系统中用户序列建模的核心问题，提出统一框架来融合LLM语义理解与业务需求，高度契合LLM在推荐系统的直接应用和异构数据统一建模的关注点。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08545v1": {
    "title": "DA-RAG: Dynamic Attributed Community Search for Retrieval-Augmented Generation",
    "url": "https://www.alphaxiv.org/abs/2602.08545v1",
    "arxiv_id": "2602.08545v1",
    "authors": "Xingyuan Zeng, Zuohan Wu, Yue Wang, Chen Zhang, Quanming Yao, Libin Zheng, Jian Yin",
    "categories": "cs.IR",
    "pub_date": "2026-02-09 11:45:13",
    "ori_summary": "Owing to their unprecedented comprehension capabilities, large language models (LLMs) have become indispensable components of modern web search engines. From a technical perspective, this integration represents retrieval-augmented generation (RAG), which enhances LLMs by grounding them in external knowledge bases. A prevalent technical approach in this context is graph-based RAG (G-RAG). However, current G-RAG methodologies frequently underutilize graph topology, predominantly focusing on low-order structures or pre-computed static communities. This limitation affects their effectiveness in addressing dynamic and complex queries. Thus, we propose DA-RAG, which leverages attributed community search (ACS) to extract relevant subgraphs based on the queried question dynamically. DA-RAG captures high-order graph structures, allowing for the retrieval of self-complementary knowledge. Furthermore, DA-RAG is equipped with a chunk-layer oriented graph index, which facilitates efficient multi-granularity retrieval while significantly reducing both computational and economic costs. We evaluate DA-RAG on multiple datasets, demonstrating that it outperforms existing RAG methods by up to 40% in head-to-head comparisons across four metrics while reducing index construction time and token overhead by up to 37% and 41%, respectively.",
    "summary": "",
    "translation": "DA-RAG：面向检索增强生成的动态属性社区搜索",
    "relevance_score": 7,
    "reasoning": "该论文涉及检索增强生成（RAG），这是LLM应用中的关键技术，属于'直接LLM应用'范畴。社区搜索方法可能应用于个性化推荐或广告定向，通过动态属性建模用户兴趣社区。虽然标题未明确提及推荐/搜索/广告，但RAG技术和社区搜索在这些领域有直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2602.08543v1": {
    "title": "GISA: A Benchmark for General Information-Seeking Assistant",
    "url": "https://www.alphaxiv.org/abs/2602.08543v1",
    "arxiv_id": "2602.08543v1",
    "authors": "Yutao Zhu, Xingshuo Zhang, Maosen Zhang, Jiajie Jin, Liancheng Zhang, Xiaoshuai Song, Kangzhi Zhao, Wencong Zeng, Ruiming Tang, Han Li, Ji-Rong Wen, Zhicheng Dou",
    "categories": "cs.CL, cs.AI, cs.IR",
    "pub_date": "2026-02-09 11:44:15",
    "ori_summary": "The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.",
    "summary": "",
    "translation": "GISA：通用信息寻求助手的基准测试",
    "relevance_score": 2,
    "reasoning": "该论文标题表明其专注于信息寻求助手的基准测试，这主要属于评估基准范畴。虽然信息寻求助手可能与搜索系统相关，但标题明确强调“基准测试”，这属于您列出的不相关主题中的“评估基准”类别。该论文没有明确涉及推荐系统、广告、LLM技术、Transformer架构进展或异构数据建模等您的关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08530v1": {
    "title": "PIT: A Dynamic Personalized Item Tokenizer for End-to-End Generative Recommendation",
    "url": "https://www.alphaxiv.org/abs/2602.08530v1",
    "arxiv_id": "2602.08530v1",
    "authors": "Huanjie Wang, Xinchen Luo, Honghui Bao, Zhang Zixing, Lejian Ren, Yunfan Wu, Hongwei Zhang, Liwei Guan, Guang Chen",
    "categories": "cs.IR",
    "pub_date": "2026-02-09 11:28:56",
    "ori_summary": "Generative Recommendation has revolutionized recommender systems by reformulating retrieval as a sequence generation task over discrete item identifiers. Despite the progress, existing approaches typically rely on static, decoupled tokenization that ignores collaborative signals. While recent methods attempt to integrate collaborative signals into item identifiers either during index construction or through end-to-end modeling, they encounter significant challenges in real-world production environments. Specifically, the volatility of collaborative signals leads to unstable tokenization, and current end-to-end strategies often devolve into suboptimal two-stage training rather than achieving true co-evolution. To bridge this gap, we propose PIT, a dynamic Personalized Item Tokenizer framework for end-to-end generative recommendation, which employs a co-generative architecture that harmonizes collaborative patterns through collaborative signal alignment and synchronizes item tokenizer with generative recommender via a co-evolution learning. This enables the dynamic, joint, end-to-end evolution of both index construction and recommendation. Furthermore, a one-to-many beam index ensures scalability and robustness, facilitating seamless integration into large-scale industrial deployments. Extensive experiments on real-world datasets demonstrate that PIT consistently outperforms competitive baselines. In a large-scale deployment at Kuaishou, an online A/B test yielded a substantial 0.402% uplift in App Stay Time, validating the framework's effectiveness in dynamic industrial environments.",
    "summary": "该论文研究生成式推荐系统中静态项目标识符忽略协同信号的问题，核心思想是提出一个动态个性化项目分词器框架，通过协同信号对齐和协同进化学习，实现索引构建与推荐生成的联合端到端动态优化。",
    "translation": "PIT：一种用于端到端生成式推荐的动态个性化物品分词器",
    "relevance_score": 9,
    "reasoning": "该论文直接涉及生成式推荐系统，属于'直接LLM应用'范畴，通过个性化分词器将物品序列转换为适合LLM处理的token表示。这种技术可应用于搜索和推荐中的序列建模，将用户历史行为编码为token序列供LLM生成个性化推荐。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对生成式推荐系统的核心挑战——动态个性化索引构建，提出了协同进化架构，属于生成式推荐的前沿方法创新，与用户关注的LLM在推荐系统中的应用高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08457v1": {
    "title": "Hybrid Pooling with LLMs via Relevance Context Learning",
    "url": "https://www.alphaxiv.org/abs/2602.08457v1",
    "arxiv_id": "2602.08457v1",
    "authors": "David Otero, Javier Parapar",
    "categories": "cs.IR",
    "pub_date": "2026-02-09 10:10:22",
    "ori_summary": "High-quality relevance judgements over large query sets are essential for evaluating Information Retrieval (IR) systems, yet manual annotation remains costly and time-consuming. Large Language Models (LLMs) have recently shown promise as automatic relevance assessors, but their reliability is still limited. Most existing approaches rely on zero-shot prompting or In-Context Learning (ICL) with a small number of labeled examples. However, standard ICL treats examples as independent instances and fails to explicitly capture the underlying relevance criteria of a topic, restricting its ability to generalize to unseen query-document pairs. To address this limitation, we introduce Relevance Context Learning (RCL), a novel framework that leverages human relevance judgements to explicitly model topic-specific relevance criteria. Rather than directly using labeled examples for in-context prediction, RCL first prompts an LLM (Instructor LLM) to analyze sets of judged query-document pairs and generate explicit narratives that describe what constitutes relevance for a given topic. These relevance narratives are then used as structured prompts to guide a second LLM (Assessor LLM) in producing relevance judgements. To evaluate RCL in a realistic data collection setting, we propose a hybrid pooling strategy in which a shallow depth-\\textit{k} pool from participating systems is judged by human assessors, while the remaining documents are labeled by LLMs. Experimental results demonstrate that RCL substantially outperforms zero-shot prompting and consistently improves over standard ICL. Overall, our findings indicate that transforming relevance examples into explicit, context-aware relevance narratives is a more effective way of exploiting human judgements for LLM-based IR dataset construction.",
    "summary": "该论文研究如何利用LLM自动生成高质量的信息检索相关性标注。其核心方法是提出相关性上下文学习框架，通过分析已标注的查询-文档对，让LLM生成描述主题相关性标准的明确叙述，再用这些叙述指导另一个LLM进行相关性判断。",
    "translation": "基于相关性上下文学习的LLM混合池化方法",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM技术（混合池化）在相关性学习中的应用，直接对应'Direct LLM Applications'和'Enabling LLM Tech'关注点。混合池化技术可应用于搜索和推荐系统中的相关性建模，通过LLM增强上下文理解能力来改进排序效果。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出的RCL框架通过将人类标注转化为结构化相关性叙述，直接提升了LLM在信息检索评估中的可靠性和泛化能力，属于LLM在搜索领域的直接应用创新。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08411v1": {
    "title": "A Sketch+Text Composed Image Retrieval Dataset for Thangka",
    "url": "https://www.alphaxiv.org/abs/2602.08411v1",
    "arxiv_id": "2602.08411v1",
    "authors": "Jinyu Xu, Yi Sun, Jiangling Zhang, Qing Xie, Daomin Ji, Zhifeng Bao, Jiachen Li, Yanchun Ma, Yongjian Liu",
    "categories": "cs.IR",
    "pub_date": "2026-02-09 09:14:29",
    "ori_summary": "Composed Image Retrieval (CIR) enables image retrieval by combining multiple query modalities, but existing benchmarks predominantly focus on general-domain imagery and rely on reference images with short textual modifications. As a result, they provide limited support for retrieval scenarios that require fine-grained semantic reasoning, structured visual understanding, and domain-specific knowledge. In this work, we introduce CIRThan, a sketch+text Composed Image Retrieval dataset for Thangka imagery, a culturally grounded and knowledge-specific visual domain characterized by complex structures, dense symbolic elements, and domain-dependent semantic conventions. CIRThan contains 2,287 high-quality Thangka images, each paired with a human-drawn sketch and hierarchical textual descriptions at three semantic levels, enabling composed queries that jointly express structural intent and multi-level semantic specification. We provide standardized data splits, comprehensive dataset analysis, and benchmark evaluations of representative supervised and zero-shot CIR methods. Experimental results reveal that existing CIR approaches, largely developed for general-domain imagery, struggle to effectively align sketch-based abstractions and hierarchical textual semantics with fine-grained Thangka images, particularly without in-domain supervision. We believe CIRThan offers a valuable benchmark for advancing sketch+text CIR, hierarchical semantic modeling, and multimodal retrieval in cultural heritage and other knowledge-specific visual domains. The dataset is publicly available at https://github.com/jinyuxu-whut/CIRThan.",
    "summary": "",
    "translation": "唐卡草图+文本组合图像检索数据集",
    "relevance_score": 1,
    "reasoning": "该论文专注于特定文化领域（唐卡）的多模态图像检索，属于纯粹的视觉-语言模型应用。虽然涉及多模态检索，但缺乏与推荐系统、搜索或广告领域的明确联系，且属于特定领域应用而非通用技术进展。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08254v1": {
    "title": "SynthAgent: A Multi-Agent LLM Framework for Realistic Patient Simulation -- A Case Study in Obesity with Mental Health Comorbidities",
    "url": "https://www.alphaxiv.org/abs/2602.08254v1",
    "arxiv_id": "2602.08254v1",
    "authors": "Arman Aghaee, Sepehr Asgarian, Jouhyun Jeon",
    "categories": "cs.AI, cs.IR, cs.MA",
    "pub_date": "2026-02-09 04:14:19",
    "ori_summary": "Simulating high-fidelity patients offers a powerful avenue for studying complex diseases while addressing the challenges of fragmented, biased, and privacy-restricted real-world data. In this study, we introduce SynthAgent, a novel Multi-Agent System (MAS) framework designed to model obesity patients with comorbid mental disorders, including depression, anxiety, social phobia, and binge eating disorder. SynthAgent integrates clinical and medical evidence from claims data, population surveys, and patient-centered literature to construct personalized virtual patients enriched with personality traits that influence adherence, emotion regulation, and lifestyle behaviors. Through autonomous agent interactions, the system simulates disease progression, treatment response, and life management across diverse psychosocial contexts. Evaluation of more than 100 generated patients demonstrated that GPT-5 and Claude 4.5 Sonnet achieved the highest fidelity as the core engine in the proposed MAS framework, outperforming Gemini 2.5 Pro and DeepSeek-R1. SynthAgent thus provides a scalable and privacy-preserving framework for exploring patient journeys, behavioral dynamics, and decision-making processes in both medical and psychological domains.",
    "summary": "",
    "translation": "SynthAgent：用于真实患者模拟的多智能体LLM框架——以伴有心理健康共病的肥胖症为例",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学领域（肥胖症与心理健康共病的患者模拟），这属于明确列出的无关主题中的“Medical, Biology, Chemistry, Physics or other domain-specific applications”。虽然框架涉及多智能体LLM，但其应用场景与推荐系统、搜索或广告的核心技术进展、使能技术或直接应用完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.09012v1": {
    "title": "Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense",
    "url": "https://www.alphaxiv.org/abs/2602.09012v1",
    "arxiv_id": "2602.09012v1",
    "authors": "Jiacheng Liu, Yaxin Luo, Jiacheng Cui, Xinyi Shang, Xiaohan Zhao, Zhiqiang Shen",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2026-02-09 18:55:33",
    "ori_summary": "The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like \"Bingo\". In response, we introduce Next-Gen CAPTCHAs, a scalable defense framework designed to secure the next-generation web against the advanced agents. Unlike static datasets, our benchmark is built upon a robust data generation pipeline, allowing for large-scale and easily scalable evaluations, notably, for backend-supported types, our system is capable of generating effectively unbounded CAPTCHA instances. We exploit the persistent human-agent \"Cognitive Gap\" in interactive perception, memory, decision-making, and action. By engineering dynamic tasks that require adaptive intuition rather than granular planning, we re-establish a robust distinction between biological users and artificial agents, offering a scalable and diverse defense mechanism for the agentic era.",
    "summary": "",
    "translation": "下一代验证码：利用认知差距实现可扩展且多样化的图形用户界面代理防御",
    "relevance_score": 1,
    "reasoning": "该论文专注于验证码技术和GUI代理防御，属于安全领域。虽然提到了可扩展性，但这与推荐系统、搜索或广告的核心技术进展、LLM应用或Transformer架构改进无关。该主题明确属于被排除的\"安全、隐私\"等非技术性话题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.09003v1": {
    "title": "Data Science and Technology Towards AGI Part I: Tiered Data Management",
    "url": "https://www.alphaxiv.org/abs/2602.09003v1",
    "arxiv_id": "2602.09003v1",
    "authors": "Yudong Wang, Zixuan Fu, Hengyu Zhao, Chen Zhao, Chuyue Zhou, Xinle Lin, Hongya Lyu, Shuaikang Xue, Yi Yi, Yingjiao Wang, Zhi Zheng, Yuzhou Zhang, Jie Zhou, Chaojun Xiao, Xu Han, Zhiyuan Liu, Maosong Sun",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2026-02-09 18:47:51",
    "ori_summary": "The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.",
    "summary": "",
    "translation": "迈向通用人工智能的数据科学与技术 第一部分：分层数据管理",
    "relevance_score": 3,
    "reasoning": "该标题聚焦于通用人工智能（AGI）的数据管理，属于基础数据科学范畴，与推荐系统、搜索或广告的直接关联较弱。虽然高效数据管理是所有AI系统的基础，但论文未明确涉及Transformer架构、LLM应用或推荐/搜索/广告的具体技术，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08997v1": {
    "title": "Paradox of De-identification: A Critique of HIPAA Safe Harbour in the Age of LLMs",
    "url": "https://www.alphaxiv.org/abs/2602.08997v1",
    "arxiv_id": "2602.08997v1",
    "authors": "Lavender Y. Jiang, Xujin Chris Liu, Kyunghyun Cho, Eric K. Oermann",
    "categories": "cs.CY, cs.CL",
    "pub_date": "2026-02-09 18:43:19",
    "ori_summary": "Privacy is a human right that sustains patient-provider trust. Clinical notes capture a patient's private vulnerability and individuality, which are used for care coordination and research. Under HIPAA Safe Harbor, these notes are de-identified to protect patient privacy. However, Safe Harbor was designed for an era of categorical tabular data, focusing on the removal of explicit identifiers while ignoring the latent information found in correlations between identity and quasi-identifiers, which can be captured by modern LLMs. We first formalize these correlations using a causal graph, then validate it empirically through individual re-identification of patients from scrubbed notes. The paradox of de-identification is further shown through a diagnosis ablation: even when all other information is removed, the model can predict the patient's neighborhood based on diagnosis alone. This position paper raises the question of how we can act as a community to uphold patient-provider trust when de-identification is inherently imperfect. We aim to raise awareness and discuss actionable recommendations.",
    "summary": "",
    "translation": "去标识化悖论：LLM时代对HIPAA安全港条款的批判",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于HIPAA隐私法规、去标识化技术和法律批判，这直接属于被明确排除的'隐私、安全、伦理等非技术性话题'范畴。虽然提到了'LLM时代'，但核心内容是法规批判而非LLM技术本身或其应用，与您关注的核心领域进展、使能技术或直接应用完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08995v1": {
    "title": "When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents",
    "url": "https://www.alphaxiv.org/abs/2602.08995v1",
    "arxiv_id": "2602.08995v1",
    "authors": "Yuting Ning, Jaylen Jones, Zhehao Zhang, Chentao Ye, Weitong Ruan, Junyi Li, Rahul Gupta, Huan Sun",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 18:41:15",
    "ori_summary": "Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency and reliability. This work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 15% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 90% under adversarial settings while preserving or even improving task success rate in benign environments.",
    "summary": "",
    "translation": "当行动偏离任务：检测并纠正计算机使用代理中的未对齐行动",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机使用代理中的行动检测与纠正，这属于特定代理行为控制领域。虽然可能涉及序列建模，但标题未表明与推荐系统、搜索或广告的直接关联，也未提及LLM、Transformer架构或异构数据处理等核心关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08984v1": {
    "title": "Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.08984v1",
    "arxiv_id": "2602.08984v1",
    "authors": "Yuliang Liu, Yunchong Song, Yixuan Wang, Kewen Ge, Alex Lamb, Qipeng Guo, Kai Chen, Bowen Zhou, Zhouhan Lin",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-02-09 18:33:31",
    "ori_summary": "We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.",
    "summary": "该论文研究如何通过改进语言模型的预训练目标来提升模型能力。其核心方法是提出“下一个概念预测”范式，通过向量量化构建概念词汇表，让模型预测跨越多个token的离散概念，从而创建比传统token预测更困难的训练任务。",
    "translation": "离散潜在空间中的下一概念预测构建更强语言模型",
    "relevance_score": 8,
    "reasoning": "该论文涉及语言模型的核心技术进步（离散潜在空间建模和概念预测），属于'Enabling LLM Tech'范畴。这种离散表示学习方法可直接应用于推荐系统或搜索中的用户意图建模，将连续行为序列转化为离散概念序列进行预测，提升个性化推荐和查询理解的准确性。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文提出了一种新的语言模型预训练范式，通过预测离散概念而非单个token来构建更具挑战性的训练目标，这直接属于核心LLM技术进步范畴，对推荐系统和搜索中的序列建模具有潜在应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08979v1": {
    "title": "Beyond Transcripts: A Renewed Perspective on Audio Chaptering",
    "url": "https://www.alphaxiv.org/abs/2602.08979v1",
    "arxiv_id": "2602.08979v1",
    "authors": "Fabian Retkowski, Maike Züfle, Thai Binh Nguyen, Jan Niehues, Alexander Waibel",
    "categories": "cs.SD, cs.CL",
    "pub_date": "2026-02-09 18:28:10",
    "ori_summary": "Audio chaptering, the task of automatically segmenting long-form audio into coherent sections, is increasingly important for navigating podcasts, lectures, and videos. Despite its relevance, research remains limited and text-based, leaving key questions unresolved about leveraging audio information, handling ASR errors, and transcript-free evaluation. We address these gaps through three contributions: (1) a systematic comparison between text-based models with acoustic features, a novel audio-only architecture (AudioSeg) operating on learned audio representations, and multimodal LLMs; (2) empirical analysis of factors affecting performance, including transcript quality, acoustic features, duration, and speaker composition; and (3) formalized evaluation protocols contrasting transcript-dependent text-space protocols with transcript-invariant time-space protocols. Our experiments on YTSeg reveal that AudioSeg substantially outperforms text-based approaches, pauses provide the largest acoustic gains, and MLLMs remain limited by context length and weak instruction following, yet MLLMs are promising on shorter audio.",
    "summary": "",
    "translation": "超越转录文本：音频章节划分的新视角",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于音频处理（音频章节划分），属于语音/音频领域。虽然音频内容可能出现在某些推荐或搜索场景中，但标题本身并未表明与推荐系统、搜索或广告的核心技术（如排序、检索、用户建模）有任何直接关联，也未提及LLM、Transformer架构或异构数据统一建模等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08964v1": {
    "title": "A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents",
    "url": "https://www.alphaxiv.org/abs/2602.08964v1",
    "arxiv_id": "2602.08964v1",
    "authors": "Raghu Arghal, Fade Chen, Niall Dalton, Evgenii Kortukov, Calum McNamara, Angelos Nalmpantis, Moksh Nirvaan, Gabriele Sarti, Mario Giulianelli",
    "categories": "cs.LG, cs.AI, cs.CL, cs.CY",
    "pub_date": "2026-02-09 18:00:28",
    "ori_summary": "Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world toward a goal state. Behaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures. We then use probing methods to decode the agent's internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection. Our findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.",
    "summary": "",
    "translation": "语言模型智能体中目标导向性的行为与表征评估",
    "relevance_score": 2,
    "reasoning": "该论文主要评估语言模型智能体的目标导向行为，属于智能体评估范畴，与推荐系统、搜索或广告的核心技术关联较弱。虽然涉及语言模型，但未明确展示在推荐/搜索/广告领域的直接应用潜力，更偏向通用智能体行为研究。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08951v1": {
    "title": "How Should We Model the Probability of a Language?",
    "url": "https://www.alphaxiv.org/abs/2602.08951v1",
    "arxiv_id": "2602.08951v1",
    "authors": "Rasul Dent, Pedro Ortiz Suarez, Thibault Clérice, Benoît Sagot",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 17:46:56",
    "ori_summary": "Of the over 7,000 languages spoken in the world, commercial language identification (LID) systems only reliably identify a few hundred in written form. Research-grade systems extend this coverage under certain circumstances, but for most languages coverage remains patchy or nonexistent. This position paper argues that this situation is largely self-imposed. In particular, it arises from a persistent framing of LID as decontextualized text classification, which obscures the central role of prior probability estimation and is reinforced by institutional incentives that favor global, fixed-prior models. We argue that improving coverage for tail languages requires rethinking LID as a routing problem and developing principled ways to incorporate environmental cues that make languages locally plausible.",
    "summary": "",
    "translation": "我们应如何建模语言的概率？",
    "relevance_score": 2,
    "reasoning": "该标题涉及语言建模的基础理论，属于LLM核心技术的概率建模范畴，可能对语言模型的基础理解有贡献。然而，它没有明确指向推荐系统、搜索或广告的具体应用，也没有涉及Transformer架构改进或多模态数据处理，因此与当前关注点的直接相关性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08948v1": {
    "title": "CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute",
    "url": "https://www.alphaxiv.org/abs/2602.08948v1",
    "arxiv_id": "2602.08948v1",
    "authors": "Chen Jin, Ryutaro Tanno, Tom Diethe, Philip Teare",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2026-02-09 17:44:41",
    "ori_summary": "Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.",
    "summary": "",
    "translation": "CoRefine：基于置信度引导的自适应测试时计算自优化方法",
    "relevance_score": 3,
    "reasoning": "该论文标题涉及测试时自适应计算优化，属于模型效率技术范畴，可能对Transformer架构的效率改进有潜在价值。然而，标题未明确说明其与推荐系统、搜索或广告的具体应用关联，且未提及LLM或Transformer架构，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08945v1": {
    "title": "GitSearch: Enhancing Community Notes Generation with Gap-Informed Targeted Search",
    "url": "https://www.alphaxiv.org/abs/2602.08945v1",
    "arxiv_id": "2602.08945v1",
    "authors": "Sahajpreet Singh, Kokil Jaidka, Min-Yen Kan",
    "categories": "cs.CL, cs.CY",
    "pub_date": "2026-02-09 17:42:32",
    "ori_summary": "Community-based moderation offers a scalable alternative to centralized fact-checking, yet it faces significant structural challenges, and existing AI-based methods fail in \"cold start\" scenarios. To tackle these challenges, we introduce GitSearch (Gap-Informed Targeted Search), a framework that treats human-perceived quality gaps, such as missing context, etc., as first-class signals. GitSearch has a three-stage pipeline: identifying information deficits, executing real-time targeted web-retrieval to resolve them, and synthesizing platform-compliant notes. To facilitate evaluation, we present PolBench, a benchmark of 78,698 U.S. political tweets with their associated Community Notes. We find GitSearch achieves 99% coverage, almost doubling coverage over the state-of-the-art. GitSearch surpasses human-authored helpful notes with a 69% win rate and superior helpfulness scores (3.87 vs. 3.36), demonstrating retrieval effectiveness that balanced the trade-off between scale and quality.",
    "summary": "",
    "translation": "GitSearch：通过基于差距感知的定向搜索增强社区笔记生成",
    "relevance_score": 3,
    "reasoning": "该论文标题涉及社区笔记生成，这属于内容生成范畴，与您关注的推荐系统、搜索或广告排名核心领域不直接相关。虽然提到了搜索（Search），但上下文是用于生成笔记而非传统的信息检索或推荐任务，因此潜在应用不明确。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08874v1": {
    "title": "Is Reasoning Capability Enough for Safety in Long-Context Language Models?",
    "url": "https://www.alphaxiv.org/abs/2602.08874v1",
    "arxiv_id": "2602.08874v1",
    "authors": "Yu Fu, Haz Sameen Shahgir, Huanli Gong, Zhipeng Wei, N. Benjamin Erichson, Yue Dong",
    "categories": "cs.CL, cs.CR",
    "pub_date": "2026-02-09 16:35:14",
    "ori_summary": "Large language models (LLMs) increasingly combine long-context processing with advanced reasoning, enabling them to retrieve and synthesize information distributed across tens of thousands of tokens. A hypothesis is that stronger reasoning capability should improve safety by helping models recognize harmful intent even when it is not stated explicitly. We test this hypothesis in long-context settings where harmful intent is implicit and must be inferred through reasoning, and find that it does not hold. We introduce compositional reasoning attacks, a new threat model in which a harmful query is decomposed into incomplete fragments that scattered throughout a long context. The model is then prompted with a neutral reasoning query that induces retrieval and synthesis, causing the harmful intent to emerge only after composition. Evaluating 14 frontier LLMs on contexts up to 64k tokens, we uncover three findings: (1) models with stronger general reasoning capability are not more robust to compositional reasoning attacks, often assembling the intent yet failing to refuse; (2) safety alignment consistently degrades as context length increases; and (3) inference-time reasoning effort is a key mitigating factor: increasing inference-time compute reduces attack success by over 50 percentage points on GPT-oss-120b model. Together, these results suggest that safety does not automatically scale with reasoning capability, especially under long-context inference.",
    "summary": "",
    "translation": "推理能力是否足以确保长上下文语言模型的安全性？",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于LLM的安全性和长上下文能力评估，属于纯粹的NLP评估基准话题，与您关注的推荐系统、搜索或广告领域的技术进展、架构创新或直接应用无关。标题中提到的“安全性”也属于您明确排除的非技术性话题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08864v1": {
    "title": "Understanding Dynamic Compute Allocation in Recurrent Transformers",
    "url": "https://www.alphaxiv.org/abs/2602.08864v1",
    "arxiv_id": "2602.08864v1",
    "authors": "Ibraheem Muhammad Moosa, Suhas Lohit, Ye Wang, Moitreya Chatterjee, Wenpeng Yin",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2026-02-09 16:27:52",
    "ori_summary": "Token-level adaptive computation seeks to reduce inference cost by allocating more computation to harder tokens and less to easier ones. However, prior work is primarily evaluated on natural-language benchmarks using task-level metrics, where token-level difficulty is unobservable and confounded with architectural factors, making it unclear whether compute allocation truly aligns with underlying complexity. We address this gap through three contributions. First, we introduce a complexity-controlled evaluation paradigm using algorithmic and synthetic language tasks with parameterized difficulty, enabling direct testing of token-level compute allocation. Second, we propose ANIRA, a unified recurrent Transformer framework that supports per-token variable-depth computation while isolating compute allocation decisions from other model factors. Third, we use this framework to conduct a systematic analysis of token-level adaptive computation across alignment with complexity, generalization, and decision timing. Our results show that compute allocation aligned with task complexity can emerge without explicit difficulty supervision, but such alignment does not imply algorithmic generalization: models fail to extrapolate to unseen input sizes despite allocating additional computation. We further find that early compute decisions rely on static structural cues, whereas online halting more closely tracks algorithmic execution state.",
    "summary": "该论文研究如何为Transformer模型中的不同token动态分配计算资源以降低推理成本。其核心方法是提出ANIRA框架，通过统一循环Transformer架构实现基于token难度的可变深度计算，并建立复杂度可控的评估范式来分析计算分配与任务复杂度的对齐关系。",
    "translation": "理解循环Transformer中的动态计算分配",
    "relevance_score": 8,
    "reasoning": "该论文属于'Enabling Transformer Tech'范畴，研究循环Transformer中的动态计算分配机制，这直接关系到Transformer架构的效率优化。在推荐系统和搜索领域，这种动态计算分配技术可以显著提升长序列处理效率，优化资源利用，对于处理用户历史行为序列等长上下文场景具有重要应用价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接研究Transformer架构的效率优化，通过动态计算分配机制提升推理效率，对搜索和推荐系统的实时性能有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08857v1": {
    "title": "Discovering Interpretable Algorithms by Decompiling Transformers to RASP",
    "url": "https://www.alphaxiv.org/abs/2602.08857v1",
    "arxiv_id": "2602.08857v1",
    "authors": "Xinting Huang, Aleksandra Bakalova, Satwik Bhattamishra, William Merrill, Michael Hahn",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2026-02-09 16:22:29",
    "ori_summary": "Recent work has shown that the computations of Transformers can be simulated in the RASP family of programming languages. These findings have enabled improved understanding of the expressive capacity and generalization abilities of Transformers. In particular, Transformers have been suggested to length-generalize exactly on problems that have simple RASP programs. However, it remains open whether trained models actually implement simple interpretable programs. In this paper, we present a general method to extract such programs from trained Transformers. The idea is to faithfully re-parameterize a Transformer as a RASP program and then apply causal interventions to discover a small sufficient sub-program. In experiments on small Transformers trained on algorithmic and formal language tasks, we show that our method often recovers simple and interpretable RASP programs from length-generalizing transformers. Our results provide the most direct evidence so far that Transformers internally implement simple RASP programs.",
    "summary": "该论文研究如何从已训练的Transformer模型中提取可解释的算法程序。核心方法是先将Transformer忠实重参数化为RASP程序，再通过因果干预发现最小充分子程序，从而揭示模型内部实现的简单可解释算法。",
    "translation": "通过将Transformer反编译为RASP来发现可解释算法",
    "relevance_score": 8,
    "reasoning": "该论文涉及Transformer架构的可解释性研究，属于'Enabling Transformer Tech'范畴。通过反编译Transformer来发现可解释算法，这种技术可以应用于推荐系统、搜索和广告中的模型可解释性需求，帮助理解模型决策过程并提升系统透明度。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文提出了一种将Transformer模型反编译为可解释RASP程序的方法，直接揭示了模型内部算法实现机制，对理解Transformer架构的运作原理具有核心价值，属于Transformer架构基础研究的重要进展。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08829v1": {
    "title": "WildReward: Learning Reward Models from In-the-Wild Human Interactions",
    "url": "https://www.alphaxiv.org/abs/2602.08829v1",
    "arxiv_id": "2602.08829v1",
    "authors": "Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Lei Hou, Juanzi Li",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-02-09 16:00:30",
    "ori_summary": "Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.",
    "summary": "",
    "translation": "WildReward：从真实世界人类交互中学习奖励模型",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及从人类交互中学习奖励模型，这本质上是强化学习（RL）领域的研究。虽然奖励建模在理论上可能应用于推荐系统或搜索的某些方面（如个性化排序），但标题本身没有明确说明与RecSys/Search/Ads的具体关联，且RL论文若无明确相关性属于排除范围。因此，其潜在应用不明确，相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08826v1": {
    "title": "Affective Flow Language Model for Emotional Support Conversation",
    "url": "https://www.alphaxiv.org/abs/2602.08826v1",
    "arxiv_id": "2602.08826v1",
    "authors": "Chenghui Zou, Ning Wang, Tiesunlong Shen, Luwei Xiao, Chuan Ma, Xiangpeng Li, Rui Mao, Erik Cambria",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-02-09 15:58:50",
    "ori_summary": "Large language models (LLMs) have been widely applied to emotional support conversation (ESC). However, complex multi-turn support remains challenging.This is because existing alignment schemes rely on sparse outcome-level signals, thus offering limited supervision for intermediate strategy decisions. To fill this gap, this paper proposes affective flow language model for emotional support conversation (AFlow), a framework that introduces fine-grained supervision on dialogue prefixes by modeling a continuous affective flow along multi-turn trajectories. AFlow can estimate intermediate utility over searched trajectories and learn preference-consistent strategy transitions. To improve strategy coherence and empathetic response quality, a subpath-level flow-balance objective is presented to propagate preference signals to intermediate states. Experiment results show consistent and significant improvements over competitive baselines in diverse emotional contexts. Remarkably, AFlow with a compact open-source backbone outperforms proprietary LMMs such as GPT-4o and Claude-3.5 on major ESC metrics. Our code is available at https://github.com/chzou25-lgtm/AffectiveFlow.",
    "summary": "",
    "translation": "情感支持对话中的情感流语言模型",
    "relevance_score": 1,
    "reasoning": "该论文标题表明其专注于情感支持对话，这属于纯粹的LLM应用领域，与推荐系统、搜索或广告的核心技术无关。虽然涉及语言模型，但其应用场景（情感支持对话）与指定的技术焦点（推荐、搜索、广告中的LLM应用）没有直接关联，因此相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08819v1": {
    "title": "Bayesian Preference Learning for Test-Time Steerable Reward Models",
    "url": "https://www.alphaxiv.org/abs/2602.08819v1",
    "arxiv_id": "2602.08819v1",
    "authors": "Jiwoo Hong, Shao Tang, Zhipeng Wang",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2026-02-09 15:55:56",
    "ori_summary": "Reward models are central to aligning language models with human preferences via reinforcement learning (RL). As RL is increasingly applied to settings such as verifiable rewards and multi-objective alignment, RMs are expected to encode more complex and multifaceted preference distributions. However, classifier RMs remain static once trained, limiting their adaptability at test time. We propose Variational In-Context Reward Modeling (ICRM), a novel Bayesian reward modeling objective that enables test-time steerability via in-context preference demonstrations. ICRM casts reward modeling as amortized variational inference over a latent preference probability under the Bradley-Terry model using a conjugate Beta prior. We show that ICRM adapt to unseen preference distributions at test time for both single and multi-objective settings. With more in-context demonstrations, ICRM gains 34% accuracy on SafeRLHF and 9% accuracy on RM-Bench in the single-objective setting, while widening the Pareto frontier with a 4% gain in hypervolume on helpfulness and refusal benchmarks. We further study the practical applicability of ICRM for RL training, showing that it can effectively encode verifiable rewards by outperforming a conventional RM in math reasoning. Finally, we provide theoretical guarantees that the variational objective admits a global interior optimum with finite confidence, and we analyze how KL regularization mitigates reward over-optimization.",
    "summary": "",
    "translation": "贝叶斯偏好学习用于测试时可引导的奖励模型",
    "relevance_score": 3,
    "reasoning": "该论文涉及奖励模型和偏好学习，这些概念与强化学习（RL）相关。虽然奖励模型在推荐系统和广告中可能有潜在应用（例如，用于学习用户偏好），但论文标题没有明确表明与推荐系统、搜索或广告的直接相关性。它可能更偏向于通用的RL方法，而非特定于这些领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08796v1": {
    "title": "The Use of AI Tools to Develop and Validate Q-Matrices",
    "url": "https://www.alphaxiv.org/abs/2602.08796v1",
    "arxiv_id": "2602.08796v1",
    "authors": "Kevin Fan, Jacquelyn A. Bialo, Hongli Li",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2026-02-09 15:36:53",
    "ori_summary": "Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a reading comprehension test. In May 2025, multiple AI models were provided with the same training materials as human experts. Agreement among AI-generated Q-matrices, the validated Q-matrix, and human raters' Q-matrices was assessed using Cohen's kappa. Results showed substantial variation across AI models, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding that of all human experts. A follow-up analysis in January 2026 using newer AI versions, however, revealed lower agreement with the validated Q-matrix. Implications and directions for future research are discussed.",
    "summary": "",
    "translation": "使用人工智能工具开发和验证Q矩阵",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及AI在教育测量（Q矩阵）中的应用，属于特定领域应用而非核心推荐/搜索/广告技术。Q矩阵是认知诊断评估中的概念，与推荐系统、搜索或广告的架构、算法或应用无关，也不涉及LLM、Transformer或异构数据处理等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08793v1": {
    "title": "LakeHopper: Cross Data Lakes Column Type Annotation through Model Adaptation",
    "url": "https://www.alphaxiv.org/abs/2602.08793v1",
    "arxiv_id": "2602.08793v1",
    "authors": "Yushi Sun, Xujia Li, Nan Tang, Quanqing Xu, Chuanhui Yang, Lei Chen",
    "categories": "cs.CL, cs.DB",
    "pub_date": "2026-02-09 15:30:07",
    "ori_summary": "Column type annotation is vital for tasks like data cleaning, integration, and visualization. Recent solutions rely on resource-intensive language models fine-tuned on well-annotated columns from a particular set of tables, i.e., a source data lake. In this paper, we study whether we can adapt an existing pre-trained LM-based model to a new (i.e., target) data lake to minimize the annotations required on the new data lake. However, challenges include the source-target knowledge gap, selecting informative target data, and fine-tuning without losing shared knowledge exist. We propose LakeHopper, a framework that identifies and resolves the knowledge gap through LM interactions, employs a cluster-based data selection scheme for unannotated columns, and uses an incremental fine-tuning mechanism that gradually adapts the source model to the target data lake. Our experimental results validate the effectiveness of LakeHopper on two different data lake transfers under both low-resource and high-resource settings.",
    "summary": "",
    "translation": "LakeHopper：通过模型自适应实现跨数据湖的列类型标注",
    "relevance_score": 2,
    "reasoning": "该论文涉及数据湖中的列类型标注，属于数据预处理和模式发现领域，与推荐系统、搜索或广告的核心技术（如排序、召回、用户建模）无直接关联。虽然数据质量对下游任务有影响，但该工作本身不涉及LLM、Transformer架构创新，也不直接处理用户行为序列、上下文特征或多模态统一建模等关键问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08783v1": {
    "title": "Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure",
    "url": "https://www.alphaxiv.org/abs/2602.08783v1",
    "arxiv_id": "2602.08783v1",
    "authors": "Zirui Li, Xuefeng Bai, Kehai Chen, Yizhi Li, Jian Yang, Chenghua Lin, Min Zhang",
    "categories": "cs.AI, cs.CL",
    "pub_date": "2026-02-09 15:25:12",
    "ori_summary": "Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.",
    "summary": "",
    "translation": "潜在思维链中的动态性：因果结构的实证研究",
    "relevance_score": 2,
    "reasoning": "该论文研究思维链（CoT）中的因果结构，属于LLM推理机制的基础研究。虽然思维链技术可能间接应用于搜索或推荐系统的解释生成，但论文本身聚焦于因果推理的实证分析，与用户行为建模、排序算法或广告投放等核心领域缺乏直接关联，且未明确讨论其在推荐/搜索/广告中的具体应用场景。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08740v1": {
    "title": "Map of Encoders -- Mapping Sentence Encoders using Quantum Relative Entropy",
    "url": "https://www.alphaxiv.org/abs/2602.08740v1",
    "arxiv_id": "2602.08740v1",
    "authors": "Gaifan Zhang, Danushka Bollegala",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 14:41:46",
    "ori_summary": "We propose a method to compare and visualise sentence encoders at scale by creating a map of encoders where each sentence encoder is represented in relation to the other sentence encoders. Specifically, we first represent a sentence encoder using an embedding matrix of a sentence set, where each row corresponds to the embedding of a sentence. Next, we compute the Pairwise Inner Product (PIP) matrix for a sentence encoder using its embedding matrix. Finally, we create a feature vector for each sentence encoder reflecting its Quantum Relative Entropy (QRE) with respect to a unit base encoder. We construct a map of encoders covering 1101 publicly available sentence encoders, providing a new perspective of the landscape of the pre-trained sentence encoders. Our map accurately reflects various relationships between encoders, where encoders with similar attributes are proximally located on the map. Moreover, our encoder feature vectors can be used to accurately infer downstream task performance of the encoders, such as in retrieval and clustering tasks, demonstrating the faithfulness of our map.",
    "summary": "",
    "translation": "编码器映射图——利用量子相对熵映射句子编码器",
    "relevance_score": 2,
    "reasoning": "该论文涉及句子编码器分析和量子相对熵，属于基础NLP技术研究。虽然编码器技术可能间接支持推荐/搜索系统，但论文标题未明确展示与推荐系统、搜索或广告的直接关联，也未提及在异构数据统一建模或Transformer架构效率方面的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08716v1": {
    "title": "PERSPECTRA: A Scalable and Configurable Pluralist Benchmark of Perspectives from Arguments",
    "url": "https://www.alphaxiv.org/abs/2602.08716v1",
    "arxiv_id": "2602.08716v1",
    "authors": "Shangrui Nie, Kian Omoomi, Lucie Flek, Zhixue Zhao, Charles Welch",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 14:25:07",
    "ori_summary": "Pluralism, the capacity to engage with diverse perspectives without collapsing them into a single viewpoint, is critical for developing large language models that faithfully reflect human heterogeneity. Yet this characteristic has not been carefully examined in the LLM research community and remains absent from most alignment studies. Debate-oriented sources provide a natural entry point for pluralism research. Previous work builds on online debate sources but remains constrained by costly human validation. Other debate-rich platforms such as Reddit and Kialo also offer promising material: Reddit provides linguistic diversity and scale but lacks clear argumentative structure, while Kialo supplies explicit pro/con graphs but remains overly concise and detached from natural discourse. We introduce PERSPECTRA, a pluralist benchmark that integrates the structural clarity of Kialo debate graphs with the linguistic diversity of real Reddit discussions. Using a controlled retrieval-and-expansion pipeline, we construct 3,810 enriched arguments spanning 762 pro/con stances on 100 controversial topics. Each opinion is expanded to multiple naturalistic variants, enabling robust evaluation of pluralism. We initialise three tasks with PERSPECTRA: opinion counting (identifying distinct viewpoints), opinion matching (aligning supporting stances and discourse to source opinions), and polarity check (inferring aggregate stance in mixed discourse). Experiments with state-of-the-art open-source and proprietary LLMs, highlight systematic failures, such as overestimating the number of viewpoints and misclassifying concessive structures, underscoring the difficulty of pluralism-aware understanding and reasoning. By combining diversity with structure, PERSPECTRA establishes the first scalable, configurable benchmark for evaluating how well models represent, distinguish, and reason over multiple perspectives.",
    "summary": "",
    "translation": "PERSPECTRA：一个可扩展且可配置的多元观点基准，源自论证",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及论证中的多元观点基准，这属于NLP领域的评估基准研究，与用户关注的核心领域进展（推荐系统、搜索、广告）、LLM使能技术、Transformer架构进展或LLM直接应用无关。虽然观点分析在理论上可能与用户理解相关，但该标题明确指向基准测试，属于用户指定的无关主题（如评估基准、纯NLP中心主题），且未显示与推荐/搜索/广告系统的直接技术关联或应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08709v1": {
    "title": "FactSim: Fact-Checking for Opinion Summarization",
    "url": "https://www.alphaxiv.org/abs/2602.08709v1",
    "arxiv_id": "2602.08709v1",
    "authors": "Leandro Anghinoni, Jorge Sanchez",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 14:21:19",
    "ori_summary": "We explore the need for more comprehensive and precise evaluation techniques for generative artificial intelligence (GenAI) in text summarization tasks, specifically in the area of opinion summarization. Traditional methods, which leverage automated metrics to compare machine-generated summaries from a collection of opinion pieces, e.g. product reviews, have shown limitations due to the paradigm shift introduced by large language models (LLM). This paper addresses these shortcomings by proposing a novel, fully automated methodology for assessing the factual consistency of such summaries. The method is based on measuring the similarity between the claims in a given summary with those from the original reviews, measuring the coverage and consistency of the generated summary. To do so, we rely on a simple approach to extract factual assessment from texts that we then compare and summarize in a suitable score. We demonstrate that the proposed metric attributes higher scores to similar claims, regardless of whether the claim is negated, paraphrased, or expanded, and that the score has a high correlation to human judgment when compared to state-of-the-art metrics.",
    "summary": "",
    "translation": "FactSim：面向观点摘要的事实核查",
    "relevance_score": 1,
    "reasoning": "该论文专注于观点摘要中的事实核查，属于纯粹的NLP应用领域，与推荐系统、搜索或广告的核心技术无关。它不涉及异构数据建模、Transformer架构改进，也没有明显的推荐/搜索/广告应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08698v1": {
    "title": "Challenges in Translating Technical Lectures: Insights from the NPTEL",
    "url": "https://www.alphaxiv.org/abs/2602.08698v1",
    "arxiv_id": "2602.08698v1",
    "authors": "Basudha Raje, Sadanand Venkatraman, Nandana TP, Soumyadeepa Das, Polkam Poojitha, M. Vijaykumar, Tanima Bagchi, Hema A. Murthy",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 14:15:42",
    "ori_summary": "This study examines the practical applications and methodological implications of Machine Translation in Indian Languages, specifically Bangla, Malayalam, and Telugu, within emerging translation workflows and in relation to existing evaluation frameworks. The choice of languages prioritized in this study is motivated by a triangulation of linguistic diversity, which illustrates the significance of multilingual accommodation of educational technology under NEP 2020. This is further supported by the largest MOOC portal, i.e., NPTEL, which has served as a corpus to facilitate the arguments presented in this paper. The curation of a spontaneous speech corpora that accounts for lucid delivery of technical concepts, considering the retention of suitable register and lexical choices are crucial in a diverse country like India. The findings of this study highlight metric-specific sensitivity and the challenges of morphologically rich and semantically compact features when tested against surface overlapping metrics.",
    "summary": "",
    "translation": "技术讲座翻译中的挑战：来自NPTEL的见解",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于技术讲座翻译这一特定领域，属于语言处理或教育技术范畴。它不涉及推荐系统、搜索、广告、LLM技术、Transformer架构或异构数据建模等任何相关主题，因此与当前关注点完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08696v1": {
    "title": "Prototype-Based Disentanglement for Controllable Dysarthric Speech Synthesis",
    "url": "https://www.alphaxiv.org/abs/2602.08696v1",
    "arxiv_id": "2602.08696v1",
    "authors": "Haoshen Wang, Xueli Zhong, Bingbing Lin, Jia Huang, Xingduo Pan, Shengxiang Liang, Nizhuan Wang, Wai Ting Siok",
    "categories": "cs.SD, cs.CL",
    "pub_date": "2026-02-09 14:14:51",
    "ori_summary": "Dysarthric speech exhibits high variability and limited labeled data, posing major challenges for both automatic speech recognition (ASR) and assistive speech technologies. Existing approaches rely on synthetic data augmentation or speech reconstruction, yet often entangle speaker identity with pathological articulation, limiting controllability and robustness. In this paper, we propose ProtoDisent-TTS, a prototype-based disentanglement TTS framework built on a pre-trained text-to-speech backbone that factorizes speaker timbre and dysarthric articulation within a unified latent space. A pathology prototype codebook provides interpretable and controllable representations of healthy and dysarthric speech patterns, while a dual-classifier objective with a gradient reversal layer enforces invariance of speaker embeddings to pathological attributes. Experiments on the TORGO dataset demonstrate that this design enables bidirectional transformation between healthy and dysarthric speech, leading to consistent ASR performance gains and robust, speaker-aware speech reconstruction.",
    "summary": "",
    "translation": "基于原型的解耦可控构音障碍语音合成",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音合成领域，特别是针对构音障碍这一医疗相关应用。虽然涉及可控生成和解耦技术，但明确属于'纯粹语音论文，与推荐系统/搜索/广告无明确关联'的无关主题范畴，且与医疗应用相关，这也在无关主题列表中。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08688v1": {
    "title": "Old wine in old glasses: Comparing computational and qualitative methods in identifying incivility on Persian Twitter during the #MahsaAmini movement",
    "url": "https://www.alphaxiv.org/abs/2602.08688v1",
    "arxiv_id": "2602.08688v1",
    "authors": "Hossein Kermani, Fatemeh Oudlajani, Pardis Yarahmadi, Hamideh Mahdi Soltani, Mohammad Makki, Zahra HosseiniKhoo",
    "categories": "cs.CL, cs.CY",
    "pub_date": "2026-02-09 14:10:24",
    "ori_summary": "This paper compares three approaches to detecting incivility in Persian tweets: human qualitative coding, supervised learning with ParsBERT, and large language models (ChatGPT). Using 47,278 tweets from the #MahsaAmini movement in Iran, we evaluate the accuracy and efficiency of each method. ParsBERT substantially outperforms seven evaluated ChatGPT models in identifying hate speech. We also find that ChatGPT struggles not only with subtle cases but also with explicitly uncivil content, and that prompt language (English vs. Persian) does not meaningfully affect its outputs. The study provides a detailed comparison of these approaches and clarifies their strengths and limitations for analyzing hate speech in a low-resource language context.",
    "summary": "",
    "translation": "旧瓶装旧酒：在#MahsaAmini运动期间，比较计算方法和定性方法在识别波斯语Twitter不文明言论方面的效果",
    "relevance_score": 1,
    "reasoning": "该论文主要涉及社交媒体内容分析、不文明言论检测和定性研究方法，这些主题与推荐系统、搜索或广告的核心技术进展、LLM应用或Transformer架构改进无关。虽然涉及社交媒体数据，但焦点是内容分析和定性方法，而非推荐、搜索或广告的算法、架构或应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08672v1": {
    "title": "Learning to Judge: LLMs Designing and Applying Evaluation Rubrics",
    "url": "https://www.alphaxiv.org/abs/2602.08672v1",
    "arxiv_id": "2602.08672v1",
    "authors": "Clemencia Siro, Pourya Aliannejadi, Mohammad Aliannejadi",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2026-02-09 13:56:06",
    "ori_summary": "Large language models (LLMs) are increasingly used as evaluators for natural language generation, applying human-defined rubrics to assess system outputs. However, human rubrics are often static and misaligned with how models internally represent language quality. We introduce GER-Eval (Generating Evaluation Rubrics for Evaluation) to investigate whether LLMs can design and apply their own evaluation rubrics. We evaluate the semantic coherence and scoring reliability of LLM-defined criteria and their alignment with human criteria. LLMs reliably generate interpretable and task-aware evaluation dimensions and apply them consistently within models, but their scoring reliability degrades in factual and knowledge-intensive settings. Closed-source models such as GPT-4o achieve higher agreement and cross-model generalization than open-weight models such as Llama. Our findings position evaluation as a learned linguistic capability of LLMs, consistent within models but fragmented across them, and call for new methods that jointly model human and LLM evaluative language to improve reliability and interpretability.",
    "summary": "",
    "translation": "学会评判：大型语言模型设计与应用评估标准",
    "relevance_score": 2,
    "reasoning": "该论文标题表明其核心关注LLM如何设计和应用评估标准，这属于纯粹的LLM评估基准研究范畴，而非LLM在推荐/搜索/广告领域的直接应用或使能技术。虽然评估方法可能间接影响系统优化，但论文本身缺乏明确的推荐/搜索/广告应用指向，更侧重于NLP领域的评估方法论。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08658v1": {
    "title": "Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.08658v1",
    "arxiv_id": "2602.08658v1",
    "authors": "Mingzi Cao, Xingwei Tan, Mahmud Akhter, Marco Valentino, Maria Liakata, Xi Wang, Nikolaos Aletras",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 13:51:48",
    "ori_summary": "Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to $14.60$) across realistic tasks.",
    "summary": "",
    "translation": "基础推理范式诱导语言模型实现跨领域泛化",
    "relevance_score": 6,
    "reasoning": "该论文探讨语言模型的基础推理范式与跨领域泛化能力，属于'Enabling LLM Tech'范畴，因为更强大的泛化能力对推荐系统、搜索和广告中的少样本学习、冷启动问题有直接应用价值。然而，论文标题未明确说明具体技术机制或与推荐/搜索/广告的直接联系，因此相关性中等。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2602.08632v1": {
    "title": "We Should Separate Memorization from Copyright",
    "url": "https://www.alphaxiv.org/abs/2602.08632v1",
    "arxiv_id": "2602.08632v1",
    "authors": "Adi Haviv, Niva Elkin-Koren, Uri Hacohen, Roi Livni, Shay Moran",
    "categories": "cs.CY, cs.AI, cs.CL, cs.CV, cs.LG",
    "pub_date": "2026-02-09 13:24:06",
    "ori_summary": "The widespread use of foundation models has introduced a new risk factor of copyright issue. This issue is leading to an active, lively and on-going debate amongst the data-science community as well as amongst legal scholars. Where claims and results across both sides are often interpreted in different ways and leading to different implications. Our position is that much of the technical literature relies on traditional reconstruction techniques that are not designed for copyright analysis. As a result, memorization and copying have been conflated across both technical and legal communities and in multiple contexts. We argue that memorization, as commonly studied in data science, should not be equated with copying and should not be used as a proxy for copyright infringement. We distinguish technical signals that meaningfully indicate infringement risk from those that instead reflect lawful generalization or high-frequency content. Based on this analysis, we advocate for an output-level, risk-based evaluation process that aligns technical assessments with established copyright standards and provides a more principled foundation for research, auditing, and policy.",
    "summary": "",
    "translation": "我们应将记忆与版权分离",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及版权问题，属于法律/伦理范畴，与我的技术焦点无关。我的焦点是推荐系统、搜索、广告中的技术进展，以及LLM/Transformer架构的进步及其应用，不包括隐私、公平性、伦理等非技术主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08625v1": {
    "title": "Do Multilingual LLMs have specialized language heads?",
    "url": "https://www.alphaxiv.org/abs/2602.08625v1",
    "arxiv_id": "2602.08625v1",
    "authors": "Muhammad Naufil",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 13:15:17",
    "ori_summary": "Multilingual large language models (LLMs) have gained significant popularity for their ability to process and generate text across multiple languages. However, deploying these models in production can be inefficient when only a subset of the supported languages is of interest. There has been some research conducted on identifying whether machine translation models have language-specific or language-agnostic heads, however no research has been conducted for multilingual LLMs, to the best of our knowledge, that as we know are capable of performing diverse tasks beyond just translation. This paper explores whether multilingual LLMs have specialized language attention heads for each language, and investigates the possibility of removing language-specific heads for unwanted languages without degrading performance in the targeted languages. Our findings could inform more efficient deployment strategies for multilingual LLMs, enabling reduced model complexity while maintaining high accuracy for targeted languages.",
    "summary": "",
    "translation": "多语言大语言模型是否具有专门的语言头？",
    "relevance_score": 3,
    "reasoning": "该论文探讨多语言LLMs的内部机制，属于核心LLM技术研究，但未明确说明其在推荐系统、搜索或广告领域的潜在应用。虽然理解模型架构对优化多语言场景可能有间接价值，但缺乏直接的应用连接使其相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08607v1": {
    "title": "VocalNet-MDM: Accelerating Streaming Speech LLM via Self-Distilled Masked Diffusion Modeling",
    "url": "https://www.alphaxiv.org/abs/2602.08607v1",
    "arxiv_id": "2602.08607v1",
    "authors": "Ziyang Cheng, Yuhao Wang, Heyang Liu, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang",
    "categories": "cs.CL, cs.SD",
    "pub_date": "2026-02-09 12:52:59",
    "ori_summary": "Recent Speech Large Language Models~(LLMs) have achieved impressive capabilities in end-to-end speech interaction. However, the prevailing autoregressive paradigm imposes strict serial constraints, limiting generation efficiency and introducing exposure bias. In this paper, we investigate Masked Diffusion Modeling~(MDM) as a non-autoregressive paradigm for speech LLMs and introduce VocalNet-MDM. To adapt MDM for streaming speech interaction, we address two critical challenges: training-inference mismatch and iterative overhead. We propose Hierarchical Block-wise Masking to align training objectives with the progressive masked states encountered during block diffusion decoding, and Iterative Self-Distillation to compress multi-step refinement into fewer steps for low-latency inference. Trained on a limited scale of only 6K hours of speech data, VocalNet-MDM achieves a 3.7$\\times$--10$\\times$ decoding speedup and reduces first-chunk latency by 34\\% compared to AR baselines. It maintains competitive recognition accuracy while achieving state-of-the-art text quality and speech naturalness, demonstrating that MDM is a promising and scalable alternative for low-latency, efficient speech LLMs.",
    "summary": "",
    "translation": "VocalNet-MDM：通过自蒸馏掩码扩散建模加速流式语音大语言模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于语音处理和流式LLM加速，属于纯语音技术领域，与RecSys/Search/Ads的核心关注点（文本/多模态推荐、搜索排序、广告排名）没有直接关联。虽然提到了LLM加速技术，但针对的是语音模态而非推荐/搜索/广告中常见的文本或异构数据场景，因此不符合任何当前关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08600v1": {
    "title": "Beyond Scalar Scores: Reinforcement Learning for Error-Aware Quality Estimation of Machine Translation",
    "url": "https://www.alphaxiv.org/abs/2602.08600v1",
    "arxiv_id": "2602.08600v1",
    "authors": "Archchana Sindhujan, Girish A. Koushik, Shenbin Qian, Diptesh Kanojia, Constantin Orăsan",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 12:42:41",
    "ori_summary": "Quality Estimation (QE) aims to assess the quality of machine translation (MT) outputs without relying on reference translations, making it essential for real-world, large-scale MT evaluation. Large Language Models (LLMs) have shown significant promise in advancing the field of quality estimation of machine translation. However, most of the QE approaches solely rely on scalar quality scores, offering no explicit information about the translation errors that should drive these judgments. Moreover, for low-resource languages where annotated QE data is limited, existing approaches struggle to achieve reliable performance. To address these challenges, we introduce the first segment-level QE dataset for English to Malayalam, a severely resource-scarce language pair in the QE domain, comprising human-annotated Direct Assessment (DA) scores and Translation Quality Remarks (TQR), which are short, contextual, free-form annotator comments that describe translation errors. We further introduce ALOPE-RL, a policy-based reinforcement learning framework that trains efficient adapters based on policy rewards derived from DA score and TQR. Integrating error-aware rewards with ALOPE-RL, enables LLMs to reason about translation quality beyond numeric scores. Despite being trained on a small-scale QE dataset, ALOPE-RL achieves state-of-the-art performance on English to Malayalam QE using compact LLMs (<=4B parameters}) fine-tuned with LoRA and 4-bit quantization, outperforming both larger LLM-based baselines and leading encoder-based QE models. Our results demonstrate that error-aware, policy-based learning can deliver strong QE performance under limited data and compute budgets. We release our dataset, code, and trained models to support future research.",
    "summary": "",
    "translation": "超越标量分数：基于强化学习的机器翻译错误感知质量评估",
    "relevance_score": 1,
    "reasoning": "该论文专注于机器翻译质量评估，属于纯粹的NLP评估任务，与推荐系统、搜索或广告的核心技术无关。虽然涉及强化学习，但未展示在推荐/搜索/广告领域的明确应用潜力，因此不符合任何关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08567v1": {
    "title": "ValueFlow: Measuring the Propagation of Value Perturbations in Multi-Agent LLM Systems",
    "url": "https://www.alphaxiv.org/abs/2602.08567v1",
    "arxiv_id": "2602.08567v1",
    "authors": "Jinnuo Liu, Chuke Liu, Hua Shen",
    "categories": "cs.MA, cs.CL",
    "pub_date": "2026-02-09 12:06:07",
    "ori_summary": "Multi-agent large language model (LLM) systems increasingly consist of agents that observe and respond to one another's outputs. While value alignment is typically evaluated for isolated models, how value perturbations propagate through agent interactions remains poorly understood. We present ValueFlow, a perturbation-based evaluation framework for measuring and analyzing value drift in multi-agent systems. ValueFlow introduces a 56-value evaluation dataset derived from the Schwartz Value Survey and quantifies agents' value orientations during interaction using an LLM-as-a-judge protocol. Building on this measurement layer, ValueFlow decomposes value drift into agent-level response behavior and system-level structural effects, operationalized by two metrics: beta-susceptibility, which measures an agent's sensitivity to perturbed peer signals, and system susceptibility (SS), which captures how node-level perturbations affect final system outputs. Experiments across multiple model backbones, prompt personas, value dimensions, and network structures show that susceptibility varies widely across values and is strongly shaped by structural topology.",
    "summary": "",
    "translation": "ValueFlow：测量多智能体大语言模型系统中价值扰动的传播",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及多智能体LLM系统中的价值扰动传播测量，这属于LLM系统层面的分析而非核心架构或应用创新。虽然提到了LLM系统，但主要关注价值扰动传播这一特定现象，与当前关注的推荐系统、搜索、广告领域的核心进展、Transformer架构改进或直接LLM应用缺乏明确关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08561v1": {
    "title": "Automating Computational Reproducibility in Social Science: Comparing Prompt-Based and Agent-Based Approaches",
    "url": "https://www.alphaxiv.org/abs/2602.08561v1",
    "arxiv_id": "2602.08561v1",
    "authors": "Syed Mehtab Hussain Shah, Frank Hopfgartner, Arnim Bleier",
    "categories": "cs.SE, cs.CL",
    "pub_date": "2026-02-09 11:59:59",
    "ori_summary": "Reproducing computational research is often assumed to be as simple as rerunning the original code with provided data. In practice, missing packages, fragile file paths, version conflicts, or incomplete logic frequently cause analyses to fail, even when materials are shared. This study investigates whether large language models and AI agents can automate the diagnosis and repair of such failures, making computational results easier to reproduce and verify. We evaluate this using a controlled reproducibility testbed built from five fully reproducible R-based social science studies. Realistic failures were injected, ranging from simple issues to complex missing logic, and two automated repair workflows were tested in clean Docker environments. The first workflow is prompt-based, repeatedly querying language models with structured prompts of varying context, while the second uses agent-based systems that inspect files, modify code, and rerun analyses autonomously. Across prompt-based runs, reproduction success ranged from 31-79 percent, with performance strongly influenced by prompt context and error complexity. Complex cases benefited most from additional context. Agent-based workflows performed substantially better, with success rates of 69-96 percent across all complexity levels. These results suggest that automated workflows, especially agent-based systems, can significantly reduce manual effort and improve reproduction success across diverse error types. Unlike prior benchmarks, our testbed isolates post-publication repair under controlled failure modes, allowing direct comparison of prompt-based and agent-based approaches.",
    "summary": "",
    "translation": "社会科学中计算可重复性的自动化：基于提示与基于智能体方法的比较",
    "relevance_score": 1,
    "reasoning": "该论文标题关注社会科学领域的计算可重复性自动化方法比较，属于特定领域应用（社会科学），与推荐系统、搜索或广告的核心技术进展、LLM/Transformer架构改进或直接应用无关。标题中提到的“提示”可能涉及LLM提示工程，但论文焦点是社会科学研究流程，而非LLM在推荐/搜索/广告中的技术应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08548v1": {
    "title": "How Do Language Models Understand Tables? A Mechanistic Analysis of Cell Location",
    "url": "https://www.alphaxiv.org/abs/2602.08548v1",
    "arxiv_id": "2602.08548v1",
    "authors": "Xuanliang Zhang, Dingzirui Wang, Keyan Xu, Qingfu Zhu, Wanxiang Che",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 11:47:34",
    "ori_summary": "While Large Language Models (LLMs) are increasingly deployed for table-related tasks, the internal mechanisms enabling them to process linearized two-dimensional structured tables remain opaque. In this work, we investigate the process of table understanding by dissecting the atomic task of cell location. Through activation patching and complementary interpretability techniques, we delineate the table understanding mechanism into a sequential three-stage pipeline: Semantic Binding, Coordinate Localization, and Information Extraction. We demonstrate that models locate the target cell via an ordinal mechanism that counts discrete delimiters to resolve coordinates. Furthermore, column indices are encoded within a linear subspace that allows for precise steering of model focus through vector arithmetic. Finally, we reveal that models generalize to multi-cell location tasks by multiplexing the identical attention heads identified during atomic location. Our findings provide a comprehensive explanation of table understanding within Transformer architectures.",
    "summary": "",
    "translation": "语言模型如何理解表格？一种关于单元格位置的机制分析",
    "relevance_score": 3,
    "reasoning": "该论文探讨语言模型理解表格的机制，属于LLM基础研究（Enabling LLM Tech），可能有助于改进RecSys/Search/Ads中处理表格化数据（如用户特征表、商品属性表）的能力。但标题未明确指向具体应用，且机制分析可能偏理论，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08503v1": {
    "title": "Learning Self-Correction in Vision-Language Models via Rollout Augmentation",
    "url": "https://www.alphaxiv.org/abs/2602.08503v1",
    "arxiv_id": "2602.08503v1",
    "authors": "Yi Ding, Ziliang Qiu, Bolian Li, Ruqi Zhang",
    "categories": "cs.CV, cs.CL, cs.LG",
    "pub_date": "2026-02-09 10:55:13",
    "ori_summary": "Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\\times$ training time per step.",
    "summary": "",
    "translation": "通过回滚增强在视觉语言模型中学习自我纠正",
    "relevance_score": 3,
    "reasoning": "该论文涉及视觉语言模型（VLM）的自我纠正机制，属于VLM技术范畴。虽然VLM与异构数据处理有概念相似性，但论文标题明确聚焦于视觉-语言模态，没有明确提及推荐系统、搜索或广告领域的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08498v1": {
    "title": "Characterizing, Evaluating, and Optimizing Complex Reasoning",
    "url": "https://www.alphaxiv.org/abs/2602.08498v1",
    "arxiv_id": "2602.08498v1",
    "authors": "Haoran Zhang, Yafu Li, Zhi Wang, Zhilin Wang, Shunkai Zhang, Xiaoye Qu, Yu Cheng",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 10:51:14",
    "ori_summary": "Large Reasoning Models (LRMs) increasingly rely on reasoning traces with complex internal structures. However, existing work lacks a unified answer to three fundamental questions: (1) what defines high-quality reasoning, (2) how to reliably evaluate long, implicitly structured reasoning traces, and (3) how to use such evaluation signals for reasoning optimization. To address these challenges, we provide a unified perspective. (1) We introduce the ME$^2$ principle to characterize reasoning quality along macro- and micro-level concerning efficiency and effectiveness. (2) Built on this principle, we model reasoning traces as directed acyclic graphs (DAGs) and develop a DAG-based pairwise evaluation method, capturing complex reasoning structures. (3) Based on this method, we construct the TRM-Preference dataset and train a Thinking Reward Model (TRM) to evaluate reasoning quality at scale. Experiments show that thinking rewards serve as an effective optimization signal. At test time, selecting better reasoning leads to better outcomes (up to 19.3% gain), and during RL training, thinking rewards enhance reasoning and performance (up to 3.9% gain) across diverse tasks.",
    "summary": "",
    "translation": "复杂推理的表征、评估与优化",
    "relevance_score": 3,
    "reasoning": "该论文标题聚焦于复杂推理，这可能与LLM的核心能力相关，属于'Enabling LLM Tech'范畴。然而，标题未明确指向其在推荐系统、搜索或广告中的具体应用，仅暗示了潜在的基础技术改进。因此，相关性较低，但若论文内容涉及推理优化在个性化或排序任务中的应用，则可能具有间接价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08489v1": {
    "title": "Beyond Correctness: Learning Robust Reasoning via Transfer",
    "url": "https://www.alphaxiv.org/abs/2602.08489v1",
    "arxiv_id": "2602.08489v1",
    "authors": "Hyunseok Lee, Soheil Abbasloo, Jihoon Tack, Jinwoo Shin",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2026-02-09 10:41:44",
    "ori_summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient.",
    "summary": "",
    "translation": "超越正确性：通过迁移学习鲁棒推理",
    "relevance_score": 3,
    "reasoning": "该论文标题涉及推理鲁棒性，可能属于LLM核心技术进步范畴，但未明确说明与推荐系统、搜索或广告的具体应用关联。需要更多信息判断其是否属于'Enabling LLM Tech'类别并具有明确的领域应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08437v1": {
    "title": "Large Language Models and Impossible Language Acquisition: \"False Promise\" or an Overturn of our Current Perspective towards AI",
    "url": "https://www.alphaxiv.org/abs/2602.08437v1",
    "arxiv_id": "2602.08437v1",
    "authors": "Ziyan wang, Longlong Ma",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 09:50:12",
    "ori_summary": "In Chomsky's provocative critique \"The False Promise of CHATGPT,\" Large Language Models (LLMs) are characterized as mere pattern predictors that do not acquire languages via intrinsic causal and self-correction structures like humans, therefore are not able to distinguish impossible languages. It stands as a representative in a fundamental challenge to the intellectual foundations of AI, for it integrally synthesizes major issues in methodologies within LLMs and possesses an iconic a priori rationalist perspective. We examine this famous critic from both the perspective in pre-existing literature of linguistics and psychology as well as a research based on an experiment inquiring the capacity of learning both possible and impossible languages among LLMs. We constructed a set of syntactically impossible languages by applying certain transformations to English. These include reversing whole sentences, and adding negation based on word-count parity. Two rounds of controlled experiments were each conducted on GPT-2 small models and long short-term memory (LSTM) models. Statistical analysis (Welch's t-test) shows GPT2 small models underperform in learning all of the impossible languages compared to their performance on the possible language (p<.001). On the other hand, LSTM models' performance tallies with Chomsky's argument, suggesting the irreplaceable role of the evolution of transformer architecture. Based on theoretical analysis and empirical findings, we propose a new vision within Chomsky's theory towards LLMs, and a shift of theoretical paradigm outside Chomsky, from his \"rationalist-romantics\" paradigm to functionalism and empiricism in LLMs research.",
    "summary": "",
    "translation": "大语言模型与不可能的语言习得：“虚假承诺”还是对我们当前人工智能视角的颠覆",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于LLM的语言习得哲学讨论和AI视角的宏观反思，属于纯粹理论性、哲学性的探讨。虽然提及LLM，但完全没有涉及技术进展、架构改进或在推荐/搜索/广告领域的应用潜力，完全不符合当前关注的任何技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08426v1": {
    "title": "Prism: Spectral-Aware Block-Sparse Attention",
    "url": "https://www.alphaxiv.org/abs/2602.08426v1",
    "arxiv_id": "2602.08426v1",
    "authors": "Xinghao Wang, Pengyu Wang, Xiaoran Liu, Fangxu Liu, Jason Chu, Kai Song, Xipeng Qiu",
    "categories": "cs.CL, cs.AI, cs.CV",
    "pub_date": "2026-02-09 09:31:06",
    "ori_summary": "Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a \"blind spot\" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to $\\mathbf{5.1\\times}$ speedup.",
    "summary": "论文研究块稀疏注意力中识别相关块的效率瓶颈问题。核心思想是通过频谱分析揭示均值池化与RoPE交互导致局部位置信息丢失的理论根源，提出训练无关的频谱感知方法，将块选择分解为高低频分支并应用能量温度校准来恢复衰减的位置信号。",
    "translation": "Prism：基于频谱感知的块稀疏注意力机制",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能Transformer技术'范畴，提出了一种新的注意力机制改进方案。频谱感知的块稀疏注意力通过优化计算模式，能够显著提升Transformer在长序列处理中的效率，这对于推荐系统中的用户行为序列建模和搜索中的长文档理解具有直接应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文直接针对Transformer注意力机制的核心效率瓶颈提出创新解决方案，通过频谱分析改进块稀疏注意力，属于Transformer架构进步的关键领域，对长上下文LLM的搜索推荐应用具有直接价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08404v1": {
    "title": "TEAM: Temporal-Spatial Consistency Guided Expert Activation for MoE Diffusion Language Model Acceleration",
    "url": "https://www.alphaxiv.org/abs/2602.08404v1",
    "arxiv_id": "2602.08404v1",
    "authors": "Linye Wei, Zixiang Luo, Pingzhi Tang, Meng Li",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 09:05:46",
    "ori_summary": "Diffusion large language models (dLLMs) have recently gained significant attention due to their inherent support for parallel decoding. Building on this paradigm, Mixture-of-Experts (MoE) dLLMs with autoregressive (AR) initialization have further demonstrated strong performance competitive with mainstream AR models. However, we identify a fundamental mismatch between MoE architectures and diffusion-based decoding. Specifically, a large number of experts are activated at each denoising step, while only a small subset of tokens is ultimately accepted, resulting in substantial inference overhead and limiting their deployment in latency-sensitive applications. In this work, we propose TEAM, a plug-and-play framework that accelerates MoE dLLMs by enabling more accepted tokens with fewer activated experts. TEAM is motivated by the observation that expert routing decisions exhibit strong temporal consistency across denoising levels as well as spatial consistency across token positions. Leveraging these properties, TEAM employs three complementary expert activation and decoding strategies, conservatively selecting necessary experts for decoded and masked tokens and simultaneously performing aggressive speculative exploration across multiple candidates. Experimental results demonstrate that TEAM achieves up to 2.2x speedup over vanilla MoE dLLM, with negligible performance degradation. Code is released at https://github.com/PKU-SEC-Lab/TEAM-MoE-dLLM.",
    "summary": "",
    "translation": "TEAM：时空一致性引导的专家激活机制用于MoE扩散语言模型加速",
    "relevance_score": 4,
    "reasoning": "该论文涉及Transformer架构中的MoE（专家混合）技术，属于'Enabling Transformer Tech'范畴，具有潜在的应用价值。然而，其具体应用场景是扩散语言模型，而非直接针对推荐系统、搜索或广告领域，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2602.08382v1": {
    "title": "Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2602.08382v1",
    "arxiv_id": "2602.08382v1",
    "authors": "Zhuoen Chen, Dongfang Li, Meishan Zhang, Baotian Hu, Min Zhang",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-02-09 08:33:11",
    "ori_summary": "Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.",
    "summary": "该论文研究LLM长上下文处理中的计算效率和信息遗忘问题，核心思想是通过分块压缩、动态记忆选择和强化学习端到端优化，实现高效的长序列推理。",
    "translation": "基于端到端强化学习的压缩内存动态长上下文推理",
    "relevance_score": 8,
    "reasoning": "该论文涉及长上下文推理和压缩内存技术，属于Transformer架构效率优化（Enabling Transformer Tech），直接适用于处理推荐/搜索中的长用户序列。端到端强化学习组件虽然通常被排除，但这里与核心的压缩内存长上下文处理紧密结合，对推荐系统的序列建模有明确应用价值。",
    "rerank_relevance_score": 9,
    "rerank_reasoning": "该论文提出基于强化学习的动态长上下文推理框架，直接针对LLM在长序列处理中的核心效率问题，其压缩记忆和选择性召回机制对搜索推荐系统的长序列建模具有重要应用潜力。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08377v1": {
    "title": "Reinforcement Learning with Backtracking Feedback",
    "url": "https://www.alphaxiv.org/abs/2602.08377v1",
    "arxiv_id": "2602.08377v1",
    "authors": "Bilgehan Sel, Vaishakh Keshava, Phillip Wallis, Lukas Rutishauser, Ming Jin, Dingcheng Li",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2026-02-09 08:23:19",
    "ori_summary": "Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient \"backtrack by x tokens\" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility.",
    "summary": "",
    "translation": "基于回溯反馈的强化学习",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及强化学习（RL），但未表明与推荐系统、搜索或广告领域的明确关联。根据用户指定的无关主题，强化学习论文若无明确相关性应排除。标题未提供任何关于如何应用于推荐/搜索/广告的线索，因此判断为不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08371v1": {
    "title": "ViGoEmotions: A Benchmark Dataset For Fine-grained Emotion Detection on Vietnamese Texts",
    "url": "https://www.alphaxiv.org/abs/2602.08371v1",
    "arxiv_id": "2602.08371v1",
    "authors": "Hung Quang Tran, Nam Tien Pham, Son T. Luu, Kiet Van Nguyen",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 08:10:40",
    "ori_summary": "Emotion classification plays a significant role in emotion prediction and harmful content detection. Recent advancements in NLP, particularly through large language models (LLMs), have greatly improved outcomes in this field. This study introduces ViGoEmotions -- a Vietnamese emotion corpus comprising 20,664 social media comments in which each comment is classified into 27 fine-grained distinct emotions. To evaluate the quality of the dataset and its impact on emotion classification, eight pre-trained Transformer-based models were evaluated under three preprocessing strategies: preserving original emojis with rule-based normalization, converting emojis into textual descriptions, and applying ViSoLex, a model-based lexical normalization system. Results show that converting emojis into text often improves the performance of several BERT-based baselines, while preserving emojis yields the best results for ViSoBERT and CafeBERT. In contrast, removing emojis generally leads to lower performance. ViSoBERT achieved the highest Macro F1-score of 61.50% and Weighted F1-score of 63.26%. Strong performance was also observed from CafeBERT and PhoBERT. These findings highlight that while the proposed corpus can support diverse architectures effectively, preprocessing strategies and annotation quality remain key factors influencing downstream performance.",
    "summary": "",
    "translation": "ViGoEmotions：面向越南语文本细粒度情感检测的基准数据集",
    "relevance_score": 1,
    "reasoning": "该论文标题表明这是一个针对越南语文本情感检测的基准数据集，属于特定语言的情感分析任务。这完全属于纯粹的自然语言处理（NLP）领域，与推荐系统、搜索或广告的核心技术、LLM应用、Transformer架构进展或异构数据统一建模均无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08369v1": {
    "title": "MemAdapter: Fast Alignment across Agent Memory Paradigms via Generative Subgraph Retrieval",
    "url": "https://www.alphaxiv.org/abs/2602.08369v1",
    "arxiv_id": "2602.08369v1",
    "authors": "Xin Zhang, Kailai Yang, Chenyue Li, Hao Li, Qiyu Wei, Jun'ichi Tsujii, Sophia Ananiadou",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2026-02-09 08:09:25",
    "ori_summary": "Memory mechanism is a core component of LLM-based agents, enabling reasoning and knowledge discovery over long-horizon contexts. Existing agent memory systems are typically designed within isolated paradigms (e.g., explicit, parametric, or latent memory) with tightly coupled retrieval methods that hinder cross-paradigm generalization and fusion. In this work, we take a first step toward unifying heterogeneous memory paradigms within a single memory system. We propose MemAdapter, a memory retrieval framework that enables fast alignment across agent memory paradigms. MemAdapter adopts a two-stage training strategy: (1) training a generative subgraph retriever from the unified memory space, and (2) adapting the retriever to unseen memory paradigms by training a lightweight alignment module through contrastive learning. This design improves the flexibility for memory retrieval and substantially reduces alignment cost across paradigms. Comprehensive experiments on three public evaluation benchmarks demonstrate that the generative subgraph retriever consistently outperforms five strong agent memory systems across three memory paradigms and agent model scales. Notably, MemAdapter completes cross-paradigm alignment within 13 minutes on a single GPU, achieving superior performance over original memory retrievers with less than 5% of training compute. Furthermore, MemAdapter enables effective zero-shot fusion across memory paradigms, highlighting its potential as a plug-and-play solution for agent memory systems.",
    "summary": "",
    "translation": "MemAdapter：通过生成式子图检索实现跨智能体记忆范式的快速对齐",
    "relevance_score": 4,
    "reasoning": "该论文涉及智能体记忆对齐和检索增强生成技术，属于LLM应用范畴，可能通过改进记忆检索机制间接应用于推荐或搜索系统的用户历史建模。然而，标题未明确指向推荐/搜索/广告领域，且核心是智能体记忆而非直接的应用场景，因此相关性中等。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2602.08367v1": {
    "title": "WorldTravel: A Realistic Multimodal Travel-Planning Benchmark with Tightly Coupled Constraints",
    "url": "https://www.alphaxiv.org/abs/2602.08367v1",
    "arxiv_id": "2602.08367v1",
    "authors": "Zexuan Wang, Chenghao Yang, Yingqi Que, Zhenzhu Yang, Huaqing Yuan, Yiwen Wang, Zhengxuan Jiang, Shengjie Fang, Zhenhe Wu, Zhaohui Wang, Zhixin Yao, Jiashuo Liu, Jincheng Ren, Yuzhen Li, Yang Yang, Jiaheng Liu, Jian Yang, Zaiyuan Wang, Ge Zhang, Zhoufutu Wen, Wenhao Huang",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 08:03:30",
    "ori_summary": "Real-world autonomous planning requires coordinating tightly coupled constraints where a single decision dictates the feasibility of all subsequent actions. However, existing benchmarks predominantly feature loosely coupled constraints solvable through local greedy decisions and rely on idealized data, failing to capture the complexity of extracting parameters from dynamic web environments. We introduce \\textbf{WorldTravel}, a benchmark comprising 150 real-world travel scenarios across 5 cities that demand navigating an average of 15+ interdependent temporal and logical constraints. To evaluate agents in realistic deployments, we develop \\textbf{WorldTravel-Webscape}, a multi-modal environment featuring over 2,000 rendered webpages where agents must perceive constraint parameters directly from visual layouts to inform their planning. Our evaluation of 10 frontier models reveals a significant performance collapse: even the state-of-the-art GPT-5.2 achieves only 32.67\\% feasibility in text-only settings, which plummets to 19.33\\% in multi-modal environments. We identify a critical Perception-Action Gap and a Planning Horizon threshold at approximately 10 constraints where model reasoning consistently fails, suggesting that perception and reasoning remain independent bottlenecks. These findings underscore the need for next-generation agents that unify high-fidelity visual perception with long-horizon reasoning to handle brittle real-world logistics.",
    "summary": "",
    "translation": "WorldTravel：一个具有紧耦合约束的逼真多模态旅行规划基准",
    "relevance_score": 2,
    "reasoning": "该论文标题表明这是一个旅行规划基准数据集，涉及多模态数据（可能包括文本、图像、位置等）。虽然多模态建模与'VLM类比'焦点有表面相似性，但旅行规划属于特定领域应用，而非通用的推荐/搜索/广告技术。没有明确证据表明该基准或方法可推广到用户行为序列、上下文特征等推荐系统核心问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08343v1": {
    "title": "ManifoldKV: Training-Free KV Cache Compression via Euclidean Outlier Detection",
    "url": "https://www.alphaxiv.org/abs/2602.08343v1",
    "arxiv_id": "2602.08343v1",
    "authors": "Debajyoti Datta, Trishala Neeraj, Bibek Paudel, Vyom Sharma, Subhabrata Mukherjee",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2026-02-09 07:28:55",
    "ori_summary": "Long-context inference is constrained by KV-cache memory, which grows linearly with sequence length; KV-cache compression therefore hinges on reliably selecting which past tokens to retain. Most geometry-based eviction methods score keys by cosine similarity to a global centroid, but cosine is scale-invariant and can discard magnitude cues that distinguish semantically salient tokens. We propose ManifoldKV, a training-free scorer that ranks tokens by Euclidean distance to the key centroid, capturing both angular and radial deviations. On the RULER benchmark, ManifoldKV achieves 95.7% accuracy at 4K-16K contexts with 20% compression; matching the best geometric baseline while improving robustness in two regimes where cosine scoring fails. First, on multi-key retrieval, ManifoldKV reduces directional collisions, achieving 92.4% vs KeyDiff's 77.0% (+15.4 points) on 3-key NIAH at 50% compression. Second, to address dilution and performance collapse of global centroids at 64K context, we introduce WindowedManifoldKV, which restores accuracy to 84.3% at 25% compression, a 49-point recovery over global L2 and +3.2 points over KeyDiff. The method requires only 3 lines of code and works across 4 architectures without tuning.",
    "summary": "论文研究长上下文推理中KV缓存内存线性增长的核心问题。核心方法是提出ManifoldKV，一种基于欧氏距离（而非余弦相似度）的无训练令牌评分器，通过同时捕捉角度和径向偏差来更可靠地选择保留的过去令牌，并针对超长上下文进一步提出窗口化变体以解决全局质心稀释问题。",
    "translation": "ManifoldKV：基于欧几里得离群点检测的无训练键值缓存压缩方法",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能Transformer技术'范畴，专注于KV缓存压缩这一关键的Transformer效率优化技术。KV缓存压缩技术可直接应用于推荐系统、搜索和广告中的大规模Transformer部署，通过减少内存占用和计算开销来提升推理效率，对处理长序列用户行为数据具有重要价值。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文直接针对Transformer架构中的KV缓存效率问题，提出了一种无需训练的新型压缩方法，属于Transformer架构优化的核心进展，对长上下文推理应用（如搜索、推荐）有直接价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08336v1": {
    "title": "UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models",
    "url": "https://www.alphaxiv.org/abs/2602.08336v1",
    "arxiv_id": "2602.08336v1",
    "authors": "Cheng Yang, Chufan Shi, Bo Shui, Yaokang Wu, Muzi Tao, Huijuan Wang, Ivan Yee Lee, Yong Liu, Xuezhe Ma, Taylor Berg-Kirkpatrick",
    "categories": "cs.CL, cs.CV",
    "pub_date": "2026-02-09 07:17:57",
    "ori_summary": "To elicit capabilities for addressing complex and implicit visual requirements, recent unified multimodal models increasingly adopt chain-of-thought reasoning to guide image generation. However, the actual effect of reasoning on visual synthesis remains unclear. We present UReason, a diagnostic benchmark for reasoning-driven image generation that evaluates whether reasoning can be faithfully executed in pixels. UReason contains 2,000 instances across five task families: Code, Arithmetic, Spatial, Attribute, and Text reasoning. To isolate the role of reasoning traces, we introduce an evaluation framework comparing direct generation, reasoning-guided generation, and de-contextualized generation which conditions only on the refined prompt. Across eight open-source unified models, we observe a consistent Reasoning Paradox: Reasoning traces generally improve performance over direct generation, yet retaining intermediate thoughts as conditioning context often hinders visual synthesis, and conditioning only on the refined prompt yields substantial gains. Our analysis suggests that the bottleneck lies in contextual interference rather than insufficient reasoning capacity. UReason provides a principled testbed for studying reasoning in unified models and motivates future methods that effectively integrate reasoning for visual generation while mitigating interference.",
    "summary": "",
    "translation": "UReason：统一多模态模型中推理悖论的基准测试",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及多模态模型的基准测试和推理评估，这属于通用评估基准范畴，而非您关注的推荐/搜索/广告领域核心技术进展。虽然统一多模态模型可能与您提到的“VLM类比”有表面关联，但该论文明显聚焦于评估基准（如“Benchmarking”所示），这属于您明确排除的“评估基准”类别，缺乏对推荐/搜索/广告系统的直接应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08332v1": {
    "title": "Latent Reasoning with Supervised Thinking States",
    "url": "https://www.alphaxiv.org/abs/2602.08332v1",
    "arxiv_id": "2602.08332v1",
    "authors": "Ido Amos, Avi Caciularu, Mor Geva, Amir Globerson, Jonathan Herzig, Lior Shani, Idan Szpektor",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-02-09 07:12:41",
    "ori_summary": "Reasoning with a chain-of-thought (CoT) enables Large Language Models (LLMs) to solve complex tasks but incurs significant inference costs due to the generation of long rationales. We propose Thinking States, a method that performs reasoning {\\em while} the input is processing. Specifically, Thinking States generates sequences of thinking tokens every few input tokens, transforms the thoughts back into embedding space, and adds them to the following input tokens. This has two key advantages. First, it captures the recurrent nature of CoT, but where the thought tokens are generated as input is processing. Second, since the thoughts are represented as tokens, they can be learned from natural language supervision, and using teacher-forcing, which is parallelizable. Empirically, Thinking States outperforms other latent reasoning methods on multiple reasoning tasks, narrowing the gap to CoT on math problems, and matching its performance on 2-Hop QA with improved latency. On state-tracking tasks, we show Thinking States leads to stronger reasoning behavior than CoT, successfully extrapolating to longer sequences than seen during training.",
    "summary": "",
    "translation": "基于监督思维状态的潜在推理",
    "relevance_score": 3,
    "reasoning": "该标题涉及推理过程的建模，可能属于LLM推理优化技术，但未明确说明与推荐系统、搜索或广告的具体关联。对于'使能技术'类论文，需要展示在RecSys/Search/Ads领域的潜在应用可能性，而该标题缺乏这种明确指向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08322v1": {
    "title": "An Attention-over-Attention Generative Model for Joint Multiple Intent Detection and Slot Filling",
    "url": "https://www.alphaxiv.org/abs/2602.08322v1",
    "arxiv_id": "2602.08322v1",
    "authors": "Wei Zhu",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 06:52:34",
    "ori_summary": "In task-oriented dialogue systems, spoken language understanding (SLU) is a critical component, which consists of two sub-tasks, intent detection and slot filling. Most existing methods focus on the single-intent SLU, where each utterance only has one intent. However, in real-world scenarios users usually express multiple intents in an utterance, which poses a challenge for existing dialogue systems and datasets. In this paper, we propose a generative framework to simultaneously address multiple intent detection and slot filling. In particular, an attention-over-attention decoder is proposed to handle the variable number of intents and the interference between the two sub-tasks by incorporating an inductive bias into the process of multi-task learning. Besides, we construct two new multi-intent SLU datasets based on single-intent utterances by taking advantage of the next sentence prediction (NSP) head of the BERT model. Experimental results demonstrate that our proposed attention-over-attention generative model achieves state-of-the-art performance on two public datasets, MixATIS and MixSNIPS, and our constructed datasets.",
    "summary": "",
    "translation": "一种基于注意力叠加注意力机制的生成式模型，用于联合多意图检测与槽位填充",
    "relevance_score": 2,
    "reasoning": "该论文主要关注对话系统中的意图识别和槽位填充任务，属于自然语言处理领域。虽然使用了注意力机制（属于Transformer技术），但其应用场景是对话理解而非推荐/搜索/广告系统，且没有明确展示如何应用于这些领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08321v1": {
    "title": "Improving Data and Reward Design for Scientific Reasoning in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.08321v1",
    "arxiv_id": "2602.08321v1",
    "authors": "Zijie Chen, Zhenghao Lin, Xiao Liu, Zhenzhong Lan, Yeyun Gong, Peng Cheng",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 06:52:03",
    "ori_summary": "Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr.SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.",
    "summary": "",
    "translation": "改进大型语言模型科学推理的数据与奖励设计",
    "relevance_score": 1,
    "reasoning": "该论文专注于科学推理领域的LLM改进，属于特定领域应用而非核心推荐/搜索/广告领域。虽然涉及LLM技术，但论文关注的是科学推理这一与当前焦点无关的领域，没有明确展示在推荐系统、搜索或广告中的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08305v1": {
    "title": "JUSTICE: Judicial Unified Synthesis Through Intermediate Conclusion Emulation for Automated Judgment Document Generation",
    "url": "https://www.alphaxiv.org/abs/2602.08305v1",
    "arxiv_id": "2602.08305v1",
    "authors": "Binglin Wu, Yingyi Zhang, Xiannneg Li",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 06:27:51",
    "ori_summary": "Automated judgment document generation is a significant yet challenging legal AI task. As the conclusive written instrument issued by a court, a judgment document embodies complex legal reasoning. However, existing methods often oversimplify this complex process, particularly by omitting the ``Pre-Judge'' phase, a crucial step where human judges form a preliminary conclusion. This omission leads to two core challenges: 1) the ineffective acquisition of foundational judicial elements, and 2) the inadequate modeling of the Pre-Judge process, which collectively undermine the final document's legal soundness. To address these challenges, we propose \\textit{\\textbf{J}udicial \\textbf{U}nified \\textbf{S}ynthesis \\textbf{T}hrough \\textbf{I}ntermediate \\textbf{C}onclusion \\textbf{E}mulation} (JUSTICE), a novel framework that emulates the ``Search $\\rightarrow$ Pre-Judge $\\rightarrow$ Write'' cognitive workflow of human judges. Specifically, it introduces the Pre-Judge stage through three dedicated components: Referential Judicial Element Retriever (RJER), Intermediate Conclusion Emulator (ICE), and Judicial Unified Synthesizer (JUS). RJER first retrieves legal articles and a precedent case to establish a referential foundation. ICE then operationalizes the Pre-Judge phase by generating a verifiable intermediate conclusion. Finally, JUS synthesizes these inputs to craft the final judgment. Experiments on both an in-domain legal benchmark and an out-of-distribution dataset show that JUSTICE significantly outperforms strong baselines, with substantial gains in legal accuracy, including a 4.6\\% improvement in prison term prediction. Our findings underscore the importance of explicitly modeling the Pre-Judge process to enhance the legal coherence and accuracy of generated judgment documents.",
    "summary": "",
    "translation": "JUSTICE：通过中间结论模拟实现司法统一合成，用于自动化判决文书生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于法律领域的自动化文档生成，属于特定领域应用而非推荐系统、搜索或广告的核心技术。虽然涉及文本生成，但缺乏与推荐/搜索/广告系统的直接关联，且未涉及Transformer架构改进、多模态建模或LLM在推荐/搜索/广告中的创新应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08294v1": {
    "title": "When Does Context Help? Error Dynamics of Contextual Information in Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.08294v1",
    "arxiv_id": "2602.08294v1",
    "authors": "Dingzirui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 05:58:41",
    "ori_summary": "Contextual information at inference time, such as demonstrations, retrieved knowledge, or interaction history, can substantially improve large language models (LLMs) without parameter updates, yet its theoretical role remains poorly understood beyond specific settings such as in-context learning (ICL). We present a unified theoretical framework for analyzing the effect of arbitrary contextual information in Transformer-based LLMs. Our analysis characterizes contextual influence through output error dynamics. In a single-layer Transformer, we prove that the context-conditioned error vector decomposes additively into the baseline error vector and a contextual correction vector. This yields necessary geometric conditions for error reduction: the contextual correction must align with the negative baseline error and satisfy a norm constraint. We further show that the contextual correction norm admits an explicit upper bound determined by context-query relevance and complementarity. These results extend to multi-context and multi-layer Transformers. Experiments across ICL, retrieval-augmented generation, and memory evolution validate our theory and motivate a principled context selection strategy that improves performance by $0.6\\%$.",
    "summary": "",
    "translation": "上下文何时有效？大型语言模型中上下文信息的误差动态分析",
    "relevance_score": 4,
    "reasoning": "该论文探讨LLM中上下文信息的误差动态，属于'Enabling LLM Tech'范畴，对理解LLM在搜索/推荐中处理上下文的能力有潜在价值。但标题未明确指向RecSys/Search/Ads应用，且可能涉及评估基准等NLP中心话题，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2602.08289v1": {
    "title": "Knowledge Augmented Entity and Relation Extraction for Legal Documents with Hypergraph Neural Network",
    "url": "https://www.alphaxiv.org/abs/2602.08289v1",
    "arxiv_id": "2602.08289v1",
    "authors": "Binglin Wu, Xianneng Li",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 05:46:11",
    "ori_summary": "With the continuous progress of digitization in Chinese judicial institutions, a substantial amount of electronic legal document information has been accumulated. To unlock its potential value, entity and relation extraction for legal documents has emerged as a crucial task. However, existing methods often lack domain-specific knowledge and fail to account for the unique characteristics of the judicial domain. In this paper, we propose an entity and relation extraction algorithm based on hypergraph neural network (Legal-KAHRE) for drug-related judgment documents. Firstly, we design a candidate span generator based on neighbor-oriented packing strategy and biaffine mechanism, which identifies spans likely to contain entities. Secondly, we construct a legal dictionary with judicial domain knowledge and integrate it into text encoding representation using multi-head attention. Additionally, we incorporate domain-specific cases like joint crimes and combined punishment for multiple crimes into the hypergraph structure design. Finally, we employ a hypergraph neural network for higher-order inference via message passing. Experimental results on the CAIL2022 information extraction dataset demonstrate that our method significantly outperforms existing baseline models.",
    "summary": "",
    "translation": "基于超图神经网络的法律文档知识增强实体与关系抽取",
    "relevance_score": 2,
    "reasoning": "该论文专注于法律文档的实体与关系抽取，属于特定领域的信息提取任务。虽然涉及知识增强和超图神经网络等高级技术，但这些方法在推荐系统、搜索或广告领域的直接应用潜力有限，且法律文档处理与当前关注的异构数据统一建模关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08281v1": {
    "title": "New Skills or Sharper Primitives? A Probabilistic Perspective on the Emergence of Reasoning in RLVR",
    "url": "https://www.alphaxiv.org/abs/2602.08281v1",
    "arxiv_id": "2602.08281v1",
    "authors": "Zhilin Wang, Yafu Li, Shunkai Zhang, Zhi Wang, Haoran Zhang, Xiaoye Qu, Yu Cheng",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 05:23:13",
    "ori_summary": "Whether Reinforcement Learning with Verifiable Rewards (RLVR) endows Large Language Models (LLMs) with new capabilities or merely elicits latent traces remains a central debate. In this work, we align with the former view, proposing a probabilistic framework where capability is defined by instance-level solvability. We hypothesize that the emergence of complex reasoning can be driven by sharpening atomic step probabilities, which enables models to overcome the exponential decay of success rates inherent in multi-step reasoning chains. Utilizing the Algebrarium framework, we train models exclusively on single-step operations and evaluate their performance on unseen multi-step tasks. Our empirical results confirm that: (1) RLVR incentivizes the exploration of previously inaccessible solution paths by amplifying the model's existing skills; (2) composite performance is strictly governed by the joint probability of atomic steps, evidenced by high Pearson correlation coefficients ($ρ\\in [0.69, 0.96]$); and (3) RLVR, acting as a global optimizer, can cause specific skills to be sacrificed to maximize aggregate reward. Our work offers a novel explanation for emergent abilities in RLVR, suggesting that the iterative optimization of solvable problems enables models to develop the capabilities to tackle previously unsolvable scenarios.",
    "summary": "",
    "translation": "新技能还是更锐利的原语？从概率视角看RLVR中推理能力的涌现",
    "relevance_score": 2,
    "reasoning": "该论文标题明确提及强化学习（RL），而RL论文若无明确与推荐系统/搜索/广告的相关性，属于应排除的无关主题。标题中的'RLVR'可能指强化学习与视觉推理，但未展示与推荐/搜索/广告领域的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08275v1": {
    "title": "Linguistics and Human Brain: A Perspective of Computational Neuroscience",
    "url": "https://www.alphaxiv.org/abs/2602.08275v1",
    "arxiv_id": "2602.08275v1",
    "authors": "Fudong Zhang, Bo Chai, Yujie Wu, Wai Ting Siok, Nizhuan Wang",
    "categories": "q-bio.NC, cs.CL",
    "pub_date": "2026-02-09 05:10:26",
    "ori_summary": "Elucidating the language-brain relationship requires bridging the methodological gap between the abstract theoretical frameworks of linguistics and the empirical neural data of neuroscience. Serving as an interdisciplinary cornerstone, computational neuroscience formalizes the hierarchical and dynamic structures of language into testable neural models through modeling, simulation, and data analysis. This enables a computational dialogue between linguistic hypotheses and neural mechanisms. Recent advances in deep learning, particularly large language models (LLMs), have powerfully advanced this pursuit. Their high-dimensional representational spaces provide a novel scale for exploring the neural basis of linguistic processing, while the \"model-brain alignment\" framework offers a methodology to evaluate the biological plausibility of language-related theories.",
    "summary": "",
    "translation": "语言学与人类大脑：计算神经科学的视角",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于计算神经科学与语言学的交叉研究，属于神经科学和认知科学领域，与推荐系统、搜索或广告的核心技术进展无关。论文内容可能涉及大脑语言处理机制的基础研究，没有明确指向推荐系统、搜索或广告领域的应用或技术启示。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08274v1": {
    "title": "Language Modeling and Understanding Through Paraphrase Generation and Detection",
    "url": "https://www.alphaxiv.org/abs/2602.08274v1",
    "arxiv_id": "2602.08274v1",
    "authors": "Jan Philip Wahle",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-02-09 05:09:03",
    "ori_summary": "Language enables humans to share knowledge, reason about the world, and pass on strategies for survival and innovation across generations. At the heart of this process is not just the ability to communicate but also the remarkable flexibility in how we can express ourselves. We can express the same thoughts in virtually infinite ways using different words and structures - this ability to rephrase and reformulate expressions is known as paraphrase. Modeling paraphrases is a keystone to meaning in computational language models; being able to construct different variations of texts that convey the same meaning or not shows strong abilities of semantic understanding. If computational language models are to represent meaning, they must understand and control the different aspects that construct the same meaning as opposed to different meanings at a fine granularity. Yet most existing approaches reduce paraphrasing to a binary decision between two texts or to producing a single rewrite of a source, obscuring which linguistic factors are responsible for meaning preservation. In this thesis, I propose that decomposing paraphrases into their constituent linguistic aspects (paraphrase types) offers a more fine-grained and cognitively grounded view of semantic equivalence. I show that even advanced machine learning models struggle with this task. Yet, when explicitly trained on paraphrase types, models achieve stronger performance on related paraphrase tasks and downstream applications. For example, in plagiarism detection, language models trained on paraphrase types surpass human baselines: 89.6% accuracy compared to 78.4% for plagiarism cases from Wikipedia, and 66.5% compared to 55.7% for plagiarism of scientific papers from arXiv. In identifying duplicate questions on Quora, models trained with paraphrase types improve over models trained on binary pairs. Furthermore, I demonstrate that...",
    "summary": "",
    "translation": "通过释义生成与检测实现语言建模与理解",
    "relevance_score": 3,
    "reasoning": "该论文主要关注语言建模和释义技术，属于核心NLP领域。虽然语言建模是LLM的基础，但论文标题未明确指向推荐系统、搜索或广告的具体应用场景，也未提及Transformer架构改进或异构数据处理等当前关注点。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08252v1": {
    "title": "Language Predicts Identity Fusion Across Cultures and Reveals Divergent Pathways to Violence",
    "url": "https://www.alphaxiv.org/abs/2602.08252v1",
    "arxiv_id": "2602.08252v1",
    "authors": "Devin R. Wright, Justin E. Lane, F. LeRon Shults",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 04:11:23",
    "ori_summary": "In light of increasing polarization and political violence, understanding the psychological roots of extremism is increasingly important. Prior research shows that identity fusion predicts willingness to engage in extreme acts. We evaluate the Cognitive Linguistic Identity Fusion Score, a method that uses cognitive linguistic patterns, LLMs, and implicit metaphor to measure fusion from language. Across datasets from the United Kingdom and Singapore, this approach outperforms existing methods in predicting validated fusion scores. Applied to extremist manifestos, two distinct high-fusion pathways to violence emerge: ideologues tend to frame themselves in terms of group, forming kinship bonds; whereas grievance-driven individuals frame the group in terms of their personal identity. These results refine theories of identity fusion and provide a scalable tool aiding fusion research and extremism detection.",
    "summary": "",
    "translation": "语言预测跨文化身份融合并揭示通向暴力的不同路径",
    "relevance_score": 1,
    "reasoning": "该论文标题关注语言与身份融合、文化差异及暴力行为的心理学/社会学关系，属于社会科学领域。它不涉及推荐系统、搜索、广告、LLM技术或Transformer架构的任何技术方面，也没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08238v1": {
    "title": "On convexity and efficiency in semantic systems",
    "url": "https://www.alphaxiv.org/abs/2602.08238v1",
    "arxiv_id": "2602.08238v1",
    "authors": "Nathaniel Imel, Noga Zaslavasky",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 03:24:28",
    "ori_summary": "There are two widely held characterizations of human semantic category systems: (1) they form convex partitions of conceptual spaces, and (2) they are efficient for communication. While prior work observed that convexity and efficiency co-occur in color naming, the analytical relation between them and why they co-occur have not been well understood. We address this gap by combining analytical and empirical analyses that build on the Information Bottleneck (IB) framework for semantic efficiency. First, we show that convexity and efficiency are distinct in the sense that neither entails the other: there are convex systems which are inefficient, and optimally-efficient systems that are non-convex. Crucially, however, the IB-optimal systems are mostly convex in the domain of color naming, explaining the main empirical basis for the convexity approach. Second, we show that efficiency is a stronger predictor for discriminating attested color naming systems from hypothetical variants, with convexity adding negligible improvement on top of that. Finally, we discuss a range of empirical phenomena that convexity cannot account for but efficiency can. Taken together, our work suggests that while convexity and efficiency can yield similar structural observations, they are fundamentally distinct, with efficiency providing a more comprehensive account of semantic typology.",
    "summary": "",
    "translation": "论语义系统中的凸性与效率",
    "relevance_score": 2,
    "reasoning": "该标题讨论语义系统中的凸性和效率，属于理论优化范畴，与推荐系统、搜索或广告的核心技术进展、LLM应用或Transformer架构改进无直接关联。虽然语义系统可能涉及搜索中的语义理解，但标题聚焦于数学凸性和效率理论，缺乏明确的实际应用指向，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08237v1": {
    "title": "Document Reconstruction Unlocks Scalable Long-Context RLVR",
    "url": "https://www.alphaxiv.org/abs/2602.08237v1",
    "arxiv_id": "2602.08237v1",
    "authors": "Yao Xiao, Lei Wang, Yue Deng, Guanzheng Chen, Ziqi Jin, Jung-jae Kim, Xiaoli Li, Roy Ka-wei Lee, Lidong Bing",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 03:23:23",
    "ori_summary": "Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.",
    "summary": "",
    "translation": "文档重构解锁可扩展的长上下文RLVR",
    "relevance_score": 3,
    "reasoning": "该论文标题涉及长上下文处理和文档重构，可能属于LLM效率或可扩展性技术（Enabling LLM Tech），但未明确说明与推荐系统、搜索或广告的具体应用关联。RLVR（可能指强化学习与视觉推理）的缩写暗示可能涉及强化学习，而根据要求，若无明确相关性则视为低相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08236v1": {
    "title": "When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning",
    "url": "https://www.alphaxiv.org/abs/2602.08236v1",
    "arxiv_id": "2602.08236v1",
    "authors": "Shoubin Yu, Yue Zhang, Zun Wang, Jaehong Yoon, Huaxiu Yao, Mingyu Ding, Mohit Bansal",
    "categories": "cs.CV, cs.AI, cs.CL",
    "pub_date": "2026-02-09 03:21:48",
    "ori_summary": "Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.",
    "summary": "",
    "translation": "何时想象与想象多少：基于世界模型的自适应测试时缩放用于视觉空间推理",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视觉空间推理和世界模型，属于计算机视觉领域。虽然标题提到了自适应测试时缩放技术，但缺乏与推荐系统、搜索或广告领域的直接联系。世界模型和自适应缩放技术可能对多模态推荐系统有潜在应用价值，但论文标题未明确表明这种应用方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08235v1": {
    "title": "When Benign Inputs Lead to Severe Harms: Eliciting Unsafe Unintended Behaviors of Computer-Use Agents",
    "url": "https://www.alphaxiv.org/abs/2602.08235v1",
    "arxiv_id": "2602.08235v1",
    "authors": "Jaylen Jones, Zhehao Zhang, Yuting Ning, Eric Fosler-Lussier, Pierre-Luc St-Charles, Yoshua Bengio, Dawn Song, Yu Su, Huan Sun",
    "categories": "cs.CL, cs.AI, cs.CR",
    "pub_date": "2026-02-09 03:20:11",
    "ori_summary": "Although computer-use agents (CUAs) hold significant potential to automate increasingly complex OS workflows, they can demonstrate unsafe unintended behaviors that deviate from expected outcomes even under benign input contexts. However, exploration of this risk remains largely anecdotal, lacking concrete characterization and automated methods to proactively surface long-tail unintended behaviors under realistic CUA scenarios. To fill this gap, we introduce the first conceptual and methodological framework for unintended CUA behaviors, by defining their key characteristics, automatically eliciting them, and analyzing how they arise from benign inputs. We propose AutoElicit: an agentic framework that iteratively perturbs benign instructions using CUA execution feedback, and elicits severe harms while keeping perturbations realistic and benign. Using AutoElicit, we surface hundreds of harmful unintended behaviors from state-of-the-art CUAs such as Claude 4.5 Haiku and Opus. We further evaluate the transferability of human-verified successful perturbations, identifying persistent susceptibility to unintended behaviors across various other frontier CUAs. This work establishes a foundation for systematically analyzing unintended behaviors in realistic computer-use settings.",
    "summary": "",
    "translation": "当良性输入导致严重危害：引发计算机使用代理的不安全意外行为",
    "relevance_score": 1,
    "reasoning": "该论文标题关注AI代理的安全性和意外行为，属于安全、伦理等非技术性话题，与您关注的推荐系统、搜索、广告等核心领域技术进展无关。论文内容可能涉及安全评估或伦理考量，这些都被明确列为不相关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08221v1": {
    "title": "CoRect: Context-Aware Logit Contrast for Hidden State Rectification to Resolve Knowledge Conflicts",
    "url": "https://www.alphaxiv.org/abs/2602.08221v1",
    "arxiv_id": "2602.08221v1",
    "authors": "Xuhua Ma, Richong Zhang, Zhijie Nie",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2026-02-09 02:49:21",
    "ori_summary": "Retrieval-Augmented Generation (RAG) often struggles with knowledge conflicts, where model-internal parametric knowledge overrides retrieved evidence, leading to unfaithful outputs. Existing approaches are often limited, relying either on superficial decoding adjustments or weight editing that necessitates ground-truth targets. Through layer-wise analysis, we attribute this failure to a parametric suppression phenomenon: specifically, in deep layers, certain FFN layers overwrite context-sensitive representations with memorized priors. To address this, we propose CoRect (Context-Aware Logit Contrast for Hidden State Rectification). By contrasting logits from contextualized and non-contextualized forward passes, CoRect identifies layers that exhibit high parametric bias without requiring ground-truth labels. It then rectifies the hidden states to preserve evidence-grounded information. Across question answering (QA) and summarization benchmarks, CoRect consistently improves faithfulness and reduces hallucinations compared to strong baselines.",
    "summary": "该论文研究检索增强生成中模型内部参数化知识覆盖检索证据导致输出不忠实的问题。其核心方法是：通过对比上下文化和非上下文化前向传播的logit，识别出具有高参数偏置的层，并修正隐状态以保留基于证据的信息。",
    "translation": "CoRect：基于上下文感知的Logit对比用于隐藏状态校正以解决知识冲突",
    "relevance_score": 8,
    "reasoning": "该论文涉及LLM中的知识冲突解决和隐藏状态校正，属于核心LLM技术进展。在推荐/搜索系统中，处理用户历史行为与当前上下文之间的冲突至关重要，这种技术可应用于多源信息融合、缓解推荐系统中的冷启动问题，以及提升搜索结果的上下文一致性。",
    "rerank_relevance_score": 7,
    "rerank_reasoning": "该论文针对检索增强生成中的知识冲突问题，提出了一种无需真实标签的隐状态修正方法，直接改进了LLM在检索场景下的输出忠实度，与搜索和推荐系统中对结果准确性和可信度的核心需求高度相关。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08220v1": {
    "title": "Pretraining with Token-Level Adaptive Latent Chain-of-Thought",
    "url": "https://www.alphaxiv.org/abs/2602.08220v1",
    "arxiv_id": "2602.08220v1",
    "authors": "Boyi Zeng, Yiqin Hao, He Li, Shixiang Song, Feichen Song, Zitong Wang, Siyuan Huang, Yi Xu, ZiWei He, Xinbing Wang, Zhouhan Lin",
    "categories": "cs.CL",
    "pub_date": "2026-02-09 02:49:15",
    "ori_summary": "Scaling large language models by increasing parameters and training data is increasingly constrained by limited high-quality corpora and rising communication costs. This work explores an alternative axis: increasing per-token computation without expanding parameters, by internalizing latent Chain-of-Thought (CoT) into pretraining. We propose Pretraining with Token-Level Adaptive Latent CoT (adaptive latent CoT), where the model generates a variable-length latent CoT trajectory before emitting each token -- allocating longer trajectories to difficult tokens and shorter (or even zero) trajectories to easy ones. Importantly, this behavior emerges naturally from one-stage pretraining on general text and reduces computation in both training and inference via token-wise adaptive halting. Experiments with Llama architectures show that adaptive latent CoT consistently improves language modeling perplexity and broad downstream accuracy, even with fewer training FLOPs than prior recurrent baselines.",
    "summary": "该论文研究如何在不增加模型参数的情况下提升大语言模型的推理能力。其核心方法是：在预训练阶段为每个输出token生成可变长度的潜在思维链轨迹，让模型自动为困难token分配更长计算路径，为简单token分配更短路径，实现token级别的自适应计算分配。",
    "translation": "基于令牌级自适应潜在思维链的预训练",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能LLM技术'类别，因为它提出了一种改进LLM推理能力的预训练方法。令牌级自适应潜在思维链技术可以增强LLM的推理和决策能力，这在推荐系统和搜索中具有直接应用价值，例如改善复杂查询理解、多步骤推理推荐和广告相关性判断。",
    "rerank_relevance_score": 8,
    "rerank_reasoning": "该论文通过引入潜在思维链和自适应计算机制，直接提升了Transformer架构的推理效率和能力，属于核心LLM技术进步，对推荐和搜索系统的实时推理具有重要应用价值。",
    "is_filtered": false,
    "is_fine_ranked": true
  },
  "2602.08213v1": {
    "title": "DrugR: Optimizing Molecular Drugs through LLM-based Explicit Reasoning",
    "url": "https://www.alphaxiv.org/abs/2602.08213v1",
    "arxiv_id": "2602.08213v1",
    "authors": "Haoran Liu, Zheni Zeng, Yukun Yan, Yuxuan Chen, Yunduo Xiao",
    "categories": "cs.LG, cs.AI, cs.CL, q-bio.QM",
    "pub_date": "2026-02-09 02:26:25",
    "ori_summary": "Molecule generation and optimization is a fundamental task in chemical domain. The rapid development of intelligent tools, especially large language models (LLMs) with powerful knowledge reserves and interactive capabilities, has provided new paradigms for it. Nevertheless, the intrinsic challenge for LLMs lies in the complex implicit relationship between molecular structure and pharmacological properties and the lack of corresponding labeled data. To bridge this gap, we propose DrugR, an LLM-based method that introduces explicit, step-by-step pharmacological reasoning into the optimization process. Our approach integrates domain-specific continual pretraining, supervised fine-tuning via reverse data engineering, and self-balanced multi-granular reinforcement learning. This framework enables DrugR to effectively improve key ADMET properties while preserving the original molecule's core efficacy. Experimental results demonstrate that DrugR achieves comprehensive enhancement across multiple properties without compromising structural similarity or target binding affinity. Importantly, its explicit reasoning process provides clear, interpretable rationales for each optimization step, yielding actionable design insights and advancing toward automated, knowledge-driven scientific discovery. Our code and model checkpoints are open-sourced to foster future research.",
    "summary": "",
    "translation": "DrugR：基于大语言模型显式推理的分子药物优化",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及药物分子优化，属于化学/生物医学领域的特定应用，这直接属于用户指定的无关主题范畴。虽然提到了LLM技术，但其应用场景与推荐系统、搜索或广告领域完全无关，没有任何潜在的应用关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08208v1": {
    "title": "LLMs and people both learn to form conventions -- just not with each other",
    "url": "https://www.alphaxiv.org/abs/2602.08208v1",
    "arxiv_id": "2602.08208v1",
    "authors": "Cameron R. Jones, Agnese Lombardi, Kyle Mahowald, Benjamin K. Bergen",
    "categories": "cs.CL, cs.HC",
    "pub_date": "2026-02-09 02:15:18",
    "ori_summary": "Humans align to one another in conversation -- adopting shared conventions that ease communication. We test whether LLMs form the same kinds of conventions in a multimodal communication game. Both humans and LLMs display evidence of convention-formation (increasing the accuracy and consistency of their turns while decreasing their length) when communicating in same-type dyads (humans with humans, AI with AI). However, heterogenous human-AI pairs fail -- suggesting differences in communicative tendencies. In Experiment 2, we ask whether LLMs can be induced to behave more like human conversants, by prompting them to produce superficially humanlike behavior. While the length of their messages matches that of human pairs, accuracy and lexical overlap in human-LLM pairs continues to lag behind that of both human-human and AI-AI pairs. These results suggest that conversational alignment requires more than just the ability to mimic previous interactions, but also shared interpretative biases toward the meanings that are conveyed.",
    "summary": "",
    "translation": "大语言模型与人类都学习形成惯例——只是彼此之间无法达成一致",
    "relevance_score": 2,
    "reasoning": "该论文标题主要探讨LLMs与人类在惯例形成方面的学习差异，这属于LLM行为特性研究，与推荐系统、搜索或广告的核心技术应用没有直接关联。虽然涉及LLMs，但未提及任何在RecSys/Search/Ads领域的潜在应用场景或技术启示。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08194v1": {
    "title": "Dreaming in Code for Curriculum Learning in Open-Ended Worlds",
    "url": "https://www.alphaxiv.org/abs/2602.08194v1",
    "arxiv_id": "2602.08194v1",
    "authors": "Konstantinos Mitsides, Maxence Faldor, Antoine Cully",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2026-02-09 01:24:40",
    "ori_summary": "Open-ended learning frames intelligence as emerging from continual interaction with an ever-expanding space of environments. While recent advances have utilized foundation models to programmatically generate diverse environments, these approaches often focus on discovering isolated behaviors rather than orchestrating sustained progression. In complex open-ended worlds, the large combinatorial space of possible challenges makes it difficult for agents to discover sequences of experiences that remain consistently learnable. To address this, we propose Dreaming in Code (DiCode), a framework in which foundation models synthesize executable environment code to scaffold learning toward increasing competence. In DiCode, \"dreaming\" takes the form of materializing code-level variations of the world. We instantiate DiCode in Craftax, a challenging open-ended benchmark characterized by rich mechanics and long-horizon progression. Empirically, DiCode enables agents to acquire long-horizon skills, achieving a $16\\%$ improvement in mean return over the strongest baseline and non-zero success on late-game combat tasks where prior methods fail. Our results suggest that code-level environment design provides a practical mechanism for curriculum control, enabling the construction of intermediate environments that bridge competence gaps in open-ended worlds. Project page and source code are available at https://konstantinosmitsides.github.io/dreaming-in-code and https://github.com/konstantinosmitsides/dreaming-in-code.",
    "summary": "",
    "translation": "开放世界课程学习中基于代码的梦想机制",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及课程学习和开放世界环境，这属于强化学习或机器人学习领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然课程学习在机器学习中有应用，但论文标题没有表明与Transformer架构、LLM技术或推荐/搜索/广告系统的直接联系，因此相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08169v1": {
    "title": "Spherical Steering: Geometry-Aware Activation Rotation for Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.08169v1",
    "arxiv_id": "2602.08169v1",
    "authors": "Zejia You, Chunyuan Deng, Hanjie Chen",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2026-02-09 00:15:47",
    "ori_summary": "Inference-time steering has emerged as a promising paradigm for controlling language models (LMs) without the cost of retraining. However, standard approaches typically rely on activation addition, a geometric operation that inevitably alters the magnitude of hidden representations. This raises concerns about representation collapse and degradation of open-ended generation capabilities. In this work, we explore Spherical Steering, a training-free primitive that resolves this trade-off through activation rotation. Rather than shifting activations with a fixed vector, our method rotates them along a geodesic toward a target direction, guiding the activation toward the target concept while preserving the integrity of the signal. To further enhance adaptivity, we incorporate a confidence gate that dynamically modulates steering strength based on input uncertainty. Extensive experiments across multiple-choice benchmarks demonstrate that Spherical Steering significantly outperforms addition-based baselines (notably by +10% on TruthfulQA, COPA, and Storycloze), while simultaneously maintaining the model's general open-ended generation quality. This work highlights the value of geometric consistency, suggesting that norm-preserving rotation is a robust and effective primitive for precise inference-time control.",
    "summary": "",
    "translation": "球面导向：面向语言模型的几何感知激活旋转",
    "relevance_score": 3,
    "reasoning": "该论文提出了一种针对语言模型激活函数的几何感知旋转方法，属于Transformer架构效率优化范畴，可能通过改进激活机制提升模型性能。然而，标题未明确说明该方法在推荐系统、搜索或广告中的具体应用潜力，因此相关性有限。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.09024v1": {
    "title": "Autoregressive Image Generation with Masked Bit Modeling",
    "url": "https://www.alphaxiv.org/abs/2602.09024v1",
    "arxiv_id": "2602.09024v1",
    "authors": "Qihang Yu, Qihao Liu, Ju He, Xinyang Zhang, Yang Liu, Liang-Chieh Chen, Xi Chen",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 18:59:58",
    "ori_summary": "This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/",
    "summary": "",
    "translation": "基于掩码比特建模的自回归图像生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于图像生成的自回归方法，属于纯粹的计算机视觉领域。虽然标题提到'掩码建模'这一在语言模型中常用的技术，但论文的核心是图像生成，没有明确指向推荐系统、搜索或广告领域的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.09022v1": {
    "title": "WorldCompass: Reinforcement Learning for Long-Horizon World Models",
    "url": "https://www.alphaxiv.org/abs/2602.09022v1",
    "arxiv_id": "2602.09022v1",
    "authors": "Zehan Wang, Tengfei Wang, Haiyu Zhang, Xuhui Zuo, Junta Wu, Haoyuan Wang, Wenqiang Sun, Zhenwei Wang, Chenjie Cao, Hengshuang Zhao, Chunchao Guo, Zhou Zhao",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 18:59:47",
    "ori_summary": "This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively \"steer\" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.",
    "summary": "",
    "translation": "WorldCompass：面向长视野世界模型的强化学习",
    "relevance_score": 2,
    "reasoning": "该论文标题聚焦于强化学习（RL）和世界模型，属于RL领域的研究。虽然世界模型在理论上可能用于模拟用户行为或环境，但标题未明确表明与推荐系统、搜索或广告的关联。根据用户要求，若无明确相关性，RL论文应视为低相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.09021v1": {
    "title": "$χ_{0}$: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies",
    "url": "https://www.alphaxiv.org/abs/2602.09021v1",
    "arxiv_id": "2602.09021v1",
    "authors": "Checheng Yu, Chonghao Sima, Gangcheng Jiang, Hai Zhang, Haoguang Mai, Hongyang Li, Huijie Wang, Jin Chen, Kaiyang Wu, Li Chen, Lirui Zhao, Modi Shi, Ping Luo, Qingwen Bu, Shijia Peng, Tianyu Li, Yibo Yuan",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2026-02-09 18:59:45",
    "ori_summary": "High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose $χ_{0}$, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. $χ_{0}$ enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that $χ_{0}$ surpasses the state-of-the-art $π_{0.5}$ in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.",
    "summary": "",
    "translation": "χ₀：通过驯服分布不一致性实现资源感知的鲁棒操作",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及资源感知操作和分布不一致性，这些主题与推荐系统、搜索或广告的核心领域进展、LLM技术应用或Transformer架构无关。标题暗示了可能涉及强化学习或控制系统，但没有明确与推荐系统/搜索/广告的相关性，因此不符合任何关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.09018v1": {
    "title": "Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving",
    "url": "https://www.alphaxiv.org/abs/2602.09018v1",
    "arxiv_id": "2602.09018v1",
    "authors": "Amir Mallak, Alaa Maalouf",
    "categories": "cs.RO, cs.AI, cs.CV, cs.LG",
    "pub_date": "2026-02-09 18:59:03",
    "ori_summary": "Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \\in \\{0,1,2,3\\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline. (3) The largest single factor drops are rural $\\rightarrow$ urban and day $\\rightarrow$ night ($\\sim 31\\%$ each); actor swaps $\\sim 10\\%$, moderate rain $\\sim 7\\%$; season shifts can be drastic, and combining a time flip with other changes further degrades performance. (4) FM-feature policies stay above $85\\%$ under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below $50\\%$ by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful. (6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness ($+11.8$ points from $5$ to $14$ traces), yet targeted exposure to hard conditions can substitute for scale. (8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD $60.6\\% \\rightarrow 70.1\\%$) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.",
    "summary": "",
    "translation": "鲁棒性是一个函数而非数字：基于视觉的驾驶中OOD鲁棒性的因子化综合研究",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于计算机视觉在自动驾驶领域的应用，特别是OOD（分布外）鲁棒性问题。虽然涉及鲁棒性概念，但完全限定在视觉驾驶这一特定领域，与搜索、推荐、广告等核心关注领域无直接关联。论文未提及任何可能应用于RecSys/Search/Ads的通用技术或架构创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.09016v1": {
    "title": "Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2602.09016v1",
    "arxiv_id": "2602.09016v1",
    "authors": "Hao Phung, Hadar Averbuch-Elor",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 18:58:46",
    "ori_summary": "Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.",
    "summary": "",
    "translation": "Raster2Seq：用于平面图重建的多边形序列生成",
    "relevance_score": 1,
    "reasoning": "这篇论文专注于计算机视觉中的平面图重建和几何生成任务，属于纯粹的视觉/图形学领域。虽然涉及序列生成，但其应用场景（建筑平面图）与推荐系统、搜索或广告领域没有直接关联。该技术没有展示出在异构数据处理、用户行为序列建模或内容理解方面的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.09014v1": {
    "title": "ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation",
    "url": "https://www.alphaxiv.org/abs/2602.09014v1",
    "arxiv_id": "2602.09014v1",
    "authors": "Zihan Yang, Shuyuan Tu, Licheng Zhang, Qi Dai, Yu-Gang Jiang, Zuxuan Wu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-09 18:56:14",
    "ori_summary": "Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.",
    "summary": "",
    "translation": "ArcFlow：通过高精度非线性流蒸馏实现两步式文本到图像生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于文本到图像生成技术，属于纯粹的AIGC/内容生成领域。虽然涉及生成模型技术，但标题中明确表明其应用是文本到图像生成，与推荐系统、搜索或广告的排名/检索核心任务没有直接关联。根据您的关注点排除标准，这属于'纯粹LLM中心化主题'或'AIGC、内容生成'等无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.09013v1": {
    "title": "Dexterous Manipulation Policies from RGB Human Videos via 4D Hand-Object Trajectory Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2602.09013v1",
    "arxiv_id": "2602.09013v1",
    "authors": "Hongyi Chen, Tony Dong, Tiancheng Wu, Liquan Wang, Yash Jangir, Yaru Niu, Yufei Ye, Homanga Bharadhwaj, Zackory Erickson, Jeffrey Ichnowski",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2026-02-09 18:56:02",
    "ori_summary": "Multi-finger robotic hand manipulation and grasping are challenging due to the high-dimensional action space and the difficulty of acquiring large-scale training data. Existing approaches largely rely on human teleoperation with wearable devices or specialized sensing equipment to capture hand-object interactions, which limits scalability. In this work, we propose VIDEOMANIP, a device-free framework that learns dexterous manipulation directly from RGB human videos. Leveraging recent advances in computer vision, VIDEOMANIP reconstructs explicit 4D robot-object trajectories from monocular videos by estimating human hand poses, object meshes, and retargets the reconstructed human motions to robotic hands for manipulation learning. To make the reconstructed robot data suitable for dexterous manipulation training, we introduce hand-object contact optimization with interaction-centric grasp modeling, as well as a demonstration synthesis strategy that generates diverse training trajectories from a single video, enabling generalizable policy learning without additional robot demonstrations. In simulation, the learned grasping model achieves a 70.25% success rate across 20 diverse objects using the Inspire Hand. In the real world, manipulation policies trained from RGB videos achieve an average 62.86% success rate across seven tasks using the LEAP Hand, outperforming retargeting-based methods by 15.87%. Project videos are available at videomanip.github.io.",
    "summary": "",
    "translation": "基于4D手-物体轨迹重建的RGB人类视频灵巧操作策略",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及计算机视觉（RGB视频、4D重建）和机器人操作（灵巧操作策略），属于纯粹的视觉和机器人领域研究。标题中没有任何元素表明与推荐系统、搜索、广告或相关使能技术（如LLM、Transformer架构）有关，完全属于被排除的'纯粹视觉'类别。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.09007v1": {
    "title": "GEBench: Benchmarking Image Generation Models as GUI Environments",
    "url": "https://www.alphaxiv.org/abs/2602.09007v1",
    "arxiv_id": "2602.09007v1",
    "authors": "Haodong Li, Jingwei Wu, Quan Sun, Guopeng Li, Juanxi Tian, Huanyu Zhang, Yanlin Lai, Ruichuan An, Hongbo Peng, Yuhong Dai, Chenxi Li, Chunmei Qing, Jia Wang, Ziyang Meng, Zheng Ge, Xiangyu Zhang, Daxin Jiang",
    "categories": "cs.AI, cs.CV",
    "pub_date": "2026-02-09 18:52:02",
    "ori_summary": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.",
    "summary": "",
    "translation": "GEBench：将图像生成模型作为图形用户界面环境进行基准测试",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于图像生成模型的基准测试，属于纯粹的视觉生成领域。虽然提到了GUI环境，但这与推荐系统、搜索或广告的核心技术（如排序、检索、用户建模）没有直接关联。该工作没有涉及异构数据统一建模、Transformer架构改进或LLM在推荐/搜索/广告中的直接应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08996v1": {
    "title": "Generalizing Sports Feedback Generation by Watching Competitions and Reading Books: A Rock Climbing Case Study",
    "url": "https://www.alphaxiv.org/abs/2602.08996v1",
    "arxiv_id": "2602.08996v1",
    "authors": "Arushi Rai, Adriana Kovashka",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 18:41:43",
    "ori_summary": "While there is rapid progress in video-LLMs with advanced reasoning capabilities, prior work shows that these models struggle on the challenging task of sports feedback generation and require expensive and difficult-to-collect finetuning feedback data for each sport. This limitation is evident from the poor generalization to sports unseen during finetuning. Furthermore, traditional text generation evaluation metrics (e.g., BLEU-4, METEOR, ROUGE-L, BERTScore), originally developed for machine translation and summarization, fail to capture the unique aspects of sports feedback quality. To address the first problem, using rock climbing as our case study, we propose using auxiliary freely-available web data from the target domain, such as competition videos and coaching manuals, in addition to existing sports feedback from a disjoint, source domain to improve sports feedback generation performance on the target domain. To improve evaluation, we propose two evaluation metrics: (1) specificity and (2) actionability. Together, our approach enables more meaningful and practical generation of sports feedback under limited annotations.",
    "summary": "",
    "translation": "通过观看比赛和阅读书籍实现体育反馈生成的泛化：以攀岩为例的研究",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于体育领域的反馈生成，属于特定领域应用（攀岩案例研究），与推荐系统、搜索或广告的核心技术进展无关。虽然涉及生成技术，但属于AIGC/内容生成范畴，且没有明确指向RecSys/Search/Ads的应用潜力，因此属于无关主题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08971v1": {
    "title": "WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models",
    "url": "https://www.alphaxiv.org/abs/2602.08971v1",
    "arxiv_id": "2602.08971v1",
    "authors": "Yu Shang, Zhuohang Li, Yiding Ma, Weikang Su, Xin Jin, Ziyou Wang, Xin Zhang, Yinzhou Tang, Chen Gao, Wei Wu, Xihui Liu, Dhruv Shah, Zhaoxiang Zhang, Zhibo Chen, Jun Zhu, Yonghong Tian, Tat-Seng Chua, Wenwu Zhu, Yong Li",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2026-02-09 18:09:20",
    "ori_summary": "While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.",
    "summary": "",
    "translation": "WorldArena：一个用于评估具身世界模型的感知与功能效用的统一基准",
    "relevance_score": 1,
    "reasoning": "该论文标题聚焦于具身世界模型的评估基准，属于机器人学或具身智能领域，与推荐系统、搜索或广告的核心技术无直接关联。标题中未提及任何与Transformer架构、LLM技术、多模态建模或推荐/搜索/广告应用相关的关键词，因此判定为不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08962v1": {
    "title": "Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting",
    "url": "https://www.alphaxiv.org/abs/2602.08962v1",
    "arxiv_id": "2602.08962v1",
    "authors": "Guangxun Zhu, Xuan Liu, Nicolas Pugeault, Chongfeng Wei, Edmond S. L. Ho",
    "categories": "cs.CV, cs.RO",
    "pub_date": "2026-02-09 17:58:53",
    "ori_summary": "Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D",
    "summary": "",
    "translation": "面向车辆条件姿态预测的3D行人-车辆交互建模",
    "relevance_score": 1,
    "reasoning": "该论文专注于3D行人姿态预测和车辆交互，属于计算机视觉和自动驾驶领域。虽然涉及交互建模，但主要应用于物理环境感知和预测，与推荐系统、搜索或广告的异构数据统一建模、Transformer架构进展或LLM应用没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08961v1": {
    "title": "MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE",
    "url": "https://www.alphaxiv.org/abs/2602.08961v1",
    "arxiv_id": "2602.08961v1",
    "authors": "Ruijie Zhu, Jiahao Lu, Wenbo Hu, Xiaoguang Han, Jianfei Cai, Ying Shan, Chuanxia Zheng",
    "categories": "cs.CV, cs.AI, cs.CG, cs.LG",
    "pub_date": "2026-02-09 17:58:12",
    "ori_summary": "We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page",
    "summary": "",
    "translation": "MotionCrafter：基于四维变分自编码器的密集几何与运动重建",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及3D视觉和运动重建，属于纯粹的计算机视觉领域。虽然提到了4D VAE（变分自编码器），但核心关注点是几何与运动重建，与推荐系统、搜索或广告的异构数据处理、Transformer架构进展或LLM应用没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08958v1": {
    "title": "Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields",
    "url": "https://www.alphaxiv.org/abs/2602.08958v1",
    "arxiv_id": "2602.08958v1",
    "authors": "Weihan Luo, Lily Goli, Sherwin Bahmani, Felix Taubner, Andrea Tagliasacchi, David B. Lindell",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 17:55:01",
    "ori_summary": "Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.",
    "summary": "",
    "translation": "随流生长：基于高斯流场的植物生长四维重建",
    "relevance_score": 1,
    "reasoning": "该论文专注于植物生长的4D重建和计算机视觉技术，属于纯粹的视觉和3D视觉研究领域。虽然标题中提到了“流场”这一技术概念，但论文内容明显与推荐系统、搜索或广告的核心技术进展无关，也不涉及LLM技术、Transformer架构改进或异构数据统一建模等当前关注领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08909v1": {
    "title": "Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit",
    "url": "https://www.alphaxiv.org/abs/2602.08909v1",
    "arxiv_id": "2602.08909v1",
    "authors": "Zhendong Wang, Cihan Ruan, Jingchuan Xiao, Chuqing Shi, Wei Jiang, Wei Wang, Wenjie Liu, Nam Ling",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-02-09 17:09:08",
    "ori_summary": "We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.",
    "summary": "",
    "translation": "融合3D高斯泼溅解决方案分析：密度效应与预测极限",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向3D高斯泼溅技术，这是计算机图形学和3D视觉领域的专用方法。虽然标题提到“预测极限”，但这与推荐系统、搜索或广告中的预测任务无关，而是指3D渲染中的技术限制。该论文没有显示出与推荐系统、搜索、广告、LLM技术或Transformer架构的任何关联，完全属于被排除的“纯粹视觉/3D视觉”类别。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08882v1": {
    "title": "Designing Multi-Robot Ground Video Sensemaking with Public Safety Professionals",
    "url": "https://www.alphaxiv.org/abs/2602.08882v1",
    "arxiv_id": "2602.08882v1",
    "authors": "Puqi Zhou, Ali Asgarov, Aafiya Hussain, Wonjoon Park, Amit Paudyal, Sameep Shrestha, Chia-wei Tang, Michael F. Lighthiser, Michael R. Hieb, Xuesu Xiao, Chris Thomas, Sungsoo Ray Hong",
    "categories": "cs.HC, cs.CV",
    "pub_date": "2026-02-09 16:43:37",
    "ori_summary": "Videos from fleets of ground robots can advance public safety by providing scalable situational awareness and reducing professionals' burden. Yet little is known about how to design and integrate multi-robot videos into public safety workflows. Collaborating with six police agencies, we examined how such videos could be made practical. In Study 1, we presented the first testbed for multi-robot ground video sensemaking. The testbed includes 38 events-of-interest (EoI) relevant to public safety, a dataset of 20 robot patrol videos (10 day/night pairs) covering EoI types, and 6 design requirements aimed at improving current video sensemaking practices. In Study 2, we built MRVS, a tool that augments multi-robot patrol video streams with a prompt-engineered video understanding model. Participants reported reduced manual workload and greater confidence with LLM-based explanations, while noting concerns about false alarms and privacy. We conclude with implications for designing future multi-robot video sensemaking tools. The testbed is available at https://github.com/Puqi7/MRVS\\_VideoSensemaking",
    "summary": "",
    "translation": "与公共安全专业人员共同设计多机器人地面视频感知系统",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及多机器人系统和公共安全应用，属于特定领域应用（公共安全/机器人学），与搜索、推荐、广告等核心领域无关。标题中提到的视频感知可能涉及计算机视觉，但没有明确指向推荐系统、搜索或广告中的异构数据处理或统一建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08861v1": {
    "title": "TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.08861v1",
    "arxiv_id": "2602.08861v1",
    "authors": "Xiangtian Zheng, Zishuo Wang, Yuxin Peng",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 16:24:53",
    "ori_summary": "With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.",
    "summary": "",
    "translation": "TiFRe：面向高效视频多模态大语言模型的文本引导视频帧缩减方法",
    "relevance_score": 3,
    "reasoning": "该论文主要关注视频多模态大语言模型中的效率优化技术（视频帧缩减），属于多模态模型效率改进范畴。虽然效率改进是通用技术，但视频模态与推荐/搜索/广告领域的典型数据模态（文本、用户行为序列、上下文特征）差异较大，且论文未明确展示在推荐/搜索/广告场景的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08858v1": {
    "title": "FlattenGPT: Depth Compression for Transformer with Layer Flattening",
    "url": "https://www.alphaxiv.org/abs/2602.08858v1",
    "arxiv_id": "2602.08858v1",
    "authors": "Ruihan Xu, Qingpei Guo, Yao Zhu, Xiangyang Ji, Ming Yang, Shiliang Zhang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-09 16:22:58",
    "ori_summary": "Recent works have indicated redundancy across transformer blocks, prompting the research of depth compression to prune less crucial blocks. However, current ways of entire-block pruning suffer from risks of discarding meaningful cues learned in those blocks, leading to substantial performance degradation. As another line of model compression, channel pruning can better preserve performance, while it cannot reduce model depth and is challenged by inconsistent pruning ratios for individual layers. To pursue better model compression and acceleration, this paper proposes \\textbf{FlattenGPT}, a novel way to detect and reduce depth-wise redundancies. By flatting two adjacent blocks into one, it compresses the network depth, meanwhile enables more effective parameter redundancy detection and removal. FlattenGPT allows to preserve the knowledge learned in all blocks, and remains consistent with the original transformer architecture. Extensive experiments demonstrate that FlattenGPT enhances model efficiency with a decent trade-off to performance. It outperforms existing pruning methods in both zero-shot accuracies and WikiText-2 perplexity across various model types and parameter sizes. On LLaMA-2/3 and Qwen-1.5 models, FlattenGPT retains 90-96\\% of zero-shot performance with a compression ratio of 20\\%. It also outperforms other pruning methods in accelerating LLM inference, making it promising for enhancing the efficiency of transformers.",
    "summary": "",
    "translation": "FlattenGPT：基于层扁平化的Transformer深度压缩",
    "relevance_score": 8,
    "reasoning": "该论文属于'使能Transformer技术'范畴，专注于通过层扁平化实现Transformer架构的深度压缩，这直接关系到模型效率提升。在推荐系统、搜索和广告领域，更高效的Transformer架构可以降低推理成本、加速响应时间，并可能实现更大模型在资源受限环境中的部署。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2602.08828v1": {
    "title": "VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2602.08828v1",
    "arxiv_id": "2602.08828v1",
    "authors": "Hao Tan, Jun Lan, Senyuan Shi, Zichang Tan, Zijian Yu, Huijia Zhu, Weiqiang Wang, Jun Wan, Zhen Lei",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 16:00:01",
    "ori_summary": "The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.",
    "summary": "",
    "translation": "VideoVeritas：通过感知预任务强化学习进行AI生成视频检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于AI生成视频检测，属于内容真实性验证领域，与推荐系统、搜索或广告的核心技术（如排序、召回、用户建模）无关。虽然涉及AI技术，但未提及在RecSys/Search/Ads中的潜在应用，且检测任务本身属于内容安全范畴，不在当前关注范围内。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08822v1": {
    "title": "Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications",
    "url": "https://www.alphaxiv.org/abs/2602.08822v1",
    "arxiv_id": "2602.08822v1",
    "authors": "Yao Pu, Yiming Shi, Zhenxi Zhang, Peixin Yu, Yitao Zhuang, Xiang Wang, Hongzhao Chen, Jing Cai, Ge Ren",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 15:56:53",
    "ori_summary": "Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.",
    "summary": "",
    "translation": "任意到全部MRI合成：鼻咽癌及其下游应用的统一基础模型",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于医学影像（MRI）和特定疾病（鼻咽癌），这属于明确的无关主题（医学/生物领域特定应用）。虽然提到了“基础模型”和“下游应用”，但核心内容与推荐系统、搜索或广告的技术进步无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08820v1": {
    "title": "Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing",
    "url": "https://www.alphaxiv.org/abs/2602.08820v1",
    "arxiv_id": "2602.08820v1",
    "authors": "Hao Yang, Zhiyu Tan, Jia Gong, Luozheng Qin, Hesen Chen, Xiaomeng Yang, Yuqing Sun, Yuetan Lin, Mengping Yang, Hao Li",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 15:56:05",
    "ori_summary": "We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \\emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.",
    "summary": "",
    "translation": "Omni-Video 2：扩展基于多模态大语言模型条件的扩散模型，实现统一视频生成与编辑",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频生成与编辑技术，属于纯粹的视觉内容生成领域。虽然涉及多模态大语言模型和扩散模型，但其核心应用场景（视频生成/编辑）与推荐系统、搜索或广告的排名、检索、个性化等核心任务没有直接关联，属于明确列出的无关主题范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08797v1": {
    "title": "Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework",
    "url": "https://www.alphaxiv.org/abs/2602.08797v1",
    "arxiv_id": "2602.08797v1",
    "authors": "Jiaming Liu, Cheng Ding, Daoqiang Zhang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-09 15:37:40",
    "ori_summary": "Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.",
    "summary": "",
    "translation": "基于半监督师生框架解决3D MRI扫描中脑肿瘤分割的数据标注稀缺问题",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（脑肿瘤分割）这一特定领域应用，属于明确的无关主题。虽然涉及半监督学习技术，但其核心应用场景（医学3D MRI）与推荐系统、搜索或广告领域没有直接关联，也不涉及LLM、Transformer架构或异构数据统一建模等当前关注的技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08794v1": {
    "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation",
    "url": "https://www.alphaxiv.org/abs/2602.08794v1",
    "arxiv_id": "2602.08794v1",
    "authors": "SII-OpenMOSS Team, :, Donghua Yu, Mingshu Chen, Qi Chen, Qi Luo, Qianyi Wu, Qinyuan Cheng, Ruixiao Li, Tianyi Liang, Wenbo Zhang, Wenming Tu, Xiangyu Peng, Yang Gao, Yanru Huo, Ying Zhu, Yinze Luo, Yiyang Zhang, Yuerong Song, Zhe Xu, Zhiyu Zhang, Chenchen Yang, Cheng Chang, Chushu Zhou, Hanfu Chen, Hongnan Ma, Jiaxi Li, Jingqi Tong, Junxi Liu, Ke Chen, Shimin Li, Songlin Wang, Wei Jiang, Zhaoye Fei, Zhiyuan Ning, Chunguo Li, Chenhui Li, Ziwei He, Zengfeng Huang, Xie Chen, Xipeng Qiu",
    "categories": "cs.CV, cs.SD",
    "pub_date": "2026-02-09 15:31:54",
    "ori_summary": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.",
    "summary": "",
    "translation": "MOVA：迈向可扩展且同步的视频-音频生成",
    "relevance_score": 1,
    "reasoning": "该论文专注于视频-音频生成，属于纯粹的AIGC/内容生成领域，与推荐系统、搜索或广告的排名核心任务无关。虽然标题提到“可扩展性”，但这指的是生成系统的扩展，而非推荐/搜索系统的可扩展性。该技术没有明显的应用场景可以转化为推荐、搜索或广告领域的实际解决方案。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08792v1": {
    "title": "Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems",
    "url": "https://www.alphaxiv.org/abs/2602.08792v1",
    "arxiv_id": "2602.08792v1",
    "authors": "Hao Dong, Eleni Chatzi, Olga Fink",
    "categories": "cs.CV, cs.AI, cs.LG",
    "pub_date": "2026-02-09 15:29:19",
    "ori_summary": "The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.",
    "summary": "",
    "translation": "用于受电弓-接触网系统电弧检测的多模态学习",
    "relevance_score": 1,
    "reasoning": "该论文涉及电力系统/铁路工程中的特定领域应用（受电弓-接触网电弧检测），属于明确的非相关领域（如物理、工程应用）。虽然标题包含“多模态学习”，但这与RecSys/Search/Ads中处理异构数据（如用户序列和上下文特征）的多模态建模无关，而是针对物理传感器数据的工程问题。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08775v1": {
    "title": "VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars",
    "url": "https://www.alphaxiv.org/abs/2602.08775v1",
    "arxiv_id": "2602.08775v1",
    "authors": "Vineet Kumar Rakesh, Ahana Bhattacharjee, Soumya Mazumdar, Tapas Samanta, Hemendra Kumar Pandey, Amitabha Das, Sarbajit Pal",
    "categories": "cs.CV, cs.CG",
    "pub_date": "2026-02-09 15:17:56",
    "ori_summary": "Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg",
    "summary": "",
    "translation": "VedicTHG：用于教育虚拟人低资源说话头部生成的符号吠陀计算",
    "relevance_score": 1,
    "reasoning": "该论文专注于特定领域的说话头部生成技术，属于计算机视觉和图形学范畴，与推荐系统、搜索或广告的核心技术无直接关联。虽然涉及教育应用，但未展示在异构数据处理、Transformer架构改进或LLM应用方面的潜力，完全属于被排除的“纯视觉/图形学”类别。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08764v1": {
    "title": "Efficient Brain Extraction of MRI Scans with Mild to Moderate Neuropathology",
    "url": "https://www.alphaxiv.org/abs/2602.08764v1",
    "arxiv_id": "2602.08764v1",
    "authors": "Hjalti Thrastarson, Lotta M. Ellingsen",
    "categories": "eess.IV, cs.AI, cs.CV",
    "pub_date": "2026-02-09 15:03:30",
    "ori_summary": "Skull stripping magnetic resonance images (MRI) of the human brain is an important process in many image processing techniques, such as automatic segmentation of brain structures. Numerous methods have been developed to perform this task, however, they often fail in the presence of neuropathology and can be inconsistent in defining the boundary of the brain mask. Here, we propose a novel approach to skull strip T1-weighted images in a robust and efficient manner, aiming to consistently segment the outer surface of the brain, including the sulcal cerebrospinal fluid (CSF), while excluding the full extent of the subarachnoid space and meninges. We train a modified version of the U-net on silver-standard ground truth data using a novel loss function based on the signed-distance transform (SDT). We validate our model both qualitatively and quantitatively using held-out data from the training dataset, as well as an independent external dataset. The brain masks used for evaluation partially or fully include the subarachnoid space, which may introduce bias into the comparison; nonetheless, our model demonstrates strong performance on the held-out test data, achieving a consistent mean Dice similarity coefficient (DSC) of 0.964$\\pm$0.006 and an average symmetric surface distance (ASSD) of 1.4mm$\\pm$0.2mm. Performance on the external dataset is comparable, with a DSC of 0.958$\\pm$0.006 and an ASSD of 1.7$\\pm$0.2mm. Our method achieves performance comparable to or better than existing state-of-the-art methods for brain extraction, particularly in its highly consistent preservation of the brain's outer surface. The method is publicly available on GitHub.",
    "summary": "",
    "translation": "针对轻度至中度神经病理学MRI扫描的高效大脑提取方法",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及医学影像处理（MRI扫描）和神经病理学领域，属于明确的医学/生物学应用范畴。根据用户指定的不相关主题列表，这属于'Medical, Biology, Chemistry, Physics or other domain-specific applications'类别，与推荐系统、搜索或广告的核心技术进展完全无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08753v1": {
    "title": "MVAnimate: Enhancing Character Animation with Multi-View Optimization",
    "url": "https://www.alphaxiv.org/abs/2602.08753v1",
    "arxiv_id": "2602.08753v1",
    "authors": "Tianyu Sun, Zhoujie Fu, Bang Zhang, Guosheng Lin",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 14:55:21",
    "ori_summary": "The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.",
    "summary": "",
    "translation": "MVAnimate：通过多视角优化增强角色动画",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机图形学中的角色动画技术，属于纯粹的视觉/图形领域。虽然涉及多视角优化，但内容与推荐系统、搜索或广告的核心技术（如排序、召回、用户建模、内容理解）没有直接关联。论文没有展示在推荐/搜索/广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08749v1": {
    "title": "Shifting the Breaking Point of Flow Matching for Multi-Instance Editing",
    "url": "https://www.alphaxiv.org/abs/2602.08749v1",
    "arxiv_id": "2602.08749v1",
    "authors": "Carmine Zaccagnino, Fabio Quattrini, Enis Simsar, Marta Tintoré Gazulla, Rita Cucchiara, Alessio Tonioni, Silvia Cascianelli",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 14:52:45",
    "ori_summary": "Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.",
    "summary": "",
    "translation": "移动流匹配的断点以实现多实例编辑",
    "relevance_score": 1,
    "reasoning": "该标题涉及流匹配和多实例编辑，这属于生成模型和图像编辑领域，与推荐系统、搜索或广告的核心进展、LLM技术应用或Transformer架构改进无关。没有明确的潜在应用指向RecSys/Search/Ads领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08735v1": {
    "title": "From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.08735v1",
    "arxiv_id": "2602.08735v1",
    "authors": "Masanari Oi, Koki Maeda, Ryuto Koike, Daisuke Oba, Nakamasa Inoue, Naoaki Okazaki",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 14:39:43",
    "ori_summary": "While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.",
    "summary": "",
    "translation": "从对应关系到行动：多模态大语言模型中类人多图像空间推理",
    "relevance_score": 3,
    "reasoning": "该论文主要关注多模态大语言模型中的空间推理能力，属于计算机视觉与语言模型的交叉领域。虽然涉及多模态建模，但论文标题明确聚焦于空间推理和图像理解，这与推荐系统、搜索或广告中处理异构数据（如用户序列和上下文特征）的统一建模需求关联较弱。标题未提及任何与推荐、搜索或广告相关的应用场景，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08730v1": {
    "title": "Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation",
    "url": "https://www.alphaxiv.org/abs/2602.08730v1",
    "arxiv_id": "2602.08730v1",
    "authors": "Shanshan Wang, Ziying Feng, Xiaozheng Shen, Xun Yang, Pichao Wang, Zhenwei He, Xingyi Zhang",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 14:37:05",
    "ori_summary": "Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA",
    "summary": "",
    "translation": "闭合混淆循环：基于CLIP引导的无源域自适应对齐",
    "relevance_score": 2,
    "reasoning": "该论文涉及域自适应和视觉-语言模型（CLIP），属于计算机视觉领域。虽然提到了CLIP（视觉-语言模型），但论文核心是视觉域自适应问题，没有明确涉及推荐系统、搜索或广告中的异构数据处理或应用。标题中未显示与推荐/搜索/广告系统的直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08727v1": {
    "title": "Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework",
    "url": "https://www.alphaxiv.org/abs/2602.08727v1",
    "arxiv_id": "2602.08727v1",
    "authors": "Johannes Thalhammer, Tina Dorosti, Sebastian Peterhansl, Daniela Pfeiffer, Franz Pfeiffer, Florian Schaff",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-09 14:36:05",
    "ori_summary": "Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.",
    "summary": "",
    "translation": "使用混合2D-3D CNN框架减少欠采样3D锥束CT中的伪影",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（CT扫描）的伪影减少技术，属于医学/生物医学工程领域。虽然涉及深度学习（CNN），但其应用场景（3D锥束CT）与推荐系统、搜索或广告领域没有直接关联，也不符合任何指定的技术焦点类别。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08726v1": {
    "title": "SynSacc: A Blender-to-V2E Pipeline for Synthetic Neuromorphic Eye-Movement Data and Sim-to-Real Spiking Model Training",
    "url": "https://www.alphaxiv.org/abs/2602.08726v1",
    "arxiv_id": "2602.08726v1",
    "authors": "Khadija Iddrisu, Waseem Shariff, Suzanne Little, Noel OConnor",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 14:34:31",
    "ori_summary": "The study of eye movements, particularly saccades and fixations, are fundamental to understanding the mechanisms of human cognition and perception. Accurate classification of these movements requires sensing technologies capable of capturing rapid dynamics without distortion. Event cameras, also known as Dynamic Vision Sensors (DVS), provide asynchronous recordings of changes in light intensity, thereby eliminating motion blur inherent in conventional frame-based cameras and offering superior temporal resolution and data efficiency. In this study, we introduce a synthetic dataset generated with Blender to simulate saccades and fixations under controlled conditions. Leveraging Spiking Neural Networks (SNNs), we evaluate its robustness by training two architectures and finetuning on real event data. The proposed models achieve up to 0.83 accuracy and maintain consistent performance across varying temporal resolutions, demonstrating stability in eye movement classification. Moreover, the use of SNNs with synthetic event streams yields substantial computational efficiency gains over artificial neural network (ANN) counterparts, underscoring the utility of synthetic data augmentation in advancing event-based vision. All code and datasets associated with this work is available at https: //github.com/Ikhadija-5/SynSacc-Dataset.",
    "summary": "",
    "translation": "SynSacc：用于合成神经形态眼动数据及模拟到真实脉冲模型训练的Blender到V2E流水线",
    "relevance_score": 1,
    "reasoning": "该论文专注于神经形态计算和眼动数据的合成生成，属于计算机视觉和神经科学的交叉领域。虽然提到了模拟到真实的模型训练，但其核心内容（神经形态传感器、脉冲神经网络、眼动数据生成）与推荐系统、搜索或广告的技术栈和应用场景没有直接关联，也不涉及Transformer架构或LLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08725v1": {
    "title": "FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing",
    "url": "https://www.alphaxiv.org/abs/2602.08725v1",
    "arxiv_id": "2602.08725v1",
    "authors": "Yongwen Lai, Chaoqun Wang, Shaobo Min",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 14:34:18",
    "ori_summary": "Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \\href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.",
    "summary": "",
    "translation": "FusionEdit：用于免训练图像编辑的语义融合与注意力调制",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于图像编辑任务，属于计算机视觉领域，与推荐系统、搜索或广告的核心技术无关。标题中提到的语义融合和注意力调制技术虽然可能涉及注意力机制，但专门应用于图像编辑场景，没有表明对推荐/搜索/广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08724v1": {
    "title": "Rotated Lights for Consistent and Efficient 2D Gaussians Inverse Rendering",
    "url": "https://www.alphaxiv.org/abs/2602.08724v1",
    "arxiv_id": "2602.08724v1",
    "authors": "Geng Lin, Matthias Zwicker",
    "categories": "cs.CV, cs.GR",
    "pub_date": "2026-02-09 14:34:06",
    "ori_summary": "Inverse rendering aims to decompose a scene into its geometry, material properties and light conditions under a certain rendering model. It has wide applications like view synthesis, relighting, and scene editing. In recent years, inverse rendering methods have been inspired by view synthesis approaches like neural radiance fields and Gaussian splatting, which are capable of efficiently decomposing a scene into its geometry and radiance. They then further estimate the material and lighting that lead to the observed scene radiance. However, the latter step is highly ambiguous and prior works suffer from inaccurate color and baked shadows in their albedo estimation albeit their regularization. To this end, we propose RotLight, a simple capturing setup, to address the ambiguity. Compared to a usual capture, RotLight only requires the object to be rotated several times during the process. We show that as few as two rotations is effective in reducing artifacts. To further improve 2DGS-based inverse rendering, we additionally introduce a proxy mesh that not only allows accurate incident light tracing, but also enables a residual constraint and improves global illumination handling. We demonstrate with both synthetic and real world datasets that our method achieves superior albedo estimation while keeping efficient computation.",
    "summary": "",
    "translation": "用于一致高效二维高斯逆渲染的旋转光",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及计算机图形学中的逆渲染和二维高斯技术，属于纯粹的视觉/图形学领域。虽然提到了效率和一致性，但没有表明与推荐系统、搜索或广告有任何关联。标题中没有任何元素暗示可以应用于异构数据处理、Transformer架构或LLM技术。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08717v1": {
    "title": "Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images",
    "url": "https://www.alphaxiv.org/abs/2602.08717v1",
    "arxiv_id": "2602.08717v1",
    "authors": "Farnaz Khun Jush, Grit Werner, Mark Klemens, Matthias Lenga",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-09 14:26:24",
    "ori_summary": "Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.",
    "summary": "",
    "translation": "用于体积CT和MR图像的自动身体区域检测的零样本系统",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学影像（CT/MR）中的身体区域检测，属于医学/生物学领域的特定应用，与推荐系统、搜索或广告的核心技术无关。虽然提到了零样本学习，但这是应用于医学图像分析而非推荐/搜索/广告领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08711v1": {
    "title": "TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions",
    "url": "https://www.alphaxiv.org/abs/2602.08711v1",
    "arxiv_id": "2602.08711v1",
    "authors": "Linli Yao, Yuancheng Wei, Yaojie Zhang, Lei Li, Xinlong Chen, Feifan Song, Ziyue Wang, Kun Ouyang, Yuanxin Liu, Lingpeng Kong, Qi Liu, Pengfei Wan, Kun Gai, Yuanxing Zhang, Xu Sun",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 14:21:58",
    "ori_summary": "This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create \"script-like\" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.",
    "summary": "",
    "translation": "TimeChat-Captioner：基于时间感知与结构化音视频字幕的多场景视频脚本生成",
    "relevance_score": 2,
    "reasoning": "该论文主要关注多模态视频理解与脚本生成，属于计算机视觉与自然语言处理的交叉领域。虽然涉及多模态建模，但其核心是视频内容分析与生成，与推荐系统、搜索或广告中的异构数据处理（如用户序列与上下文特征）的直接相关性较弱。论文未明确展示在RecSys/Search/Ads中的潜在应用，因此评分较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08699v1": {
    "title": "Low-Light Video Enhancement with An Effective Spatial-Temporal Decomposition Paradigm",
    "url": "https://www.alphaxiv.org/abs/2602.08699v1",
    "arxiv_id": "2602.08699v1",
    "authors": "Xiaogang Xu, Kun Zhou, Tao Hu, Jiafei Wu, Ruixing Wang, Hao Peng, Bei Yu",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 14:15:47",
    "ori_summary": "Low-Light Video Enhancement (LLVE) seeks to restore dynamic or static scenes plagued by severe invisibility and noise. In this paper, we present an innovative video decomposition strategy that incorporates view-independent and view-dependent components to enhance the performance of LLVE. The framework is called View-aware Low-light Video Enhancement (VLLVE). We leverage dynamic cross-frame correspondences for the view-independent term (which primarily captures intrinsic appearance) and impose a scene-level continuity constraint on the view-dependent term (which mainly describes the shading condition) to achieve consistent and satisfactory decomposition results. To further ensure consistent decomposition, we introduce a dual-structure enhancement network featuring a cross-frame interaction mechanism. By supervising different frames simultaneously, this network encourages them to exhibit matching decomposition features. This mechanism can seamlessly integrate with encoder-decoder single-frame networks, incurring minimal additional parameter costs. Building upon VLLVE, we propose a more comprehensive decomposition strategy by introducing an additive residual term, resulting in VLLVE++. This residual term can simulate scene-adaptive degradations, which are difficult to model using a decomposition formulation for common scenes, thereby further enhancing the ability to capture the overall content of videos. In addition, VLLVE++ enables bidirectional learning for both enhancement and degradation-aware correspondence refinement (end-to-end manner), effectively increasing reliable correspondences while filtering out incorrect ones. Notably, VLLVE++ demonstrates strong capability in handling challenging cases, such as real-world scenes and videos with high dynamics. Extensive experiments are conducted on widely recognized LLVE benchmarks.",
    "summary": "",
    "translation": "基于高效时空分解范式的低光视频增强",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的低光视频增强技术，属于纯粹的视觉处理领域。虽然标题中提到“时空分解”可能涉及序列建模，但论文核心是视觉质量提升，与推荐系统、搜索或广告中的用户行为建模、内容理解或排序优化没有直接关联，也不涉及LLM、Transformer架构或异构数据处理。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08683v1": {
    "title": "OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence",
    "url": "https://www.alphaxiv.org/abs/2602.08683v1",
    "arxiv_id": "2602.08683v1",
    "authors": "Feilong Tang, Xiang An, Yunyao Yan, Yin Xie, Bin Qin, Kaicheng Yang, Yifei Shen, Yuanhan Zhang, Chunyuan Li, Shikun Feng, Changrui Chen, Huajie Tan, Ming Hu, Manyuan Zhang, Bo Li, Ziyong Feng, Ziwei Liu, Zongyuan Ge, Jiankang Deng",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 14:06:17",
    "ori_summary": "Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs. Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics. Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.",
    "summary": "",
    "translation": "OneVision-Encoder：编解码器对齐的稀疏性作为多模态智能的基础原则",
    "relevance_score": 7,
    "reasoning": "该论文涉及多模态智能的基础原则，与'VLM类比用于异构数据'相关，可将不同数据模态（如上下文特征和用户序列）视为不同模态进行统一建模。稀疏性技术（如MoE）是Transformer架构效率提升的关键方向，在推荐/搜索系统中具有应用潜力，用于处理大规模特征和序列数据。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2602.08682v1": {
    "title": "ALIVE: Animate Your World with Lifelike Audio-Video Generation",
    "url": "https://www.alphaxiv.org/abs/2602.08682v1",
    "arxiv_id": "2602.08682v1",
    "authors": "Ying Guo, Qijun Gan, Yifu Zhang, Jinlai Liu, Yifei Hu, Pan Xie, Dongjun Qian, Yu Zhang, Ruiqi Li, Yuqi Zhang, Ruibiao Lu, Xiaofeng Mei, Bo Han, Xiang Yin, Bingyue Peng, Zehuan Yuan",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 14:06:03",
    "ori_summary": "Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.",
    "summary": "",
    "translation": "ALIVE：通过逼真的音视频生成技术为您的世界注入生命",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向音视频生成技术，属于AIGC/内容生成领域，这被明确列为无关主题。虽然标题提到“为您的世界注入生命”，但这更像是创意内容生成，而非与推荐系统、搜索或广告相关的排名、匹配或用户理解技术。没有迹象表明该技术具有在推荐/搜索/广告领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08670v1": {
    "title": "A Machine Learning accelerated geophysical fluid solver",
    "url": "https://www.alphaxiv.org/abs/2602.08670v1",
    "arxiv_id": "2602.08670v1",
    "authors": "Yang Bai",
    "categories": "cs.CV, cs.CE, cs.PF, physics.comp-ph",
    "pub_date": "2026-02-09 13:55:26",
    "ori_summary": "Machine learning methods have been successful in many areas, like image classification and natural language processing. However, it still needs to be determined how to apply ML to areas with mathematical constraints, like solving PDEs. Among various approaches to applying ML techniques to solving PDEs, the data-driven discretization method presents a promising way of accelerating and improving existing PDE solver on structured grids where it predicts the coefficients of quasi-linear stencils for computing values or derivatives of a function at given positions. It can improve the accuracy and stability of low-resolution simulation compared with using traditional finite difference or finite volume schemes. Meanwhile, it can also benefit from traditional numerical schemes like achieving conservation law by adapting finite volume type formulations. In this thesis, we have implemented the shallow water equation and Euler equation classic solver under a different framework. Experiments show that our classic solver performs much better than the Pyclaw solver. Then we propose four different deep neural networks for the ML-based solver. The results indicate that two of these approaches could output satisfactory solutions.",
    "summary": "",
    "translation": "一种机器学习加速的地球物理流体求解器",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于地球物理流体求解这一特定科学计算领域，属于物理学或地球科学应用范畴。虽然涉及机器学习加速技术，但缺乏与推荐系统、搜索或广告领域的任何直接或潜在联系，完全属于您列出的无关主题中的“Physics or other domain-specific applications”。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08661v1": {
    "title": "WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling",
    "url": "https://www.alphaxiv.org/abs/2602.08661v1",
    "arxiv_id": "2602.08661v1",
    "authors": "Yi Dao, Lankai Zhang, Hao Liu, Haiwei Zhang, Wenbo Wang",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 13:52:43",
    "ori_summary": "Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.",
    "summary": "",
    "translation": "WiFlow：一种基于WiFi的轻量级连续人体姿态估计网络，具有时空特征解耦",
    "relevance_score": 1,
    "reasoning": "该论文专注于WiFi信号处理、人体姿态估计和计算机视觉技术，属于纯粹的感知技术研究。虽然涉及时空特征处理，但其应用场景（人体姿态估计）和核心技术（WiFi信号分析）与推荐系统、搜索或广告的核心技术栈（用户行为建模、内容理解、排序算法）没有直接关联，也没有明显的Transformer架构或LLM应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08652v1": {
    "title": "Deep Learning-Based Fixation Type Prediction for Quality Assurance in Digital Pathology",
    "url": "https://www.alphaxiv.org/abs/2602.08652v1",
    "arxiv_id": "2602.08652v1",
    "authors": "Oskar Thaeter, Tanja Niedermair, Johannes Raffler, Ralf Huss, Peter J. Schüffler",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 13:46:55",
    "ori_summary": "Accurate annotation of fixation type is a critical step in slide preparation for pathology laboratories. However, this manual process is prone to errors, impacting downstream analyses and diagnostic accuracy. Existing methods for verifying formalin-fixed, paraffin-embedded (FFPE), and frozen section (FS) fixation types typically require full-resolution whole-slide images (WSIs), limiting scalability for high-throughput quality control. We propose a deep-learning model to predict fixation types using low-resolution, pre-scan thumbnail images. The model was trained on WSIs from the TUM Institute of Pathology (n=1,200, Leica GT450DX) and evaluated on a class-balanced subset of The Cancer Genome Atlas dataset (TCGA, n=8,800, Leica AT2), as well as on class-balanced datasets from Augsburg (n=695 [392 FFPE, 303 FS], Philips UFS) and Regensburg (n=202, 3DHISTECH P1000). Our model achieves an AUROC of 0.88 on TCGA, outperforming comparable pre-scan methods by 4.8%. It also achieves AUROCs of 0.72 on Regensburg and Augsburg slides, underscoring challenges related to scanner-induced domain shifts. Furthermore, the model processes each slide in 21 ms, $400\\times$ faster than existing high-magnification, full-resolution methods, enabling rapid, high-throughput processing. This approach provides an efficient solution for detecting labelling errors without relying on high-magnification scans, offering a valuable tool for quality control in high-throughput pathology workflows. Future work will improve and evaluate the model's generalisation to additional scanner types. Our findings suggest that this method can increase accuracy and efficiency in digital pathology workflows and may be extended to other low-resolution slide annotations.",
    "summary": "",
    "translation": "基于深度学习的数字病理学质量保证中注视类型预测",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向数字病理学这一医学领域应用，属于明确的医学/生物学特定领域研究。虽然涉及深度学习技术，但没有任何迹象表明该研究涉及推荐系统、搜索、广告或相关技术，也不涉及LLM、Transformer架构或异构数据建模。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08626v1": {
    "title": "Revisiting [CLS] and Patch Token Interaction in Vision Transformers",
    "url": "https://www.alphaxiv.org/abs/2602.08626v1",
    "arxiv_id": "2602.08626v1",
    "authors": "Alexis Marouani, Oriane Siméoni, Hervé Jégou, Piotr Bojanowski, Huy V. Vo",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 13:16:01",
    "ori_summary": "Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.",
    "summary": "",
    "translation": "重新审视视觉Transformer中的[CLS]与图像块标记交互机制",
    "relevance_score": 3,
    "reasoning": "该论文聚焦于视觉Transformer架构中特定组件的交互机制分析，属于Transformer架构效率与机制优化的技术范畴。虽然Transformer架构优化对推荐/搜索系统有潜在价值，但论文标题明确限定于视觉领域（Vision Transformers），且未提及跨模态或多模态应用，因此与您关注的异质数据统一建模（VLM类比）的直接关联性较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08620v1": {
    "title": "Improving Reconstruction of Representation Autoencoder",
    "url": "https://www.alphaxiv.org/abs/2602.08620v1",
    "arxiv_id": "2602.08620v1",
    "authors": "Siyu Liu, Chujie Qin, Hubery Yin, Qixin Yan, Zheng-Peng Duan, Chen Li, Jing Lyu, Chun-Le Guo, Chongyi Li",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 13:12:35",
    "ori_summary": "Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.",
    "summary": "",
    "translation": "改进表示自编码器的重建能力",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及自编码器表示重建的改进，属于通用的表示学习方法。虽然表示学习是推荐系统和搜索的基础技术之一，但该标题没有明确指向推荐系统、搜索或广告的具体应用，也没有涉及LLM、Transformer架构或VLM类比等当前关注的核心领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08615v1": {
    "title": "Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration",
    "url": "https://www.alphaxiv.org/abs/2602.08615v1",
    "arxiv_id": "2602.08615v1",
    "authors": "Kfir Goldberg, Elad Richardson, Yael Vinker",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 13:00:16",
    "ori_summary": "While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.",
    "summary": "",
    "translation": "灵感种子：学习非字面视觉组合以进行生成式探索",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于视觉生成和组合探索，属于AIGC/内容生成领域。虽然提到了“生成式探索”，但没有表明与推荐系统、搜索或广告的直接关联。根据用户列出的无关主题，这属于“AIGC, Content generation, Summarization, or other purely LLM-centric topics”，因此相关性极低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08613v1": {
    "title": "Overview and Comparison of AVS Point Cloud Compression Standard",
    "url": "https://www.alphaxiv.org/abs/2602.08613v1",
    "arxiv_id": "2602.08613v1",
    "authors": "Wei Gao, Wenxu Gao, Xingming Mu, Changhao Peng, Ge Li",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 12:58:41",
    "ori_summary": "Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.",
    "summary": "",
    "translation": "AVS点云压缩标准综述与比较",
    "relevance_score": 1,
    "reasoning": "该论文专注于点云压缩标准，属于3D视觉技术领域，与推荐系统、搜索或广告的核心技术没有直接关联。点云压缩主要应用于计算机视觉、自动驾驶和3D建模等场景，在您的关注领域中缺乏明确的应用潜力。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08582v1": {
    "title": "SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2602.08582v1",
    "arxiv_id": "2602.08582v1",
    "authors": "Melany Yang, Yuhang Yu, Diwang Weng, Jinwei Chen, Wei Dong",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 12:20:33",
    "ori_summary": "Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.",
    "summary": "",
    "translation": "SemiNFT：通过混合样本强化学习实现从模仿到欣赏的预设迁移学习",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及强化学习和预设迁移，但未明确提及推荐系统、搜索或广告的核心领域。标题中的“预设”可能指某些配置或参数，但缺乏与异构数据处理、Transformer架构或LLM应用的直接联系。尽管强化学习在推荐系统中可能有应用，但标题未提供足够信息表明其与当前关注领域的明确相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08580v1": {
    "title": "retinalysis-vascx: An explainable software toolbox for the extraction of retinal vascular biomarkers",
    "url": "https://www.alphaxiv.org/abs/2602.08580v1",
    "arxiv_id": "2602.08580v1",
    "authors": "Jose D. Vargas Quiros, Michael J. Beyeler, Sofia Ortin Vela, EyeNED Reading Center, Sven Bergmann, Caroline C. W. Klaver, Bart Liefers",
    "categories": "q-bio.TO, cs.CV",
    "pub_date": "2026-02-09 12:19:33",
    "ori_summary": "The automatic extraction of retinal vascular biomarkers from color fundus images (CFI) is essential for large-scale studies of the retinal vasculature. We present VascX, an open-source Python toolbox designed for the automated extraction of biomarkers from artery and vein segmentations. The VascX workflow processes vessel segmentation masks into skeletons to build undirected and directed vessel graphs, which are then used to resolve segments into continuous vessels. This architecture enables the calculation of a comprehensive suite of biomarkers, including vascular density, bifurcation angles, central retinal equivalents (CREs), tortuosity, and temporal angles, alongside image quality metrics. A distinguishing feature of VascX is its region awareness; by utilizing the fovea, optic disc, and CFI boundaries as anatomical landmarks, the tool ensures spatially standardized measurements and identifies when specific biomarkers are not computable. Spatially localized biomarkers are calculated over grids relative to these landmarks, facilitating precise clinical analysis. Released via GitHub and PyPI, VascX provides an explainable and modifiable framework that supports reproducible vascular research through integrated visualizations. By enabling the rapid extraction of established biomarkers and the development of new ones, VascX advances the field of oculomics, offering a robust, computationally efficient solution for scalable deployment in large-scale clinical and epidemiological databases.",
    "summary": "",
    "translation": "retinalysis-vascx：一个用于提取视网膜血管生物标志物的可解释性软件工具箱",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学影像分析领域（视网膜血管分析），属于明确的医学/生物学应用范畴。虽然涉及特征提取和可解释性技术，但这是针对特定医疗领域的工具开发，与推荐系统、搜索、广告等商业应用领域无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08558v1": {
    "title": "FLAG-4D: Flow-Guided Local-Global Dual-Deformation Model for 4D Reconstruction",
    "url": "https://www.alphaxiv.org/abs/2602.08558v1",
    "arxiv_id": "2602.08558v1",
    "authors": "Guan Yuan Tan, Ngoc Tuan Vu, Arghya Pal, Sailaja Rajanala, Raphael Phan C. -W., Mettu Srinivas, Chee-Ming Ting",
    "categories": "cs.CV, cs.GT",
    "pub_date": "2026-02-09 11:55:15",
    "ori_summary": "We introduce FLAG-4D, a novel framework for generating novel views of dynamic scenes by reconstructing how 3D Gaussian primitives evolve through space and time. Existing methods typically rely on a single Multilayer Perceptron (MLP) to model temporal deformations, and they often struggle to capture complex point motions and fine-grained dynamic details consistently over time, especially from sparse input views. Our approach, FLAG-4D, overcomes this by employing a dual-deformation network that dynamically warps a canonical set of 3D Gaussians over time into new positions and anisotropic shapes. This dual-deformation network consists of an Instantaneous Deformation Network (IDN) for modeling fine-grained, local deformations and a Global Motion Network (GMN) for capturing long-range dynamics, refined through mutual learning. To ensure these deformations are both accurate and temporally smooth, FLAG-4D incorporates dense motion features from a pretrained optical flow backbone. We fuse these motion cues from adjacent timeframes and use a deformation-guided attention mechanism to align this flow information with the current state of each evolving 3D Gaussian. Extensive experiments demonstrate that FLAG-4D achieves higher-fidelity and more temporally coherent reconstructions with finer detail preservation than state-of-the-art methods.",
    "summary": "",
    "translation": "FLAG-4D：用于4D重建的流引导局部-全局双变形模型",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于4D重建（3D+时间）和计算机视觉中的变形模型，属于纯粹的视觉/3D视觉领域。标题中没有任何元素表明与推荐系统、搜索或广告相关，也没有提到Transformer架构、LLM技术或异构数据处理。根据给定的无关主题列表，这属于'Purely Vision、3D Vision, Graphic或Speech papers without clear relevance to RecSys/Search/Ads'的范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08550v1": {
    "title": "GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing",
    "url": "https://www.alphaxiv.org/abs/2602.08550v1",
    "arxiv_id": "2602.08550v1",
    "authors": "Shih-Fang Chen, Jun-Cheng Chen, I-Hong Jhuo, Yen-Yu Lin",
    "categories": "cs.CV, cs.AI, cs.LG, cs.MM, eess.IV",
    "pub_date": "2026-02-09 11:50:29",
    "ori_summary": "Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.",
    "summary": "",
    "translation": "GOT-Edit：基于在线模型编辑的几何感知通用目标跟踪",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉领域的通用目标跟踪，属于纯粹的视觉任务，没有涉及推荐系统、搜索或广告的明确关联。虽然提到了在线模型编辑技术，但论文主要关注几何感知和视觉跟踪，与用户行为建模、内容排序或个性化推荐等核心关注领域无关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08540v1": {
    "title": "TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation",
    "url": "https://www.alphaxiv.org/abs/2602.08540v1",
    "arxiv_id": "2602.08540v1",
    "authors": "He Wu, Xia Yan, Yanghui Xu, Liegang Xia, Jiazhou Chen",
    "categories": "cs.CV, cs.GR",
    "pub_date": "2026-02-09 11:41:06",
    "ori_summary": "Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.",
    "summary": "",
    "translation": "TIBR4D：基于轨迹引导的迭代边界精细化高效4D高斯分割",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向4D高斯分割和边界精细化，属于计算机视觉中的3D/4D视觉处理领域。虽然涉及分割技术，但论文专注于4D高斯表示和边界优化，没有表明与推荐系统、搜索或广告的相关性。根据用户指定的无关主题列表，这属于'Purely Vision、3D Vision, Graphic或Speech papers without clear relevance to RecSys/Search/Ads'的范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08531v1": {
    "title": "Thegra: Graph-based SLAM for Thermal Imagery",
    "url": "https://www.alphaxiv.org/abs/2602.08531v1",
    "arxiv_id": "2602.08531v1",
    "authors": "Anastasiia Kornilova, Ivan Moskalenko, Arabella Gromova, Gonzalo Ferrer, Alexander Menshchikov",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 11:29:42",
    "ori_summary": "Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.",
    "summary": "",
    "translation": "Thegra：基于图的热成像同步定位与地图构建",
    "relevance_score": 1,
    "reasoning": "该论文专注于热成像的SLAM（同步定位与地图构建），这是计算机视觉和机器人领域的技术，与推荐系统、搜索或广告的核心领域进展、LLM技术、Transformer架构或异构数据统一建模没有直接关联。热成像属于特定传感器模态，其应用场景（如机器人导航、环境感知）与RecSys/Search/Ads的排名、个性化或内容理解需求不匹配。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08528v1": {
    "title": "Automatic regularization parameter choice for tomography using a double model approach",
    "url": "https://www.alphaxiv.org/abs/2602.08528v1",
    "arxiv_id": "2602.08528v1",
    "authors": "Chuyang Wu, Samuli Siltanen",
    "categories": "cs.CV, math.OC",
    "pub_date": "2026-02-09 11:23:57",
    "ori_summary": "Image reconstruction in X-ray tomography is an ill-posed inverse problem, particularly with limited available data. Regularization is thus essential, but its effectiveness hinges on the choice of a regularization parameter that balances data fidelity against a priori information. We present a novel method for automatic parameter selection based on the use of two distinct computational discretizations of the same problem. A feedback control algorithm dynamically adjusts the regularization strength, driving an iterative reconstruction toward the smallest parameter that yields sufficient similarity between reconstructions on the two grids. The effectiveness of the proposed approach is demonstrated using real tomographic data.",
    "summary": "",
    "translation": "基于双重模型方法的断层扫描自动正则化参数选择",
    "relevance_score": 1,
    "reasoning": "该论文标题涉及医学成像领域的断层扫描技术，属于明确的医学/生物学应用范畴，与用户关注的推荐系统、搜索、广告等核心领域无关。正则化参数选择虽然是通用技术概念，但论文明确限定在断层扫描这一特定医学应用场景中，没有任何迹象表明该技术会被应用于推荐系统、搜索或广告领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08524v1": {
    "title": "GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving",
    "url": "https://www.alphaxiv.org/abs/2602.08524v1",
    "arxiv_id": "2602.08524v1",
    "authors": "Linger Deng, Yuliang Liu, Wenwen Yu, Zujia Zhang, Jianzhong Ju, Zhenbo Luo, Xiang Bai",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 11:15:01",
    "ori_summary": "Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus",
    "summary": "",
    "translation": "GeoFocus：融合高效全局到局部感知的多模态几何问题求解",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及多模态几何问题求解，主要关注几何推理和空间感知，属于计算机视觉或数学推理领域。虽然提到了“多模态”和“全局到局部感知”，但缺乏与推荐系统、搜索或广告的直接关联，也不属于核心LLM技术、Transformer架构进展或VLM类比应用范畴。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08505v1": {
    "title": "Are Vision Foundation Models Foundational for Electron Microscopy Image Segmentation?",
    "url": "https://www.alphaxiv.org/abs/2602.08505v1",
    "arxiv_id": "2602.08505v1",
    "authors": "Caterina Fuster-Barceló, Virginie Uhlmann",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 10:55:18",
    "ori_summary": "Although vision foundation models (VFMs) are increasingly reused for biomedical image analysis, it remains unclear whether the latent representations they provide are general enough to support effective transfer and reuse across heterogeneous microscopy image datasets. Here, we study this question for the problem of mitochondria segmentation in electron microscopy (EM) images, using two popular public EM datasets (Lucchi++ and VNC) and three recent representative VFMs (DINOv2, DINOv3, and OpenCLIP). We evaluate two practical model adaptation regimes: a frozen-backbone setting in which only a lightweight segmentation head is trained on top of the VFM, and parameter-efficient fine-tuning (PEFT) via Low-Rank Adaptation (LoRA) in which the VFM is fine-tuned in a targeted manner to a specific dataset. Across all backbones, we observe that training on a single EM dataset yields good segmentation performance (quantified as foreground Intersection-over-Union), and that LoRA consistently improves in-domain performance. In contrast, training on multiple EM datasets leads to severe performance degradation for all models considered, with only marginal gains from PEFT. Exploration of the latent representation space through various techniques (PCA, Fréchet Dinov2 distance, and linear probes) reveals a pronounced and persistent domain mismatch between the two considered EM datasets in spite of their visual similarity, which is consistent with the observed failure of paired training. These results suggest that, while VFMs can deliver competitive results for EM segmentation within a single domain under lightweight adaptation, current PEFT strategies are insufficient to obtain a single robust model across heterogeneous EM datasets without additional domain-alignment mechanisms.",
    "summary": "",
    "translation": "视觉基础模型是电子显微镜图像分割的基础吗？",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向电子显微镜图像分割这一特定生物医学应用领域，属于明确的无关主题（Medical/Biology domain-specific applications）。虽然标题提到“Vision Foundation Models”，但应用场景与推荐系统、搜索或广告领域无任何关联，也没有暗示任何可迁移的技术或架构创新。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08491v1": {
    "title": "Enhanced Food Category Recognition under Illumination-Induced Domain Shift",
    "url": "https://www.alphaxiv.org/abs/2602.08491v1",
    "arxiv_id": "2602.08491v1",
    "authors": "Keonvin Park, Aditya Pal, Jin Hong Mok",
    "categories": "cs.CV, cs.LG",
    "pub_date": "2026-02-09 10:43:51",
    "ori_summary": "Visual food recognition systems deployed in real-world environments, such as automated conveyor-belt inspection, are highly sensitive to domain shifts caused by illumination changes. While recent studies have shown that lighting variations can significantly distort food perception by both humans and AI, existing works are often limited to single food categories or controlled settings, and most public food datasets lack explicit illumination annotations. In this work, we investigate illumination-induced domain shift in multi-class food category recognition using two widely adopted datasets, Food-101 and Fruits-360. We demonstrate substantial accuracy degradation under cross-dataset evaluation due to mismatched visual conditions. To address this challenge, we construct synthetic illumination-augmented datasets by systematically varying light temperature and intensity, enabling controlled robustness analysis without additional labels. We further evaluate cross-dataset transfer learning and domain generalization, with a focus on illumination-sensitive target categories such as apple-based classes. Experimental results show that illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance. Our findings highlight the importance of illumination robustness and provide practical insights for deploying reliable food recognition systems in real-world inspection scenarios.",
    "summary": "",
    "translation": "光照诱导域偏移下的增强食品类别识别",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于计算机视觉中的食品识别任务，属于纯粹的视觉领域研究。虽然食品推荐是推荐系统的一个应用场景，但该论文的核心是解决光照变化下的视觉识别问题，没有涉及推荐算法、用户建模、排序或广告等核心领域。标题中未提及任何与推荐系统、搜索、广告、LLM或Transformer架构相关的技术要素，因此与用户关注的所有焦点领域均不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08479v1": {
    "title": "Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation",
    "url": "https://www.alphaxiv.org/abs/2602.08479v1",
    "arxiv_id": "2602.08479v1",
    "authors": "Alif Rizqullah Mahdi, Mahdi Rezaei, Natasha Merat",
    "categories": "cs.CV, cs.AI, cs.ET, cs.HC, cs.LG",
    "pub_date": "2026-02-09 10:28:21",
    "ori_summary": "Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gesture classification framework using 2D pose estimation applied to real-world video sequences from the WIVW dataset. We categorise gestures into four primary classes (Stop, Go, Thank & Greet, and No Gesture) and extract 76 static and dynamic features from normalised keypoints. Our analysis demonstrates that hand position and movement velocity are especially discriminative in distinguishing between gesture classes, achieving a classification accuracy score of 87%. These findings not only improve the perceptual capabilities of AV systems but also contribute to the broader understanding of pedestrian behaviour in traffic contexts.",
    "summary": "",
    "translation": "手势至关重要：通过骨架姿态评估实现自动驾驶车辆的行人手势识别",
    "relevance_score": 1,
    "reasoning": "该论文专注于计算机视觉中的手势识别和姿态估计，属于纯粹的视觉技术研究。虽然涉及自动驾驶场景，但与推荐系统、搜索或广告的核心领域进展、LLM技术、Transformer架构改进、LLM直接应用或异构数据统一建模均无直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08466v1": {
    "title": "Reliability-aware Execution Gating for Near-field and Off-axis Vision-guided Robotic Alignment",
    "url": "https://www.alphaxiv.org/abs/2602.08466v1",
    "arxiv_id": "2602.08466v1",
    "authors": "Ning Hu, Senhao Cao, Maochen Li",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2026-02-09 10:14:39",
    "ori_summary": "Vision-guided robotic systems are increasingly deployed in precision alignment tasks that require reliable execution under near-field and off-axis configurations. While recent advances in pose estimation have significantly improved numerical accuracy, practical robotic systems still suffer from frequent execution failures even when pose estimates appear accurate. This gap suggests that pose accuracy alone is insufficient to guarantee execution-level reliability. In this paper, we reveal that such failures arise from a deterministic geometric error amplification mechanism, in which small pose estimation errors are magnified through system structure and motion execution, leading to unstable or failed alignment. Rather than modifying pose estimation algorithms, we propose a Reliability-aware Execution Gating mechanism that operates at the execution level. The proposed approach evaluates geometric consistency and configuration risk before execution, and selectively rejects or scales high-risk pose updates. We validate the proposed method on a real UR5 robotic platform performing single-step visual alignment tasks under varying camera-target distances and off-axis configurations. Experimental results demonstrate that the proposed execution gating significantly improves task success rates, reduces execution variance, and suppresses tail-risk behavior, while leaving average pose accuracy largely unchanged. Importantly, the proposed mechanism is estimator-agnostic and can be readily integrated with both classical geometry-based and learning-based pose estimation pipelines. These results highlight the importance of execution-level reliability modeling and provide a practical solution for improving robustness in near-field vision-guided robotic systems.",
    "summary": "",
    "translation": "面向近场和离轴视觉引导机器人对准的可靠性感知执行门控",
    "relevance_score": 1,
    "reasoning": "该论文标题明确涉及机器人视觉引导对准技术，属于机器人学和计算机视觉领域。虽然提到了“可靠性感知”和“执行门控”等概念，但核心应用场景（机器人对准）与推荐系统、搜索或广告的排名、检索、建模等核心任务没有直接关联。标题中未提及任何与Transformer架构、LLM技术、多模态建模或推荐/搜索/广告应用相关的关键词。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08462v1": {
    "title": "TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation",
    "url": "https://www.alphaxiv.org/abs/2602.08462v1",
    "arxiv_id": "2602.08462v1",
    "authors": "Yiyang Cao, Yunze Deng, Ziyu Lin, Bin Feng, Xinggang Wang, Wenyu Liu, Dandan Zheng, Jingdong Chen",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 10:12:13",
    "ori_summary": "Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.",
    "summary": "",
    "translation": "TriC-Motion：基于三领域因果建模的文本到运动生成",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于文本到运动生成，属于计算机视觉和图形学领域，与推荐系统、搜索或广告的核心技术无直接关联。虽然涉及多模态建模，但其应用场景（运动生成）与异构数据（如上下文特征和用户序列）的统一建模在推荐/搜索/广告中的应用完全不同。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08448v1": {
    "title": "Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries",
    "url": "https://www.alphaxiv.org/abs/2602.08448v1",
    "arxiv_id": "2602.08448v1",
    "authors": "Haocheng Lu, Nan Zhang, Wei Tao, Xiaoyang Qu, Guokuan Li, Jiguang Wan, Jianzong Wang",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-09 10:00:22",
    "ori_summary": "Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.",
    "summary": "",
    "translation": "Vista：面向事后查询的流式视频问答场景感知优化",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视频问答的流式处理和场景感知优化，属于计算机视觉与自然语言处理的交叉领域。虽然涉及多模态处理，但其核心是视频内容理解而非推荐/搜索/广告系统，且未明确提及在推荐、搜索或广告领域的潜在应用。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08439v1": {
    "title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition",
    "url": "https://www.alphaxiv.org/abs/2602.08439v1",
    "arxiv_id": "2602.08439v1",
    "authors": "Yuhao Dong, Shulin Tian, Shuai Liu, Shuangrui Ding, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Ziwei Liu",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 09:51:29",
    "ori_summary": "Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.",
    "summary": "",
    "translation": "Demo-ICL：面向过程性视频知识获取的上下文学习",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及视频模态的上下文学习，属于多模态学习范畴，但未明确与推荐系统、搜索或广告的关联。虽然VLMs处理异构数据的思路可能类比，但标题聚焦于过程性视频知识获取，更偏向计算机视觉应用而非推荐/搜索领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08430v1": {
    "title": "Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features",
    "url": "https://www.alphaxiv.org/abs/2602.08430v1",
    "arxiv_id": "2602.08430v1",
    "authors": "Qiang Wang",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 09:39:04",
    "ori_summary": "We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model. We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model. When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.",
    "summary": "",
    "translation": "理解与优化基于注意力的稀疏匹配用于多样化局部特征",
    "relevance_score": 8,
    "reasoning": "该论文涉及注意力机制优化（属于Enabling Transformer Tech范畴），稀疏匹配技术可直接应用于搜索中的文档-查询匹配或推荐系统中的用户-物品匹配。处理多样化局部特征的能力与推荐/搜索中处理异构特征（如用户历史序列、上下文特征）的需求高度相关，类似于VLM处理多模态数据的思路。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": false,
    "is_fine_ranked": false
  },
  "2602.08397v1": {
    "title": "RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications",
    "url": "https://www.alphaxiv.org/abs/2602.08397v1",
    "arxiv_id": "2602.08397v1",
    "authors": "Chiara Lena, Davide Milesi, Alessandro Casella, Luca Carlini, Joseph C. Norton, James Martin, Bruno Scaglioni, Keith L. Obstein, Roberto De Sire, Marco Spadaccini, Cesare Hassan, Pietro Valdastri, Elena De Momi",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 08:57:37",
    "ori_summary": "Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment. Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation. Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.",
    "summary": "",
    "translation": "RealSynCol：用于三维重建应用的高保真合成结肠数据集",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学影像领域（结肠数据集）和3D视觉应用，属于明确的无关主题。标题中没有任何元素与推荐系统、搜索、广告、LLM技术或Transformer架构相关，完全超出了您关注的所有技术领域。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08395v1": {
    "title": "D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy",
    "url": "https://www.alphaxiv.org/abs/2602.08395v1",
    "arxiv_id": "2602.08395v1",
    "authors": "Jianfeng Liang, Shaocheng Shen, Botao Xu, Qiang Hu, Xiaoyun Zhang",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 08:52:51",
    "ori_summary": "The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \\textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \\textbf{12$\\times$}",
    "summary": "",
    "translation": "D$^2$-VR：基于协同优化策略的退化鲁棒与蒸馏视频修复",
    "relevance_score": 2,
    "reasoning": "该论文专注于视频修复这一计算机视觉任务，属于纯粹的视觉处理领域。虽然标题中提到了“蒸馏”技术，但这是针对视频修复模型的优化，没有明确展示与推荐系统、搜索或广告领域的潜在应用联系。根据排除标准，纯粹的视觉论文若无明确相关性则不予考虑。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08392v1": {
    "title": "BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2602.08392v1",
    "arxiv_id": "2602.08392v1",
    "authors": "Xin Wu, Zhixuan Liang, Yue Ma, Mengkang Hu, Zhiyuan Qin, Xiu Li",
    "categories": "cs.RO, cs.AI, cs.CV",
    "pub_date": "2026-02-09 08:47:14",
    "ori_summary": "Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.",
    "summary": "",
    "translation": "BiManiBench：用于评估多模态大语言模型双手协调能力的层次化基准",
    "relevance_score": 2,
    "reasoning": "该论文标题主要关注多模态大语言模型在双手协调任务上的评估基准，这属于纯粹的评估基准研究。虽然涉及多模态LLM，但其核心是机器人控制或物理交互的评估，与推荐系统、搜索或广告领域没有直接关联，也不符合您关注的LLM在RecSys/Search/Ads中的潜在应用要求。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08388v1": {
    "title": "Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers",
    "url": "https://www.alphaxiv.org/abs/2602.08388v1",
    "arxiv_id": "2602.08388v1",
    "authors": "Shuo Zhang, Wenzhuo Wu, Huayu Zhang, Jiarong Cheng, Xianghao Zang, Chao Ban, Hao Sun, Zhongjiang He, Tianwei Cao, Kongming Liang, Zhanyu Ma",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 08:39:47",
    "ori_summary": "Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.",
    "summary": "",
    "translation": "基于扩散Transformer的效应敏感上下文修复的几何图像编辑",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于计算机视觉领域的图像编辑技术，涉及扩散Transformer和上下文修复方法。虽然提到了Transformer架构，但其应用完全局限于图像处理，没有展示任何与推荐系统、搜索或广告相关的潜在应用。根据筛选标准，这属于纯粹的视觉论文，没有明确的RecSys/Search/Ads相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08355v1": {
    "title": "E-VAds: An E-commerce Short Videos Understanding Benchmark for MLLMs",
    "url": "https://www.alphaxiv.org/abs/2602.08355v1",
    "arxiv_id": "2602.08355v1",
    "authors": "Xianjie Liu, Yiman Hu, Liang Wu, Ping Hu, Yixiong Zou, Jian Xu, Bo Zheng",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 07:43:38",
    "ori_summary": "E-commerce short videos represent a high-revenue segment of the online video industry characterized by a goal-driven format and dense multi-modal signals. Current models often struggle with these videos because existing benchmarks focus primarily on general-purpose tasks and neglect the reasoning of commercial intent. In this work, we first propose a \\textbf{multi-modal information density assessment framework} to quantify the complexity of this domain. Our evaluation reveals that e-commerce content exhibits substantially higher density across visual, audio, and textual modalities compared to mainstream datasets, establishing a more challenging frontier for video understanding. To address this gap, we introduce \\textbf{E-commerce Video Ads Benchmark (E-VAds)}, which is the first benchmark specifically designed for e-commerce short video understanding. We curated 3,961 high-quality videos from Taobao covering a wide range of product categories and used a multi-agent system to generate 19,785 open-ended Q&A pairs. These questions are organized into two primary dimensions, namely Perception and Cognition and Reasoning, which consist of five distinct tasks. Finally, we develop \\textbf{E-VAds-R1}, an RL-based reasoning model featuring a multi-grained reward design called \\textbf{MG-GRPO}. This strategy provides smooth guidance for early exploration while creating a non-linear incentive for expert-level precision. Experimental results demonstrate that E-VAds-R1 achieves a 109.2% performance gain in commercial intent reasoning with only a few hundred training samples.",
    "summary": "",
    "translation": "E-VAds：面向多模态大语言模型的电商短视频理解基准",
    "relevance_score": 2,
    "reasoning": "该论文标题表明这是一个面向多模态大语言模型（MLLMs）的基准测试，专注于电商短视频理解。虽然涉及多模态和电商领域，但核心是基准测试而非技术进展，且未明确指向推荐、搜索或广告系统的核心算法、架构或应用。基准测试本身属于评估范畴，与当前关注的“核心领域进展”、“赋能技术”或“直接应用”焦点关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08346v1": {
    "title": "What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning",
    "url": "https://www.alphaxiv.org/abs/2602.08346v1",
    "arxiv_id": "2602.08346v1",
    "authors": "Yujin Zhou, Pengcheng Wen, Jiale Chen, Boqin Yin, Han Zhu, Jiaming Ji, Juntao Dai, Chi-Min Chan, Sirui Han",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 07:31:14",
    "ori_summary": "The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.",
    "summary": "",
    "translation": "什么、是否以及如何？揭示用于图像推理思维的过程奖励模型",
    "relevance_score": 3,
    "reasoning": "该论文标题涉及过程奖励模型和图像推理，主要属于视觉推理和强化学习领域。虽然提到了奖励模型，但核心焦点是图像推理，这与RecSys/Search/Ads的直接相关性较弱。标题未明确表明其在推荐、搜索或广告中的潜在应用，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08342v1": {
    "title": "UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science",
    "url": "https://www.alphaxiv.org/abs/2602.08342v1",
    "arxiv_id": "2602.08342v1",
    "authors": "Jie Zhang, Xingtong Yu, Yuan Fang, Rudi Stouffs, Zdravko Trivic",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-09 07:28:49",
    "ori_summary": "Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.",
    "summary": "",
    "translation": "UrbanGraphEmbeddings：面向城市科学的空间基础多模态嵌入学习与评估",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及多模态嵌入学习，这可能与VLM类比异质数据的思路有微弱关联，但核心聚焦于城市科学这一特定领域应用，而非推荐系统、搜索或广告的直接技术。标题中的'Urban Science'明确指向地理/城市规划领域，属于您指定的无关主题中的'其他领域特定应用'。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08339v1": {
    "title": "CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT",
    "url": "https://www.alphaxiv.org/abs/2602.08339v1",
    "arxiv_id": "2602.08339v1",
    "authors": "Chengyi Du, Yazhe Niu, Dazhong Shen, Luxin Xu",
    "categories": "cs.AI, cs.CV",
    "pub_date": "2026-02-09 07:26:40",
    "ori_summary": "Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.",
    "summary": "",
    "translation": "CoTZero：通过分层合成思维链实现免标注类人视觉推理",
    "relevance_score": 2,
    "reasoning": "该论文主要关注视觉推理和思维链技术，属于视觉-语言交叉领域，但标题未表明与推荐系统、搜索或广告的直接关联。虽然分层合成方法可能启发异构数据处理，但缺乏明确的RecSys/Search/Ads应用场景说明，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08337v1": {
    "title": "Language-Guided Transformer Tokenizer for Human Motion Generation",
    "url": "https://www.alphaxiv.org/abs/2602.08337v1",
    "arxiv_id": "2602.08337v1",
    "authors": "Sheng Yan, Yong Wang, Xin Du, Junsong Yuan, Mengyuan Liu",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 07:22:14",
    "ori_summary": "In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.",
    "summary": "",
    "translation": "用于人体运动生成的语言引导Transformer分词器",
    "relevance_score": 2,
    "reasoning": "该论文主要关注人体运动生成，这属于计算机视觉和图形学领域，与推荐系统、搜索或广告的核心技术没有直接关联。虽然提到了语言引导和Transformer，但其应用场景（人体运动）与当前关注的文本/序列数据处理、推荐排序、广告点击率预测等商业应用场景差异较大。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08309v1": {
    "title": "CAE-AV: Improving Audio-Visual Learning via Cross-modal Interactive Enrichment",
    "url": "https://www.alphaxiv.org/abs/2602.08309v1",
    "arxiv_id": "2602.08309v1",
    "authors": "Yunzuo Hu, Wen Li, Jing Zhang",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 06:30:25",
    "ori_summary": "Audio-visual learning suffers from modality misalignment caused by off-screen sources and background clutter, and current methods usually amplify irrelevant regions or moments, leading to unstable training and degraded representation quality. To address this challenge, we proposed a novel Caption-aligned and Agreement-guided Enhancement framework (CAE-AV) for audio-visual learning, which used two complementary modules: Cross-modal Agreement-guided Spatio-Temporal Enrichment (CASTE) and Caption-Aligned Saliency-guided Enrichment (CASE) to relieve audio-visual misalignment. CASTE dynamically balances spatial and temporal relations by evaluating frame-level audio-visual agreement, ensuring that key information is captured from both preceding and subsequent frames under misalignment. CASE injects cross-modal semantic guidance into selected spatio-temporal positions, leveraging high-level semantic cues to further alleviate misalignment. In addition, we design lightweight objectives, caption-to-modality InfoNCE, visual-audio consistency, and entropy regularization to guide token selection and strengthen cross-modal semantic alignment. With frozen backbones, CAE-AV achieves state-of-the-art performance on AVE, AVVP, AVS, and AVQA benchmarks, and qualitative analyses further validate its robustness against audio-visual misalignment.",
    "summary": "",
    "translation": "CAE-AV：通过跨模态交互增强改进音视频学习",
    "relevance_score": 1,
    "reasoning": "该论文专注于音视频跨模态学习，属于纯粹的视觉和音频领域研究，与推荐系统、搜索或广告没有明确关联。虽然涉及多模态交互，但未提出适用于异构数据（如上下文特征和用户序列）的统一建模方法，也没有展示在RecSys/Search/Ads领域的潜在应用价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08282v1": {
    "title": "Tighnari v2: Mitigating Label Noise and Distribution Shift in Multimodal Plant Distribution Prediction via Mixture of Experts and Weakly Supervised Learning",
    "url": "https://www.alphaxiv.org/abs/2602.08282v1",
    "arxiv_id": "2602.08282v1",
    "authors": "Haixu Liu, Yufei Wang, Tianxiang Xu, Chuancheng Shi, Hongsheng Xing",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-09 05:23:22",
    "ori_summary": "Large-scale, cross-species plant distribution prediction plays a crucial role in biodiversity conservation, yet modeling efforts in this area still face significant challenges due to the sparsity and bias of observational data. Presence-Absence (PA) data provide accurate and noise-free labels, but are costly to obtain and limited in quantity; Presence-Only (PO) data, by contrast, offer broad spatial coverage and rich spatiotemporal distribution, but suffer from severe label noise in negative samples. To address these real-world constraints, this paper proposes a multimodal fusion framework that fully leverages the strengths of both PA and PO data. We introduce an innovative pseudo-label aggregation strategy for PO data based on the geographic coverage of satellite imagery, enabling geographic alignment between the label space and remote sensing feature space. In terms of model architecture, we adopt Swin Transformer Base as the backbone for satellite imagery, utilize the TabM network for tabular feature extraction, retain the Temporal Swin Transformer for time-series modeling, and employ a stackable serial tri-modal cross-attention mechanism to optimize the fusion of heterogeneous modalities. Furthermore, empirical analysis reveals significant geographic distribution shifts between PA training and test samples, and models trained by directly mixing PO and PA data tend to experience performance degradation due to label noise in PO data. To address this, we draw on the mixture-of-experts paradigm: test samples are partitioned according to their spatial proximity to PA samples, and different models trained on distinct datasets are used for inference and post-processing within each partition. Experiments on the GeoLifeCLEF 2025 dataset demonstrate that our approach achieves superior predictive performance in scenarios with limited PA coverage and pronounced distribution shifts.",
    "summary": "",
    "translation": "Tighnari v2：通过专家混合与弱监督学习缓解多模态植物分布预测中的标签噪声与分布偏移",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向植物分布预测这一生物学领域应用，属于明确的无关主题。虽然提到了多模态、专家混合（MoE）和弱监督学习等技术，但这些技术被应用于与推荐系统、搜索或广告完全无关的特定领域问题，因此不具备相关性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08277v1": {
    "title": "PISCO: Precise Video Instance Insertion with Sparse Control",
    "url": "https://www.alphaxiv.org/abs/2602.08277v1",
    "arxiv_id": "2602.08277v1",
    "authors": "Xiangbo Gao, Renjie Li, Xinghao Chen, Yuheng Wu, Suofei Feng, Qing Yin, Zhengzhong Tu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-09 05:15:39",
    "ori_summary": "The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and \"cherry-picking\" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.",
    "summary": "",
    "translation": "PISCO：基于稀疏控制的精确视频实例插入",
    "relevance_score": 2,
    "reasoning": "该论文标题聚焦于视频实例插入技术，属于计算机视觉领域的具体应用。虽然视频内容在推荐和广告系统中有所涉及，但该技术本身主要针对视频编辑和生成，与当前关注的推荐系统核心算法、LLM技术、Transformer架构改进或异构数据统一建模等焦点领域关联较弱。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08266v1": {
    "title": "Informative Object-centric Next Best View for Object-aware 3D Gaussian Splatting in Cluttered Scenes",
    "url": "https://www.alphaxiv.org/abs/2602.08266v1",
    "arxiv_id": "2602.08266v1",
    "authors": "Seunghoon Jeong, Eunho Lee, Jeongyun Kim, Ayoung Kim",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2026-02-09 04:50:36",
    "ori_summary": "In cluttered scenes with inevitable occlusions and incomplete observations, selecting informative viewpoints is essential for building a reliable representation. In this context, 3D Gaussian Splatting (3DGS) offers a distinct advantage, as it can explicitly guide the selection of subsequent viewpoints and then refine the representation with new observations. However, existing approaches rely solely on geometric cues, neglect manipulation-relevant semantics, and tend to prioritize exploitation over exploration. To tackle these limitations, we introduce an instance-aware Next Best View (NBV) policy that prioritizes underexplored regions by leveraging object features. Specifically, our object-aware 3DGS distills instancelevel information into one-hot object vectors, which are used to compute confidence-weighted information gain that guides the identification of regions associated with erroneous and uncertain Gaussians. Furthermore, our method can be easily adapted to an object-centric NBV, which focuses view selection on a target object, thereby improving reconstruction robustness to object placement. Experiments demonstrate that our NBV policy reduces depth error by up to 77.14% on the synthetic dataset and 34.10% on the real-world GraspNet dataset compared to baselines. Moreover, compared to targeting the entire scene, performing NBV on a specific object yields an additional reduction of 25.60% in depth error for that object. We further validate the effectiveness of our approach through real-world robotic manipulation tasks.",
    "summary": "",
    "translation": "面向杂乱场景中对象感知3D高斯泼溅的信息化对象中心最佳下一视角选择",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及3D场景重建和视图选择，属于计算机视觉领域，与推荐系统、搜索或广告的核心关注点无直接关联。虽然3D高斯泼溅是视觉技术，但标题未表明其在异构数据处理、序列建模或推荐相关应用方面的潜力，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08262v1": {
    "title": "Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification",
    "url": "https://www.alphaxiv.org/abs/2602.08262v1",
    "arxiv_id": "2602.08262v1",
    "authors": "Guoqi Yu, Xiaowei Hu, Angelica I. Aviles-Rivero, Anqi Qiu, Shujun Wang",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 04:42:42",
    "ori_summary": "Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships. In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends. Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting. Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at https://github.com/Levi-Ackman/DeCI.",
    "summary": "",
    "translation": "超越功能连接：基于fMRI的脑部疾病分类的时间序列建模",
    "relevance_score": 1,
    "reasoning": "该论文专注于医学神经影像学中的脑部疾病分类，属于明确的医学/生物学领域特定应用，与搜索、推荐、广告等商业系统完全无关。即使涉及时间序列建模技术，其应用场景和问题定义与您关注的领域没有交集，因此完全不相关。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08249v1": {
    "title": "A Unified Framework for Multimodal Image Reconstruction and Synthesis using Denoising Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2602.08249v1",
    "arxiv_id": "2602.08249v1",
    "authors": "Weijie Gan, Xucheng Wang, Tongyao Wang, Wenshang Wang, Chunwei Ying, Yuyang Hu, Yasheng Chen, Hongyu An, Ulugbek S. Kamilov",
    "categories": "eess.IV, cs.CV",
    "pub_date": "2026-02-09 03:54:24",
    "ori_summary": "Image reconstruction and image synthesis are important for handling incomplete multimodal imaging data, but existing methods require various task-specific models, complicating training and deployment workflows. We introduce Any2all, a unified framework that addresses this limitation by formulating these disparate tasks as a single virtual inpainting problem. We train a single, unconditional diffusion model on the complete multimodal data stack. This model is then adapted at inference time to ``inpaint'' all target modalities from any combination of inputs of available clean images or noisy measurements. We validated Any2all on a PET/MR/CT brain dataset. Our results show that Any2all can achieve excellent performance on both multimodal reconstruction and synthesis tasks, consistently yielding images with competitive distortion-based performance and superior perceptual quality over specialized methods.",
    "summary": "",
    "translation": "基于去噪扩散模型的多模态图像重建与合成统一框架",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉领域的图像重建与合成，属于纯粹的视觉技术研究。虽然扩散模型是当前重要的生成模型技术，但论文标题明确限定于图像模态，没有涉及文本、序列或其他与推荐系统、搜索、广告相关的异构数据处理。扩散模型在推荐系统等领域的潜在应用（如序列生成、特征增强）需要明确的跨模态设计，而本标题未体现这种关联性。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08241v1": {
    "title": "Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs",
    "url": "https://www.alphaxiv.org/abs/2602.08241v1",
    "arxiv_id": "2602.08241v1",
    "authors": "Siqu Ou, Tianrui Wan, Zhiyuan Zhao, Junyu Gao, Xuelong Li",
    "categories": "cs.AI, cs.CV",
    "pub_date": "2026-02-09 03:33:23",
    "ori_summary": "While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.",
    "summary": "",
    "translation": "多模态大语言模型真的看到了吗：在多模态大语言模型中强化视觉注意力",
    "relevance_score": 3,
    "reasoning": "该论文主要关注多模态大语言模型（MLLMs）中的视觉注意力机制，这属于视觉-语言交叉领域。虽然与VLM类比异质数据的思路有一定关联，但论文更侧重于纯粹的视觉理解能力评估与改进，而非直接应用于推荐系统、搜索或广告中的异质数据统一建模。其潜在应用可能有限，主要停留在提升多模态理解本身，而非具体业务场景的优化。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08230v1": {
    "title": "Generating Adversarial Events: A Motion-Aware Point Cloud Framework",
    "url": "https://www.alphaxiv.org/abs/2602.08230v1",
    "arxiv_id": "2602.08230v1",
    "authors": "Hongwei Ren, Youxin Jiang, Qifei Gu, Xiangqian Wu",
    "categories": "cs.CV, cs.AI",
    "pub_date": "2026-02-09 03:06:07",
    "ori_summary": "Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-based systems. Nevertheless, research into adversarial attacks on events is scarce. This is primarily due to the non-differentiable nature of mainstream event representations, which hinders the extension of gradient-based attack methods. In this paper, we propose MA-ADV, a novel \\textbf{M}otion-\\textbf{A}ware \\textbf{Adv}ersarial framework. To the best of our knowledge, this is the first work to generate adversarial events by leveraging point cloud representations. MA-ADV accounts for high-frequency noise in events and employs a diffusion-based approach to smooth perturbations, while fully leveraging the spatial and temporal relationships among events. Finally, MA-ADV identifies the minimal-cost perturbation through a combination of sample-wise Adam optimization, iterative refinement, and binary search. Extensive experimental results validate that MA-ADV ensures a 100\\% attack success rate with minimal perturbation cost, and also demonstrate enhanced robustness against defenses, underscoring the critical security challenges facing future event-based perception systems.",
    "summary": "",
    "translation": "生成对抗性事件：一种运动感知的点云框架",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及点云和运动感知，主要属于计算机视觉和3D视觉领域，与推荐系统、搜索或广告的核心关注点没有直接关联。虽然运动建模在理论上可能对某些动态推荐场景有启发，但标题未表明任何明确的RecSys/Search/Ads应用或与Transformer/LLM技术的联系，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08224v1": {
    "title": "Efficient-SAM2: Accelerating SAM2 with Object-Aware Visual Encoding and Memory Retrieval",
    "url": "https://www.alphaxiv.org/abs/2602.08224v1",
    "arxiv_id": "2602.08224v1",
    "authors": "Jing Zhang, Zhikai Li, Xuewen Liu, Qingyi Gu",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 02:58:33",
    "ori_summary": "Segment Anything Model 2 (SAM2) shows excellent performance in video object segmentation tasks; however, the heavy computational burden hinders its application in real-time video processing. Although there have been efforts to improve the efficiency of SAM2, most of them focus on retraining a lightweight backbone, with little exploration into post-training acceleration. In this paper, we observe that SAM2 exhibits sparse perception pattern as biological vision, which provides opportunities for eliminating redundant computation and acceleration: i) In mask decoder, the attention primarily focuses on the foreground objects, whereas the image encoder in the earlier stage exhibits a broad attention span, which results in unnecessary computation to background regions. ii) In memory bank, only a small subset of tokens in each frame contribute significantly to memory attention, and the salient regions exhibit temporal consistency, making full-token computation redundant. With these insights, we propose Efficient-SAM2, which promotes SAM2 to adaptively focus on object regions while eliminating task-irrelevant computations, thereby significantly improving inference efficiency. Specifically, for image encoder, we propose object-aware Sparse Window Routing (SWR), a window-level computation allocation mechanism that leverages the consistency and saliency cues from the previous-frame decoder to route background regions into a lightweight shortcut branch. Moreover, for memory attention, we propose object-aware Sparse Memory Retrieval (SMR), which allows only the salient memory tokens in each frame to participate in computation, with the saliency pattern reused from their first recollection. With negligible additional parameters and minimal training overhead, Efficient-SAM2 delivers 1.68x speedup on SAM2.1-L model with only 1.0% accuracy drop on SA-V test set.",
    "summary": "",
    "translation": "高效SAM2：通过对象感知视觉编码与记忆检索加速SAM2",
    "relevance_score": 2,
    "reasoning": "该论文主要关注计算机视觉中的分割模型加速技术，属于纯粹的视觉效率优化范畴。虽然SAM2模型可能用于图像理解，但论文标题未提及与推荐系统、搜索或广告相关的应用场景，也没有涉及LLM技术、Transformer架构改进或多模态建模的通用方法。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08211v1": {
    "title": "Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension",
    "url": "https://www.alphaxiv.org/abs/2602.08211v1",
    "arxiv_id": "2602.08211v1",
    "authors": "Yik Lung Pang, Changjae Oh",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 02:22:39",
    "ori_summary": "Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.",
    "summary": "",
    "translation": "链式描述：无需训练即可提升多模态大语言模型在指代表达理解任务上的性能",
    "relevance_score": 3,
    "reasoning": "该论文主要关注多模态大语言模型（VLM）在指代表达理解任务上的改进，属于视觉-语言交互领域。虽然涉及多模态建模，但论文焦点是纯粹的视觉-语言任务（指代表达理解），没有明确展示如何将这种技术应用于推荐系统、搜索或广告中的异构数据处理。对于'VLM类比异构数据'这一关注点，该论文缺乏明确的跨模态应用场景说明。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08206v1": {
    "title": "Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation",
    "url": "https://www.alphaxiv.org/abs/2602.08206v1",
    "arxiv_id": "2602.08206v1",
    "authors": "Chufeng Zhou, Jian Wang, Xinyuan Liu, Xiaokang Zhang",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 02:09:21",
    "ori_summary": "Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings. This ``appearance-based\" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes. To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream. The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis. This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.",
    "summary": "",
    "translation": "地理空间推理驱动的词汇无关遥感语义分割",
    "relevance_score": 2,
    "reasoning": "该论文标题表明其专注于遥感图像处理中的语义分割技术，属于计算机视觉领域。虽然提到了“地理空间推理”，但这主要针对遥感图像的地理特性分析，而非推荐系统、搜索或广告中的异构数据处理。标题中未显示与Transformer架构、LLM技术或推荐/搜索/广告应用的直接关联，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08202v1": {
    "title": "Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video",
    "url": "https://www.alphaxiv.org/abs/2602.08202v1",
    "arxiv_id": "2602.08202v1",
    "authors": "Jinrong Lv, Xun Gong, Zhaohuan Li, Weili Jiang",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 01:53:47",
    "ori_summary": "Estimating Left Ventricular Ejection Fraction (LVEF) from echocardiograms constitutes an ill-posed inverse problem. Inherent noise, artifacts, and limited viewing angles introduce ambiguity, where a single video sequence may map not to a unique ground truth, but rather to a distribution of plausible physiological values. Prevailing deep learning approaches typically formulate this task as a standard regression problem that minimizes the Mean Squared Error (MSE). However, this paradigm compels the model to learn the conditional expectation, which may yield misleading predictions when the underlying posterior distribution is multimodal or heavy-tailed -- a common phenomenon in pathological scenarios. In this paper, we investigate the paradigm shift from deterministic regression toward generative regression. We propose the Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework designed to model the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Extensive experiments conducted on the EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets demonstrate that MCSDR achieves state-of-the-art performance. Notably, qualitative analysis reveals that the generation trajectories of our model exhibit distinct behaviors in cases characterized by high noise or significant physiological variability, thereby offering a novel layer of interpretability for AI-aided diagnosis.",
    "summary": "",
    "translation": "基于超声心动图视频的左心室射血分数估计生成式回归方法",
    "relevance_score": 1,
    "reasoning": "该论文标题明确指向医学影像（超声心动图）和特定医学指标（左心室射血分数）估计，属于明确的医学领域应用。根据用户列出的无关主题，医学、生物学、化学、物理学等特定领域应用应被排除，且该标题未显示出与推荐系统、搜索、广告或相关使能技术（如Transformer架构、LLM应用）的任何潜在关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08198v1": {
    "title": "PEGAsus: 3D Personalization of Geometry and Appearance",
    "url": "https://www.alphaxiv.org/abs/2602.08198v1",
    "arxiv_id": "2602.08198v1",
    "authors": "Jingyu Hu, Bin Hu, Ka-Hei Hui, Haipeng Li, Zhengzhe Liu, Daniel Cohen-Or, Chi-Wing Fu",
    "categories": "cs.CV, cs.GR",
    "pub_date": "2026-02-09 01:41:27",
    "ori_summary": "We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.",
    "summary": "",
    "translation": "PEGAsus：几何与外观的三维个性化",
    "relevance_score": 1,
    "reasoning": "该论文标题明确聚焦于3D几何和外观的个性化，属于计算机视觉和图形学领域。虽然个性化是推荐系统的核心概念，但这里的“个性化”特指3D模型的几何形状和视觉外观定制，与推荐系统、搜索或广告中的用户行为建模、内容排序等任务没有直接关联。该研究属于纯粹的3D视觉/图形学范畴，不符合当前关注的任何技术方向。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08189v1": {
    "title": "Chamelion: Reliable Change Detection for Long-Term LiDAR Mapping in Transient Environments",
    "url": "https://www.alphaxiv.org/abs/2602.08189v1",
    "arxiv_id": "2602.08189v1",
    "authors": "Seoyeon Jang, Alex Junho Lee, I Made Aswin Nahrendra, Hyun Myung",
    "categories": "cs.RO, cs.CV",
    "pub_date": "2026-02-09 01:15:29",
    "ori_summary": "Online change detection is crucial for mobile robots to efficiently navigate through dynamic environments. Detecting changes in transient settings, such as active construction sites or frequently reconfigured indoor spaces, is particularly challenging due to frequent occlusions and spatiotemporal variations. Existing approaches often struggle to detect changes and fail to update the map across different observations. To address these limitations, we propose a dual-head network designed for online change detection and long-term map maintenance. A key difficulty in this task is the collection and alignment of real-world data, as manually registering structural differences over time is both labor-intensive and often impractical. To overcome this, we develop a data augmentation strategy that synthesizes structural changes by importing elements from different scenes, enabling effective model training without the need for extensive ground-truth annotations. Experiments conducted at real-world construction sites and in indoor office environments demonstrate that our approach generalizes well across diverse scenarios, achieving efficient and accurate map updates.\\resubmit{Our source code and additional material are available at: https://chamelion-pages.github.io/.",
    "summary": "",
    "translation": "变色龙：瞬态环境中长期激光雷达地图构建的可靠变化检测",
    "relevance_score": 1,
    "reasoning": "该论文专注于激光雷达地图构建和变化检测，属于机器人感知和3D视觉领域。虽然涉及长期环境建模，但核心是传感器数据处理和地图维护，与推荐系统、搜索或广告的异构数据统一建模、Transformer架构或LLM应用没有直接关联。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08168v1": {
    "title": "DAS-SK: An Adaptive Model Integrating Dual Atrous Separable and Selective Kernel CNN for Agriculture Semantic Segmentation",
    "url": "https://www.alphaxiv.org/abs/2602.08168v1",
    "arxiv_id": "2602.08168v1",
    "authors": "Mei Ling Chee, Thangarajah Akilan, Aparna Ravindra Phalke, Kanchan Keisham",
    "categories": "cs.CV",
    "pub_date": "2026-02-09 00:14:39",
    "ori_summary": "Semantic segmentation in high-resolution agricultural imagery demands models that strike a careful balance between accuracy and computational efficiency to enable deployment in practical systems. In this work, we propose DAS-SK, a novel lightweight architecture that retrofits selective kernel convolution (SK-Conv) into the dual atrous separable convolution (DAS-Conv) module to strengthen multi-scale feature learning. The model further enhances the atrous spatial pyramid pooling (ASPP) module, enabling the capture of fine-grained local structures alongside global contextual information. Built upon a modified DeepLabV3 framework with two complementary backbones - MobileNetV3-Large and EfficientNet-B3, the DAS-SK model mitigates limitations associated with large dataset requirements, limited spectral generalization, and the high computational cost that typically restricts deployment on UAVs and other edge devices. Comprehensive experiments across three benchmarks: LandCover.ai, VDD, and PhenoBench, demonstrate that DAS-SK consistently achieves state-of-the-art performance, while being more efficient than CNN-, transformer-, and hybrid-based competitors. Notably, DAS-SK requires up to 21x fewer parameters and 19x fewer GFLOPs than top-performing transformer models. These findings establish DAS-SK as a robust, efficient, and scalable solution for real-time agricultural robotics and high-resolution remote sensing, with strong potential for broader deployment in other vision domains.",
    "summary": "",
    "translation": "DAS-SK：一种集成双空洞可分离与选择性核卷积神经网络的农业语义分割自适应模型",
    "relevance_score": 1,
    "reasoning": "该论文专注于农业领域的语义分割，属于纯粹的计算机视觉应用，与推荐系统、搜索或广告领域没有直接关联。论文标题中提到的CNN架构改进（双空洞可分离和选择性核）是特定于视觉任务的优化，没有显示出在异构数据处理、序列建模或推荐/搜索/广告应用中的潜在价值。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2602.08167v1": {
    "title": "Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning",
    "url": "https://www.alphaxiv.org/abs/2602.08167v1",
    "arxiv_id": "2602.08167v1",
    "authors": "Milan Ganai, Katie Luo, Jonas Frey, Clark Barrett, Marco Pavone",
    "categories": "cs.RO, cs.AI, cs.CV, cs.LG",
    "pub_date": "2026-02-09 00:10:17",
    "ori_summary": "Embodied Chain-of-Thought (CoT) reasoning has significantly enhanced Vision-Language-Action (VLA) models, yet current methods rely on rigid templates to specify reasoning primitives (e.g., objects in the scene, high-level plans, structural affordances). These templates can force policies to process irrelevant information that distracts from critical action-prediction signals. This creates a bottleneck: without successful policies, we cannot verify reasoning quality; without quality reasoning, we cannot build robust policies. We introduce R&B-EnCoRe, which enables models to bootstrap embodied reasoning from internet-scale knowledge through self-supervised refinement. By treating reasoning as a latent variable within importance-weighted variational inference, models can generate and distill a refined reasoning training dataset of embodiment-specific strategies without external rewards, verifiers, or human annotation. We validate R&B-EnCoRe across manipulation (Franka Panda in simulation, WidowX in hardware), legged navigation (bipedal, wheeled, bicycle, quadruped), and autonomous driving embodiments using various VLA architectures with 1B, 4B, 7B, and 30B parameters. Our approach achieves 28% gains in manipulation success, 101% improvement in navigation scores, and 21% reduction in collision-rate metric over models that indiscriminately reason about all available primitives. R&B-EnCoRe enables models to distill reasoning that is predictive of successful control, bypassing manual annotation engineering while grounding internet-scale knowledge in physical execution.",
    "summary": "",
    "translation": "基于自监督引导的动作预测具身推理",
    "relevance_score": 2,
    "reasoning": "该论文标题涉及具身推理和动作预测，主要属于机器人或具身智能领域，与推荐系统、搜索或广告的核心关注点无直接关联。虽然自监督学习是通用技术，但论文未明确展示其在推荐/搜索/广告中的潜在应用，因此相关性较低。",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  }
}